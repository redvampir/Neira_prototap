#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –ù–µ–π—Ä—ã v2.0

–†–∞–∑–¥–µ–ª—ë–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏:
- –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å (Working Memory) - —Ç–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞
- –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (Short-Term Memory) - —Å–µ—Å—Å–∏—è, –æ—á–∏—â–∞–µ—Ç—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ
- –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (Long-Term Memory) - –ø–æ—Å—Ç–æ—è–Ω–Ω–∞—è, —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
- –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å (Episodic Memory) - —Å–æ–±—ã—Ç–∏—è –∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å (Semantic Memory) - —Ñ–∞–∫—Ç—ã –∏ –∑–Ω–∞–Ω–∏—è

–ó–∞—â–∏—Ç–∞ –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π:
- –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å
- –û—Ü–µ–Ω–∫–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏ (confidence score)
- –ò—Å—Ç–æ—á–Ω–∏–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ (source tracking)
- –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏

ROADMAP v2.2:
- [x] –ë–∞–∑–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞–º—è—Ç–∏
- [x] –î–µ—Ç–µ–∫—Ç–æ—Ä –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
- [x] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å cells.py
- [x] –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º
- [x] Decay (–∑–∞–±—ã–≤–∞–Ω–∏–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π)
- [x] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
- [x] –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
- [x] –°–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏
- [x] –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π recall –≤ –ø—Ä–æ–º–ø—Ç
- [x] Boost confidence –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
"""

import json
import os
import math


def _env_bool(name: str, default: bool = False) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    value = raw.strip().lower()
    if value in {"1", "true", "yes", "y", "on"}:
        return True
    if value in {"0", "false", "no", "n", "off"}:
        return False
    return default

try:
    import numpy as np  # type: ignore
    _NUMPY_AVAILABLE = True
except Exception:
    np = None  # type: ignore
    _NUMPY_AVAILABLE = False
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict, field
from enum import Enum
import hashlib
import re

# –ò–º–ø–æ—Ä—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ LLM –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –¥–ª—è embeddings
try:
    from llm_providers import LLMManager, create_default_manager
    LLM_MANAGER_AVAILABLE = True
except ImportError:
    LLM_MANAGER_AVAILABLE = False
    print("‚ö†Ô∏è LLMManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º legacy Ollama embeddings")

try:
    from local_embeddings import get_local_embedding
    LOCAL_EMBEDDINGS_AVAILABLE = True
except ImportError:
    LOCAL_EMBEDDINGS_AVAILABLE = False

OLLAMA_DISABLED = _env_bool("NEIRA_DISABLE_OLLAMA", False)

# –ò–º–ø–æ—Ä—Ç –∑–∞—â–∏—Ç–Ω—ã—Ö –º–æ–¥—É–ª–µ–π v3.0
try:
    from memory_anomaly_detector import MemoryAnomalyDetector
    from memory_version_control import MemoryVersionControl
    PROTECTION_MODULES_AVAILABLE = True
except ImportError:
    PROTECTION_MODULES_AVAILABLE = False
    print("‚ö†Ô∏è –ó–∞—â–∏—Ç–Ω—ã–µ –º–æ–¥—É–ª–∏ –ø–∞–º—è—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")

# Legacy –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (fallback)
OLLAMA_EMBED_URL = "http://localhost:11434/api/embeddings"
EMBED_MODEL = "nomic-embed-text"


class MemoryType(Enum):
    """–¢–∏–ø—ã –ø–∞–º—è—Ç–∏"""
    WORKING = "working"          # –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–¥–æ 10 —Å–æ–æ–±—â–µ–Ω–∏–π)
    SHORT_TERM = "short_term"    # –°–µ—Å—Å–∏—è (–¥–æ 100 –∑–∞–ø–∏—Å–µ–π)
    LONG_TERM = "long_term"      # –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è –ø–∞–º—è—Ç—å
    EPISODIC = "episodic"        # –°–æ–±—ã—Ç–∏—è
    SEMANTIC = "semantic"        # –§–∞–∫—Ç—ã –∏ –∑–Ω–∞–Ω–∏—è


class MemoryCategory(Enum):
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
    FACT = "fact"                # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π —Ñ–∞–∫—Ç
    INSTRUCTION = "instruction"  # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    PREFERENCE = "preference"    # –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    LEARNED = "learned"          # –ò–∑—É—á–µ–Ω–Ω–æ–µ (—Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏)
    PERSON = "person"            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–µ–ª–æ–≤–µ–∫–µ
    EVENT = "event"              # –°–æ–±—ã—Ç–∏–µ
    CONVERSATION = "conversation" # –ö–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞


class ValidationStatus(Enum):
    """–°—Ç–∞—Ç—É—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    PENDING = "pending"          # –û–∂–∏–¥–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏
    VALIDATED = "validated"      # –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ –∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ
    REJECTED = "rejected"        # –û—Ç–∫–ª–æ–Ω–µ–Ω–æ –∫–∞–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—è
    USER_CONFIRMED = "user_confirmed"  # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º


@dataclass
class MemoryEntry:
    """–ó–∞–ø–∏—Å—å –≤ –ø–∞–º—è—Ç–∏"""
    id: str
    text: str
    memory_type: str
    category: str
    timestamp: str
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    confidence: float = 0.5      # 0.0-1.0, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
    validation_status: str = "pending"
    source: str = "conversation" # –û—Ç–∫—É–¥–∞ –ø—Ä–∏—à–ª–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    
    # –°–≤—è–∑–∏
    related_ids: List[str] = field(default_factory=list)
    context_hash: str = ""       # –•—ç—à –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    
    # –≠–º–±–µ–¥–¥–∏–Ω–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    embedding: Optional[List[float]] = None
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    access_count: int = 0
    last_accessed: Optional[str] = None
    
    # v0.7: –ü–æ–º–µ—Ç–∫–∞ –≤–∞–∂–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
    pinned: bool = False         # –ó–∞—â–∏—Ç–∞ –æ—Ç —É–¥–∞–ª–µ–Ω–∏—è
    tags: List[str] = field(default_factory=list)  # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Ç–µ–≥–∏
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'MemoryEntry':
        return cls(**{k: v for k, v in data.items() if k in cls.__dataclass_fields__})


class HallucinationDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"""
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    SUSPICIOUS_PATTERNS = [
        r'\$\d+',                           # –î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        r'–ø—Ä–∏–±—ã–ª—å|–¥–æ—Ö–æ–¥|–∑–∞—Ä–∞–±–æ—Ç',           # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
        r'–ø—Ä–æ–¥–∞–∂[–∞–∏]? (–º–æ—â–Ω–æ—Å—Ç|—Ä–µ—Å—É—Ä—Å)',    # –ü—Ä–æ–¥–∞–∂–∞ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–π
        r'\d+ (—Å–µ–∫—É–Ω–¥|–º–∏–Ω—É—Ç|—á–∞—Å)',          # –¢–æ—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –±–µ–∑ –æ—Å–Ω–æ–≤–∞–Ω–∏—è
        r'–∏–≥—Ä[–∞—ã]? (—Ç—Ä–µ–±—É–µ—Ç|–≤–∫–ª—é—á–∞–µ—Ç)',     # –ò–≥—Ä–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∫–∏
        r'—Ö–æ–¥ (–º–æ–∂–µ—Ç|–≤–∫–ª—é—á–∞–µ—Ç)',            # –ü–æ—à–∞–≥–æ–≤—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
        r'—Ü–µ–Ω–∞ (–º–æ–¥–µ–ª–∏|–º–æ—â–Ω–æ—Å—Ç–∏)',          # –¶–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
        r'–ø–æ–±–µ–¥–∏—Ç–µ–ª—å|–≤—ã–∏–≥—Ä',                # –ò–≥—Ä–æ–≤–∞—è —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—è
    ]
    
    # –°–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π (–µ—Å–ª–∏ 1+ –Ω–∞–π–¥–µ–Ω - –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ)
    HALLUCINATION_MARKERS = [
        '–∫–æ—Å—Ç—å', '–º–æ—â–Ω–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '–ø—Ä–∏–±—ã–ª—å', 
        '–ø—Ä–æ–¥–∞–∂–∞ –º–æ—â–Ω–æ—Å—Ç', '–≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω'
    ]
    
    # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã (–æ–¥–∏–Ω –Ω–∞–π–¥–µ–Ω = –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞)
    CRITICAL_MARKERS = ['–∫–æ—Å—Ç—å']
    
    @classmethod
    def check(cls, text: str, context: Optional[List[str]] = None) -> Tuple[bool, float, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
        
        Returns:
            (is_suspicious, confidence, reason)
        """
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ (–æ–¥–∏–Ω–æ—á–Ω—ã—Ö)
        for marker in cls.CRITICAL_MARKERS:
            if marker in text_lower:
                return True, 0.2, f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –º–∞—Ä–∫–µ—Ä –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏: {marker}"
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in cls.SUSPICIOUS_PATTERNS:
            if re.search(pattern, text_lower):
                return True, 0.3, f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: {pattern}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∞—Ä–∫–µ—Ä–æ–≤
        marker_count = sum(1 for m in cls.HALLUCINATION_MARKERS if m in text_lower)
        if marker_count >= 2:
            return True, 0.2, f"–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: {marker_count}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–∞–º–æ—Å—Å—ã–ª–∫–∏ –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if context:
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–∫—Ç—ã
            pass  # TODO: –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        return False, 0.7, "OK"


class SemanticSearch:
    """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –ø–∞–º—è—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä (—Å–æ–∑–¥–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
    _llm_manager: Optional[Any] = None
    
    @classmethod
    def _get_manager(cls) -> Optional[Any]:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM Manager"""
        if not LLM_MANAGER_AVAILABLE:
            return None
        if cls._llm_manager is None:
            cls._llm_manager = create_default_manager()
        return cls._llm_manager
    
    @staticmethod
    def get_embedding(text: str) -> Optional[List[float]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ LLM Manager (–∏–ª–∏ fallback –Ω–∞ Ollama)"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ LLM Manager
        if not text or not text.strip():
            return None
        if LOCAL_EMBEDDINGS_AVAILABLE:
            try:
                local_embedding = get_local_embedding(text)
                if local_embedding:
                    return local_embedding
            except Exception as e:
                print(f"Local embedding error: {e}")
        manager = SemanticSearch._get_manager()
        if manager:
            try:
                embedding = manager.get_embedding(text)
                if embedding:
                    return embedding
            except Exception as e:
                print(f"‚ö†Ô∏è LLMManager embedding error: {e}, trying legacy Ollama")
        
        if OLLAMA_DISABLED:
            return None
        # Fallback –Ω–∞ –ø—Ä—è–º–æ–π –≤—ã–∑–æ–≤ Ollama
        try:
            import requests
            response = requests.post(
                OLLAMA_EMBED_URL,
                json={"model": EMBED_MODEL, "prompt": text},
                timeout=30
            )
            if response.status_code == 200:
                return response.json().get("embedding", [])
        except Exception as e:
            print(f"‚ö†Ô∏è Legacy Ollama embedding error: {e}")
        
        return None
    
    @staticmethod
    def cosine_similarity(a: List[float], b: List[float]) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if not a or not b:
            return 0.0
        if _NUMPY_AVAILABLE and np is not None:
            a_np = np.array(a)
            b_np = np.array(b)
            dot = np.dot(a_np, b_np)
            norm = np.linalg.norm(a_np) * np.linalg.norm(b_np)
            return float(dot / (norm + 1e-8))

        dot = 0.0
        norm_a = 0.0
        norm_b = 0.0
        for x, y in zip(a, b):
            dot += float(x) * float(y)
            norm_a += float(x) * float(x)
            norm_b += float(y) * float(y)
        return float(dot / (math.sqrt(norm_a) * math.sqrt(norm_b) + 1e-8))
    
    @classmethod
    def search(cls, query: str, entries: List['MemoryEntry'], 
               top_k: int = 5, threshold: float = 0.3) -> List[Tuple['MemoryEntry', float]]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø–∏—Å—è–º –ø–∞–º—è—Ç–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∑–∞–ø–∏—Å—å, score) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        query_embedding = cls.get_embedding(query)
        if not query_embedding:
            return []
        
        scored = []
        for entry in entries:
            if entry.embedding:
                score = cls.cosine_similarity(query_embedding, entry.embedding)
                if score >= threshold:
                    scored.append((entry, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score (—É–±—ã–≤–∞–Ω–∏–µ)
        scored.sort(key=lambda x: x[1], reverse=True)
        return scored[:top_k]


class MemoryDecay:
    """–°–∏—Å—Ç–µ–º–∞ –∑–∞–±—ã–≤–∞–Ω–∏—è –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ decay
    DECAY_RATE = 0.05           # –°–Ω–∏–∂–µ–Ω–∏–µ confidence –∑–∞ –ø–µ—Ä–∏–æ–¥
    DECAY_PERIOD_DAYS = 7       # –ü–µ—Ä–∏–æ–¥ –≤ –¥–Ω—è—Ö
    MIN_CONFIDENCE = 0.1        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π confidence –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
    PROTECTED_CATEGORIES = [    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏, –∑–∞—â–∏—â—ë–Ω–Ω—ã–µ –æ—Ç –∑–∞–±—ã–≤–∞–Ω–∏—è
        MemoryCategory.INSTRUCTION.value,
        MemoryCategory.PERSON.value,
        MemoryCategory.FACT.value,
    ]
    
    @classmethod
    def apply_decay(cls, entries: List['MemoryEntry']) -> Tuple[List['MemoryEntry'], List['MemoryEntry']]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç decay –∫ –∑–∞–ø–∏—Å—è–º –ø–∞–º—è—Ç–∏
        
        Returns:
            (kept_entries, forgotten_entries)
        """
        now = datetime.now()
        kept = []
        forgotten = []
        
        for entry in entries:
            # –ó–∞—â–∏—â—ë–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ –∑–∞–±—ã–≤–∞–µ–º
            if entry.category in cls.PROTECTED_CATEGORIES:
                kept.append(entry)
                continue
            
            # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –Ω–µ –∑–∞–±—ã–≤–∞–µ–º
            if entry.validation_status == ValidationStatus.USER_CONFIRMED.value:
                kept.append(entry)
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–æ—Å—Ç—É–ø–∞
            last_access = entry.last_accessed or entry.timestamp
            try:
                last_dt = datetime.fromisoformat(last_access)
                days_since = (now - last_dt).days
            except:
                days_since = 0
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º decay
            periods = days_since // cls.DECAY_PERIOD_DAYS
            if periods > 0 and entry.access_count < 3:
                # –°–Ω–∏–∂–∞–µ–º confidence —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                new_confidence = entry.confidence - (cls.DECAY_RATE * periods)
                entry.confidence = max(new_confidence, 0.0)
            
            # –ó–∞–±—ã–≤–∞–µ–º –µ—Å–ª–∏ confidence —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π
            if entry.confidence < cls.MIN_CONFIDENCE:
                forgotten.append(entry)
            else:
                kept.append(entry)
        
        return kept, forgotten


class AutoCategorizer:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏"""
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    PATTERNS = {
        MemoryCategory.PERSON.value: [
            r'^[–ê-–ØA-Z][–∞-—èa-z]+\s+(‚Äî|-)?\s*(—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è|—Å–æ–∑–¥–∞—Ç–µ–ª—å|–∞–≤—Ç–æ—Ä|–¥—Ä—É–≥|–∑–Ω–∞–∫–æ–º—ã–π)',
            r'(–æ–Ω|–æ–Ω–∞|–µ–≥–æ|–µ—ë|–µ–º—É)\s+(–ª—é–±–∏—Ç|—Ä–∞–±–æ—Ç–∞–µ—Ç|–∂–∏–≤—ë—Ç|–Ω—Ä–∞–≤–∏—Ç—Å—è)',
            r'(–∑–æ–≤—É—Ç|–∏–º—è)\s+[–ê-–ØA-Z][–∞-—èa-z]+',
        ],
        MemoryCategory.INSTRUCTION.value: [
            r'^(–Ω–µ\s+)?(–¥–µ–ª–∞–π|–≥–æ–≤–æ—Ä–∏|–∏—Å–ø–æ–ª—å–∑—É–π|–æ—Ç–≤–µ—á–∞–π|–ø–∏—à–∏|–Ω–∞–∑—ã–≤–∞–π)',
            r'(–¥–æ–ª–∂–Ω[–∞—ã]?|–Ω—É–∂–Ω–æ|–Ω–∞–¥–æ|—Å–ª–µ–¥—É–µ—Ç)\s+',
            r'(–≤—Å–µ–≥–¥–∞|–Ω–∏–∫–æ–≥–¥–∞)\s+(–¥–µ–ª–∞–π|–≥–æ–≤–æ—Ä–∏|–∏—Å–ø–æ–ª—å–∑—É–π)',
        ],
        MemoryCategory.PREFERENCE.value: [
            r'(–ª—é–±–ª—é|–Ω—Ä–∞–≤–∏—Ç—Å—è|–ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞|—Ö–æ—á—É|—Ö–æ—Ç–µ–ª –±—ã)',
            r'(–Ω–µ\s+–ª—é–±–ª—é|–Ω–µ\s+–Ω—Ä–∞–≤–∏—Ç—Å—è|–Ω–µ–Ω–∞–≤–∏–∂—É)',
            r'(–ª—é–±–∏–º|–º–æ[–π—è—ë]\s+–ª—é–±–∏–º)',
        ],
        MemoryCategory.EVENT.value: [
            r'(–≤—á–µ—Ä–∞|—Å–µ–≥–æ–¥–Ω—è|–∑–∞–≤—Ç—Ä–∞|–Ω–µ–¥–∞–≤–Ω–æ)\s+',
            r'\d{1,2}[./]\d{1,2}[./]\d{2,4}',
            r'(–ø—Ä–æ–∏–∑–æ—à–ª–æ|—Å–ª—É—á–∏–ª–æ—Å—å|–±—ã–ª–æ|–±—É–¥–µ—Ç)',
        ],
        MemoryCategory.FACT.value: [
            r'^[–ê-–ØA-Z].+\s+(‚Äî|—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è)\s+',
            r'(–≤–µ—Ä—Å–∏—è|–≤–µ—Ä—Å–∏–∏)\s+\d+',
        ],
    }
    
    @classmethod
    def categorize(cls, text: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°—Ç—Ä–æ–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (MemoryCategory value)
        """
        text_lower = text.lower()
        
        for category, patterns in cls.PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return category
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - conversation
        return MemoryCategory.CONVERSATION.value


class ContradictionDetector:
    """
    v2.2: –î–µ—Ç–µ–∫—Ç–æ—Ä –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π –≤ –ø–∞–º—è—Ç–∏
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π.
    """
    
    # –ê–Ω—Ç–æ–Ω–∏–º—ã –∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
    ANTONYMS = {
        '–ª—é–±–∏—Ç': ['–Ω–µ–Ω–∞–≤–∏–¥–∏—Ç', '–Ω–µ –ª—é–±–∏—Ç', '—Ç–µ—Ä–ø–µ—Ç—å –Ω–µ –º–æ–∂–µ—Ç'],
        '–Ω—Ä–∞–≤–∏—Ç—Å—è': ['–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è', '—Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç', '–±–µ—Å–∏—Ç'],
        '—Ö–æ—á–µ—Ç': ['–Ω–µ —Ö–æ—á–µ—Ç', '–æ—Ç–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è'],
        '—É–º–µ–µ—Ç': ['–Ω–µ —É–º–µ–µ—Ç', '–Ω–µ –º–æ–∂–µ—Ç'],
        '–∑–Ω–∞–µ—Ç': ['–Ω–µ –∑–Ω–∞–µ—Ç', '–Ω–µ –≤ –∫—É—Ä—Å–µ'],
        '–∏—Å–ø–æ–ª—å–∑—É–µ—Ç': ['–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç', '–∏–∑–±–µ–≥–∞–µ—Ç'],
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': ['–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '—Å–ª–æ–º–∞–Ω'],
        '–¥–∞': ['–Ω–µ—Ç'],
        'true': ['false'],
        '–≤–∫–ª—é—á—ë–Ω': ['–≤—ã–∫–ª—é—á–µ–Ω', '–æ—Ç–∫–ª—é—á—ë–Ω'],
        '–∞–∫—Ç–∏–≤–µ–Ω': ['–Ω–µ–∞–∫—Ç–∏–≤–µ–Ω', '–æ—Ç–∫–ª—é—á—ë–Ω'],
    }
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É–±—ä–µ–∫—Ç–∞ –∏ –ø—Ä–µ–¥–∏–∫–∞—Ç–∞
    SUBJECT_PATTERNS = [
        r'^([–ê-–ØA-Z][–∞-—èa-z]+)\s+(‚Äî|—ç—Ç–æ|—è–≤–ª—è–µ—Ç—Å—è|–ª—é–±–∏—Ç|–Ω–µ–Ω–∞–≤–∏–¥–∏—Ç|—Ö–æ—á–µ—Ç|—É–º–µ–µ—Ç|–∑–Ω–∞–µ—Ç|–∏—Å–ø–æ–ª—å–∑—É–µ—Ç)',
        r'^(–ù–µ–π—Ä–∞|–ü–∞–≤–µ–ª|–°–æ–∑–¥–∞—Ç–µ–ª—å)\s+',
    ]
    
    @classmethod
    def extract_subject(cls, text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É–±—ä–µ–∫—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        for pattern in cls.SUBJECT_PATTERNS:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(1).lower()
        return None
    
    @classmethod
    def check_contradiction(cls, new_text: str, existing_entries: List['MemoryEntry'],
                           similarity_threshold: float = 0.7) -> Tuple[bool, Optional['MemoryEntry'], str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç –ª–∏ –Ω–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∑–∞–ø–∏—Å—è–º
        
        Returns:
            (is_contradiction, conflicting_entry, reason)
        """
        new_lower = new_text.lower()
        new_subject = cls.extract_subject(new_text)
        
        # –ò—â–µ–º –∑–∞–ø–∏—Å–∏ —Å —Ç–µ–º –∂–µ —Å—É–±—ä–µ–∫—Ç–æ–º
        for entry in existing_entries:
            entry_lower = entry.text.lower()
            entry_subject = cls.extract_subject(entry.text)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—É–±—ä–µ–∫—Ç—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
            if new_subject and entry_subject and new_subject == entry_subject:
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä—è–º—ã—Ö –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —á–µ—Ä–µ–∑ –∞–Ω—Ç–æ–Ω–∏–º—ã
                for word, antonyms in cls.ANTONYMS.items():
                    # –ù–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–æ, —Å—Ç–∞—Ä—ã–π - –∞–Ω—Ç–æ–Ω–∏–º
                    if word in new_lower:
                        for ant in antonyms:
                            if ant in entry_lower:
                                return True, entry, f"–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: '{word}' vs '{ant}'"
                    
                    # –ò–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç
                    if word in entry_lower:
                        for ant in antonyms:
                            if ant in new_lower:
                                return True, entry, f"–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: '{word}' vs '{ant}'"
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ (–ø–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã —Å —Ä–∞–∑–Ω—ã–º —Å–º—ã—Å–ª–æ–º)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –µ—Å–ª–∏ –æ–±–∞ —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ –æ–¥–Ω–æ
                if entry.embedding:
                    new_embedding = SemanticSearch.get_embedding(new_text)
                    if new_embedding:
                        similarity = SemanticSearch.cosine_similarity(new_embedding, entry.embedding)
                        
                        # –í—ã—Å–æ–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –Ω–æ —Ä–∞–∑–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è - –≤–æ–∑–º–æ–∂–Ω–æ–µ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ
                        if similarity > similarity_threshold:
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—Ä–∏—Ü–∞–Ω–∏—è
                            new_has_neg = any(neg in new_lower for neg in ['–Ω–µ ', '–Ω–µ—Ç', '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏'])
                            old_has_neg = any(neg in entry_lower for neg in ['–Ω–µ ', '–Ω–µ—Ç', '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏'])
                            
                            if new_has_neg != old_has_neg:
                                return True, entry, f"–ü–æ—Ö–æ–∂–∏–µ —Ç–µ–º—ã, —Ä–∞–∑–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è (similarity={similarity:.2f})"
        
        return False, None, "OK"
    
    @classmethod
    def find_related(cls, text: str, entries: List['MemoryEntry'], 
                    top_k: int = 3) -> List['MemoryEntry']:
        """–ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–ª–∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è)"""
        subject = cls.extract_subject(text)
        if not subject:
            return []
        
        related = []
        for entry in entries:
            entry_subject = cls.extract_subject(entry.text)
            if entry_subject and entry_subject == subject:
                related.append(entry)
        
        return related[:top_k]


class MemorySystem:
    """–°–∏—Å—Ç–µ–º–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –ù–µ–π—Ä—ã"""
    
    def __init__(self, base_path: str = "."):
        self.base_path = base_path
        
        # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –ø–∞–º—è—Ç–∏
        self.paths = {
            MemoryType.LONG_TERM: os.path.join(base_path, "neira_memory.json"),
            MemoryType.SHORT_TERM: os.path.join(base_path, "neira_short_term.json"),
            MemoryType.EPISODIC: os.path.join(base_path, "neira_episodic.json"),
            MemoryType.SEMANTIC: os.path.join(base_path, "neira_semantic.json"),
        }
        
        # –ü–∞–º—è—Ç—å –≤ RAM
        self.working_memory: List[MemoryEntry] = []  # –¢–µ–∫—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        self.short_term: List[MemoryEntry] = []      # –°–µ—Å—Å–∏—è
        self.long_term: List[MemoryEntry] = []       # –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è
        self.episodic: List[MemoryEntry] = []        # –°–æ–±—ã—Ç–∏—è
        self.semantic: List[MemoryEntry] = []        # –ó–Ω–∞–Ω–∏—è
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.working_memory_size = 10    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π
        self.short_term_size = 100       # –ú–∞–∫—Å–∏–º—É–º –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π
        self.consolidation_threshold = 3  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –Ω—É–∂–Ω–æ –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é
        
        # üõ°Ô∏è –ó–ê–©–ò–¢–ê –û–¢ –ü–ï–†–ï–ü–û–õ–ù–ï–ù–ò–Ø v2.4
        self.max_long_term = 1000        # –ú–∞–∫—Å–∏–º—É–º –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        self.max_semantic = 500          # –ú–∞–∫—Å–∏–º—É–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞–Ω–∏–π
        self.max_episodic = 300          # –ú–∞–∫—Å–∏–º—É–º —ç–ø–∏–∑–æ–¥–æ–≤
        self.min_confidence_keep = 0.3   # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.auto_cleanup_enabled = True # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞
        
        # üõ°Ô∏è –ó–ê–©–ò–¢–ê v3.0: –î–µ—Ç–µ–∫—Ç–æ—Ä –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        if PROTECTION_MODULES_AVAILABLE:
            self.anomaly_detector = MemoryAnomalyDetector(window_size=20)
            self.version_control = MemoryVersionControl(
                snapshots_dir=os.path.join(base_path, "memory_snapshots")
            )
            print("‚úÖ –ó–∞—â–∏—Ç–Ω—ã–µ –º–æ–¥—É–ª–∏ –ø–∞–º—è—Ç–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã")
        else:
            self.anomaly_detector = None
            self.version_control = None
        
        # –ó–∞–≥—Ä—É–∑–∫–∞
        self._load_all()
        
        # –ü—Ä–∏–º–µ–Ω–∏—Ç—å –ª–∏–º–∏—Ç—ã –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
        if self.auto_cleanup_enabled:
            self._apply_limits(auto_snapshot=True)
    
    def _generate_id(self, text: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∑–∞–ø–∏—Å–∏"""
        content = f"{text}{datetime.now().isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()[:12]
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ —Å–ª–æ–≤–∞–º (Jaccard similarity)"""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _load_all(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ç–∏–ø—ã –ø–∞–º—è—Ç–∏"""
        self.long_term = self._load_memory(MemoryType.LONG_TERM)
        self.short_term = self._load_memory(MemoryType.SHORT_TERM)
        self.episodic = self._load_memory(MemoryType.EPISODIC)
        self.semantic = self._load_memory(MemoryType.SEMANTIC)
    
    def _load_memory(self, memory_type: MemoryType) -> List[MemoryEntry]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞–º—è—Ç—å –∏–∑ —Ñ–∞–π–ª–∞"""
        path = self.paths.get(memory_type)
        if not path or not os.path.exists(path):
            return []
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            entries = []
            for item in data:
                if isinstance(item, dict):
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–æ–ª—è
                    if 'id' not in item:
                        item['id'] = self._generate_id(item.get('text', ''))
                    if 'memory_type' not in item:
                        item['memory_type'] = memory_type.value
                    if 'validation_status' not in item:
                        item['validation_status'] = ValidationStatus.PENDING.value
                    if 'confidence' not in item:
                        item['confidence'] = 0.5
                    
                    entries.append(MemoryEntry.from_dict(item))
            
            return entries
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {memory_type.value}: {e}")
            return []
    
    def _apply_limits(self, auto_snapshot: bool = False):
        """
        üõ°Ô∏è –ü—Ä–∏–º–µ–Ω—è–µ—Ç –ª–∏–º–∏—Ç—ã –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏
        
        Args:
            auto_snapshot: –°–æ–∑–¥–∞—Ç—å snapshot –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π (v3.0)
        """
        # v3.0: –°–æ–∑–¥–∞—ë–º snapshot –ø–µ—Ä–µ–¥ –æ—á–∏—Å—Ç–∫–æ–π
        if auto_snapshot and self.version_control:
            try:
                self.version_control.create_snapshot(
                    [asdict(m) for m in self.long_term],
                    message="Auto-snapshot before cleanup"
                )
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è snapshot: {e}")
        
        initial_counts = {
            'long_term': len(self.long_term),
            'short_term': len(self.short_term),
            'semantic': len(self.semantic),
            'episodic': len(self.episodic)
        }
        
        # 1. –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N
        if len(self.short_term) > self.short_term_size:
            self.short_term = self.short_term[-self.short_term_size:]
        
        # 2. –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å - —Ç–æ–ø –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ + —Å–≤–µ–∂–µ—Å—Ç–∏
        if len(self.long_term) > self.max_long_term:
            # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
            self.long_term = [
                m for m in self.long_term 
                if m.confidence >= self.min_confidence_keep
            ]
            
            # –ï—Å–ª–∏ –≤—Å—ë –µ—â—ë –º–Ω–æ–≥–æ - —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∏ –±–µ—Ä—ë–º —Ç–æ–ø
            if len(self.long_term) > self.max_long_term:
                self.long_term = sorted(
                    self.long_term,
                    key=lambda x: (x.confidence, x.access_count, x.timestamp),
                    reverse=True
                )[:self.max_long_term]
        
        # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å - —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –∑–Ω–∞–Ω–∏—è
        if len(self.semantic) > self.max_semantic:
            self.semantic = sorted(
                self.semantic,
                key=lambda x: (x.confidence, x.access_count),
                reverse=True
            )[:self.max_semantic]
        
        # 4. –≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å - –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
        if len(self.episodic) > self.max_episodic:
            self.episodic = sorted(
                self.episodic,
                key=lambda x: x.timestamp,
                reverse=True
            )[:self.max_episodic]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        final_counts = {
            'long_term': len(self.long_term),
            'short_term': len(self.short_term),
            'semantic': len(self.semantic),
            'episodic': len(self.episodic)
        }
        
        if initial_counts != final_counts:
            removed = sum(initial_counts.values()) - sum(final_counts.values())
            # –¢–∏—Ö–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ –≤—ã–≤–æ–¥–∞
            self._save_memory(MemoryType.LONG_TERM, self.long_term)
            self._save_memory(MemoryType.SHORT_TERM, self.short_term)
            self._save_memory(MemoryType.SEMANTIC, self.semantic)
            self._save_memory(MemoryType.EPISODIC, self.episodic)
    
    def _save_memory(self, memory_type: MemoryType, entries: List[MemoryEntry]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–∞–º—è—Ç—å –≤ —Ñ–∞–π–ª"""
        path = self.paths.get(memory_type)
        if not path:
            return
        
        try:
            data = [e.to_dict() for e in entries]
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {memory_type.value}: {e}")
    
    def remember(self, 
                 text: str, 
                 category: Optional[MemoryCategory] = None,
                 source: str = "conversation",
                 context: Optional[List[str]] = None,
                 force_long_term: bool = False,
                 auto_embed: bool = True) -> Optional[MemoryEntry]:
        """
        –ó–∞–ø–æ–º–∏–Ω–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        
        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–¥—ë—Ç –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å.
        –í –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é - —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏–ª–∏ —è–≤–Ω–æ–≥–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è.
        
        v2.1: –î–æ–±–∞–≤–ª–µ–Ω—ã –∞–≤—Ç–æ–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        v2.3: –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è - –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        # v2.3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã (–∑–∞—â–∏—Ç–∞ –æ—Ç –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è)
        text_normalized = text.strip().lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 –∑–∞–ø–∏—Å–µ–π –Ω–∞ —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        recent_memories = (self.short_term[-50:] if len(self.short_term) > 50 else self.short_term) + \
                         (self.long_term[-50:] if len(self.long_term) > 50 else self.long_term)
        
        for existing in recent_memories:
            if existing.text.strip().lower() == text_normalized:
                # –¢–æ—á–Ω—ã–π –¥—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω - –æ–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –¥–æ—Å—Ç—É–ø–∞
                existing.access_count += 1
                existing.last_accessed = datetime.now().isoformat()
                print(f"‚ö†Ô∏è –î—É–±–ª–∏–∫–∞—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {text[:50]}...")
                return existing  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∑–∞–ø–∏—Å—å
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–æ—Ç—É –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π (–∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ø–∞–º–∞)
        similar_count = 0
        time_window = datetime.now() - timedelta(minutes=5)
        
        for existing in recent_memories:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–ø–∏—Å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –º–∏–Ω—É—Ç
            try:
                entry_time = datetime.fromisoformat(existing.timestamp)
                if entry_time > time_window:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç—å (>80% —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤)
                    similarity = self._calculate_text_similarity(text, existing.text)
                    if similarity > 0.8:
                        similar_count += 1
            except:
                pass
        
        # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 5 –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π –∑–∞ 5 –º–∏–Ω—É—Ç - –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ
        if similar_count > 5:
            print(f"üö® –ó–ê–¶–ò–ö–õ–ò–í–ê–ù–ò–ï –û–ë–ù–ê–†–£–ñ–ï–ù–û! –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–ø–∏—Å—å: {text[:50]}...")
            print(f"   –ù–∞–π–¥–µ–Ω–æ {similar_count} –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –º–∏–Ω—É—Ç")
            return None
        
        # v3.0: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π –ü–ï–†–ï–î –∑–∞–ø–∏—Å—å—é
        if self.anomaly_detector:
            anomaly_report = self.anomaly_detector.check(text)
            if anomaly_report.is_anomaly:
                print(f"üö´ –ê–ù–û–ú–ê–õ–ò–Ø –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù–ê: {anomaly_report.reason}")
                for suggestion in anomaly_report.suggestions:
                    print(f"   ‚Ä¢ {suggestion}")
                return None  # –ë–ª–æ–∫–∏—Ä—É–µ–º –∑–∞–ø–∏—Å—å
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
        is_suspicious, confidence, reason = HallucinationDetector.check(text, context)
        
        if is_suspicious:
            print(f"‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–µ–Ω–∏–µ –Ω–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—é: {reason}")
            print(f"   –¢–µ–∫—Å—Ç: {text[:100]}...")
            confidence = min(confidence, 0.3)
        
        # v2.2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–º—è—Ç—å—é
        if not is_suspicious:
            is_contradiction, conflict, contra_reason = ContradictionDetector.check_contradiction(
                text, self.long_term + self.short_term
            )
            if is_contradiction and conflict:
                print(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ: {contra_reason}")
                print(f"   –ù–æ–≤–æ–µ: {text[:60]}...")
                print(f"   –°—Ç–∞—Ä–æ–µ: {conflict.text[:60]}...")
                # –ù–µ –±–ª–æ–∫–∏—Ä—É–µ–º, –Ω–æ —Å–Ω–∏–∂–∞–µ–º confidence –∏ –ø–æ–º–µ—á–∞–µ–º
                confidence = min(confidence, 0.4)
                is_suspicious = True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ —Ç—Ä–µ–±—É—é—â–µ–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        
        # v2.1: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
        if category is None:
            auto_category = AutoCategorizer.categorize(text)
            category_value = auto_category
        else:
            category_value = category.value
        
        # v2.1: –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        embedding = None
        if auto_embed:
            embedding = SemanticSearch.get_embedding(text)
        
        # v2.2: –ù–∞—Ö–æ–¥–∏–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
        related = ContradictionDetector.find_related(text, self.long_term)
        related_ids = [r.id for r in related[:5]]
        
        # –°–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å
        entry = MemoryEntry(
            id=self._generate_id(text),
            text=text,
            memory_type=MemoryType.SHORT_TERM.value,
            category=category_value,
            timestamp=datetime.now().isoformat(),
            confidence=confidence,
            validation_status=ValidationStatus.PENDING.value if is_suspicious else ValidationStatus.VALIDATED.value,
            source=source,
            context_hash=hashlib.md5(''.join(context or []).encode()).hexdigest()[:8],
            embedding=embedding,
            related_ids=related_ids  # v2.2: —Å–≤—è–∑–∏
        )
        
        # –†–µ—à–∞–µ–º –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å
        if force_long_term and not is_suspicious:
            entry.memory_type = MemoryType.LONG_TERM.value
            entry.validation_status = ValidationStatus.USER_CONFIRMED.value
            self.long_term.append(entry)
            self._save_memory(MemoryType.LONG_TERM, self.long_term)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å: {text[:50]}...")
        else:
            self.short_term.append(entry)
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏
            if len(self.short_term) > self.short_term_size:
                self._consolidate_short_term()
            self._save_memory(MemoryType.SHORT_TERM, self.short_term)
            print(f"üìù –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å: {text[:50]}...")
        
        # üõ°Ô∏è –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –ø–æ—Å–ª–µ –∫–∞–∂–¥—ã—Ö 10 –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π
        total_memories = len(self.long_term) + len(self.short_term) + len(self.semantic)
        if self.auto_cleanup_enabled and total_memories % 10 == 0:
            self._apply_limits()
        
        return entry
    
    def add_to_working(self, text: str, role: str = "user"):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å (–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞)"""
        entry = MemoryEntry(
            id=self._generate_id(text),
            text=text,
            memory_type=MemoryType.WORKING.value,
            category=role,
            timestamp=datetime.now().isoformat(),
            confidence=1.0,  # –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å –≤—Å–µ–≥–¥–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–∞
            validation_status=ValidationStatus.VALIDATED.value,
            source="dialog"
        )
        
        self.working_memory.append(entry)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
        if len(self.working_memory) > self.working_memory_size:
            self.working_memory = self.working_memory[-self.working_memory_size:]
    
    def get_context(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        context_parts = []
        
        # –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
        if self.working_memory:
            context_parts.append("=== –¢–µ–∫—É—â–∏–π –¥–∏–∞–ª–æ–≥ ===")
            for entry in self.working_memory[-5:]:
                context_parts.append(f"[{entry.category}]: {entry.text}")
        
        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
        # TODO: –ø–æ–∏—Å–∫ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        
        return "\n".join(context_parts)
    
    def get_contextual_recall(self, current_message: str, max_memories: int = 5) -> str:
        """
        v2.2: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π recall ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
        
        Args:
            current_message: –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_memories: –ú–∞–∫—Å–∏–º—É–º –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è
            
        Returns:
            –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è–º–∏
        """
        context_parts = []
        
        # 1. –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞)
        if self.working_memory:
            context_parts.append("=== –¢–µ–∫—É—â–∏–π –¥–∏–∞–ª–æ–≥ ===")
            for entry in self.working_memory[-5:]:
                context_parts.append(f"[{entry.category}]: {entry.text}")
        
        # 2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—É—â–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é
        relevant = self.semantic_search(current_message, top_k=max_memories, threshold=0.4)
        
        if relevant:
            context_parts.append("\n=== –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è ===")
            for entry, score in relevant:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ confidence
                conf_icon = "üü¢" if entry.confidence > 0.7 else "üü°" if entry.confidence > 0.4 else "üî¥"
                context_parts.append(f"{conf_icon} [{entry.category}] {entry.text}")
        
        # 3. –í–∞–∂–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–≤—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º)
        instructions = [e for e in self.long_term 
                       if e.category == MemoryCategory.INSTRUCTION.value 
                       and e.confidence > 0.5]
        
        if instructions:
            context_parts.append("\n=== –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ===")
            for entry in instructions[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
                context_parts.append(f"üìå {entry.text}")
        
        return "\n".join(context_parts)
    
    def _consolidate_short_term(self):
        """–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏ –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é"""
        print("üîÑ –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...")
        
        validated = []
        rejected = []
        
        for entry in self.short_term:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏
            if entry.validation_status == ValidationStatus.VALIDATED.value:
                if entry.confidence >= 0.6:
                    entry.memory_type = MemoryType.LONG_TERM.value
                    validated.append(entry)
                else:
                    rejected.append(entry)
            elif entry.validation_status == ValidationStatus.USER_CONFIRMED.value:
                entry.memory_type = MemoryType.LONG_TERM.value
                validated.append(entry)
            else:
                # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏ —É–¥–∞–ª—è—é—Ç—Å—è
                rejected.append(entry)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—É—é
        self.long_term.extend(validated)
        self._save_memory(MemoryType.LONG_TERM, self.long_term)
        
        # –û—á–∏—â–∞–µ–º –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é
        self.short_term = []
        self._save_memory(MemoryType.SHORT_TERM, self.short_term)
        
        print(f"‚úÖ –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–æ: {len(validated)} –∑–∞–ø–∏—Å–µ–π")
        print(f"üóëÔ∏è –û—Ç–∫–ª–æ–Ω–µ–Ω–æ: {len(rejected)} –∑–∞–ø–∏—Å–µ–π")
    
    def confirm_memory(self, memory_id: str) -> bool:
        """–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –∑–∞–ø–∏—Å—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
        for entry in self.short_term:
            if entry.id == memory_id:
                entry.validation_status = ValidationStatus.USER_CONFIRMED.value
                entry.confidence = 1.0
                self._save_memory(MemoryType.SHORT_TERM, self.short_term)
                return True
        return False
    
    def reject_memory(self, memory_id: str) -> bool:
        """–û—Ç–∫–ª–æ–Ω—è–µ—Ç –∑–∞–ø–∏—Å—å –∫–∞–∫ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏—é"""
        for entry in self.short_term:
            if entry.id == memory_id:
                entry.validation_status = ValidationStatus.REJECTED.value
                entry.confidence = 0.0
                self._save_memory(MemoryType.SHORT_TERM, self.short_term)
                return True
        return False
    
    def search(self, query: str, memory_types: Optional[List[MemoryType]] = None) -> List[MemoryEntry]:
        """–ü–æ–∏—Å–∫ –ø–æ –ø–∞–º—è—Ç–∏"""
        results = []
        query_lower = query.lower()
        
        memories_to_search = []
        if not memory_types:
            memory_types = [MemoryType.LONG_TERM, MemoryType.SHORT_TERM]
        
        for mt in memory_types:
            if mt == MemoryType.LONG_TERM:
                memories_to_search.extend(self.long_term)
            elif mt == MemoryType.SHORT_TERM:
                memories_to_search.extend(self.short_term)
            elif mt == MemoryType.WORKING:
                memories_to_search.extend(self.working_memory)
        
        for entry in memories_to_search:
            if query_lower in entry.text.lower():
                results.append(entry)
        
        return results
    
    def semantic_search(self, query: str, top_k: int = 5, 
                        memory_types: Optional[List[MemoryType]] = None,
                        threshold: float = 0.3) -> List[Tuple[MemoryEntry, float]]:
        """
        v2.1: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –ø–∞–º—è—Ç–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∑–∞–ø–∏—Å—å, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        memories_to_search: List[MemoryEntry] = []
        
        if not memory_types:
            memory_types = [MemoryType.LONG_TERM, MemoryType.SHORT_TERM]
        
        for mt in memory_types:
            if mt == MemoryType.LONG_TERM:
                memories_to_search.extend(self.long_term)
            elif mt == MemoryType.SHORT_TERM:
                memories_to_search.extend(self.short_term)
            elif mt == MemoryType.WORKING:
                memories_to_search.extend(self.working_memory)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SemanticSearch
        results = SemanticSearch.search(query, memories_to_search, top_k, threshold)
        
        # v2.2: –û–±–Ω–æ–≤–ª—è–µ–º access_count –∏ boost confidence –¥–ª—è –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        now = datetime.now().isoformat()
        for entry, score in results:
            entry.access_count += 1
            entry.last_accessed = now
            # Boost confidence –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–º–∞–∫—Å 1.0)
            if entry.confidence < 1.0:
                entry.confidence = min(1.0, entry.confidence + 0.02)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._save_memory(MemoryType.LONG_TERM, self.long_term)
        
        return results
    
    def recall(self, query: str, top_k: int = 5) -> List[str]:
        """
        v2.1: –í—Å–ø–æ–º–Ω–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π
        """
        results = self.semantic_search(query, top_k)
        return [entry.text for entry, score in results]
    
    def apply_decay(self) -> Dict[str, int]:
        """
        v2.1: –ü—Ä–∏–º–µ–Ω—è–µ—Ç –∑–∞–±—ã–≤–∞–Ω–∏–µ –∫ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ {kept: N, forgotten: N}
        """
        kept, forgotten = MemoryDecay.apply_decay(self.long_term)
        
        if forgotten:
            self.long_term = kept
            self._save_memory(MemoryType.LONG_TERM, self.long_term)
            print(f"üßπ –ó–∞–±—ã—Ç–æ {len(forgotten)} –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        
        return {"kept": len(kept), "forgotten": len(forgotten)}
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏"""
        return {
            "working_memory": len(self.working_memory),
            "short_term": len(self.short_term),
            "long_term": len(self.long_term),
            "episodic": len(self.episodic),
            "semantic": len(self.semantic),
            "total": (len(self.working_memory) + len(self.short_term) + 
                     len(self.long_term) + len(self.episodic) + len(self.semantic)),
            "pending_validation": sum(1 for e in self.short_term 
                                     if e.validation_status == ValidationStatus.PENDING.value)
        }
    
    def clear_working_memory(self):
        """–û—á–∏—â–∞–µ—Ç —Ä–∞–±–æ—á—É—é –ø–∞–º—è—Ç—å (–Ω–æ–≤–∞—è —Å–µ—Å—Å–∏—è)"""
        self.working_memory = []
    
    def clear_short_term(self):
        """–û—á–∏—â–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—É—é –ø–∞–º—è—Ç—å"""
        self.short_term = []
        self._save_memory(MemoryType.SHORT_TERM, self.short_term)


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    print("=" * 60)
    print("–¢–ï–°–¢ –°–ò–°–¢–ï–ú–´ –ü–ê–ú–Ø–¢–ò –ù–ï–ô–†–´ v2.2")
    print("=" * 60)
    
    memory = MemorySystem(".")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = memory.get_stats()
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:")
    for key, value in stats.items():
        print(f"   {key}: {value}")
    
    # –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    print(f"\nüîç –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π:")
    test_texts = [
        "–ü–∞–≤–µ–ª –ª—é–±–∏—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ",
        "–ü—Ä–æ–¥–∞–∂–∞ –º–æ—â–Ω–æ—Å—Ç–µ–π –¥–∞—ë—Ç $500 –≤ —á–∞—Å",
        "–ù–µ–π—Ä–∞ - —ç—Ç–æ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç",
    ]
    for text in test_texts:
        is_sus, conf, reason = HallucinationDetector.check(text)
        status = "üö® BLOCKED" if is_sus else "‚úÖ OK"
        print(f"   {status} [{conf:.1f}] {text[:40]}...")
    
    # v2.1: –¢–µ—Å—Ç –∞–≤—Ç–æ–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏
    print(f"\nüè∑Ô∏è –¢–µ—Å—Ç –∞–≤—Ç–æ–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏:")
    categorize_tests = [
        "–ü–∞–≤–µ–ª ‚Äî —Å–æ–∑–¥–∞—Ç–µ–ª—å –ù–µ–π—Ä—ã",
        "–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –≤ –æ—Ç–≤–µ—Ç–∞—Ö",
        "–ú–Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è Python",
        "–í—á–µ—Ä–∞ –±—ã–ª–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã",
        "–ù–µ–π—Ä–∞ –≤–µ—Ä—Å–∏–∏ 0.6",
    ]
    for text in categorize_tests:
        cat = AutoCategorizer.categorize(text)
        print(f"   [{cat}] {text}")
    
    # v2.2: –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
    print(f"\n‚öîÔ∏è –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π:")
    contradiction_tests = [
        ("–ü–∞–≤–µ–ª –ª—é–±–∏—Ç Python", "–ü–∞–≤–µ–ª –Ω–µ–Ω–∞–≤–∏–¥–∏—Ç Python"),
        ("–ù–µ–π—Ä–∞ —É–º–µ–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å", "–ù–µ–π—Ä–∞ –Ω–µ —É–º–µ–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞—Ç—å"),
        ("–°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–°–∏—Å—Ç–µ–º–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"),
    ]
    for text1, text2 in contradiction_tests:
        # –°–æ–∑–¥–∞—ë–º —Ñ–µ–π–∫–æ–≤—É—é –∑–∞–ø–∏—Å—å –¥–ª—è —Ç–µ—Å—Ç–∞
        fake_entry = MemoryEntry(
            id="test", text=text1, memory_type="long_term",
            category="fact", timestamp="2025-01-01", confidence=0.8
        )
        is_contra, _, reason = ContradictionDetector.check_contradiction(text2, [fake_entry])
        status = "‚ö†Ô∏è –ü–†–û–¢–ò–í–û–†–ï–ß–ò–ï" if is_contra else "‚úÖ OK"
        print(f"   {status}: '{text1}' vs '{text2}'")
    
    # v2.1: –¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    print(f"\nüîé –¢–µ—Å—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞:")
    results = memory.semantic_search("—Å–æ–∑–¥–∞—Ç–µ–ª—å", top_k=3)
    if results:
        for entry, score in results:
            print(f"   [{score:.2f}] {entry.text[:50]}...")
    else:
        print("   (–Ω–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–ª–∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")
    
    # v2.2: –¢–µ—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ recall
    print(f"\nüìö –¢–µ—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ recall:")
    context = memory.get_contextual_recall("–ö—Ç–æ —Å–æ–∑–¥–∞–ª –ù–µ–π—Ä—É?")
    print(context[:500] + "..." if len(context) > 500 else context)
    
    print("\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ v2.2 –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")


# ========== –†–ê–°–®–ò–†–ï–ù–ò–Ø –î–õ–Ø TELEGRAM BOT v0.7 ==========

class MemoryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é —á–µ—Ä–µ–∑ Telegram"""
    
    def __init__(self, memory_system: MemorySystem):
        self.memory = memory_system
    
    def search_by_text(self, query: str, case_sensitive: bool = False) -> List[MemoryEntry]:
        """–ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ç–µ–∫—Å—Ç"""
        results = []
        all_entries = (self.memory.long_term + self.memory.short_term + 
                      self.memory.episodic + self.memory.semantic)
        
        for entry in all_entries:
            text_to_search = entry.text if case_sensitive else entry.text.lower()
            search_query = query if case_sensitive else query.lower()
            
            if search_query in text_to_search:
                results.append(entry)
        
        return results
    
    def delete_by_text(self, query: str, case_sensitive: bool = False) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç–µ–∫—Å—Ç (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ)
        Returns: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        if not query or not query.strip():
            return 0
        
        count = 0
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –û–î–ò–ù –†–ê–ó (–≤–Ω–µ —Ü–∏–∫–ª–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
        search_query = query.strip() if case_sensitive else query.strip().lower()
        
        # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —Ç–∏–ø–∞–º –ø–∞–º—è—Ç–∏
        for memory_list in [self.memory.long_term, self.memory.short_term, 
                           self.memory.episodic, self.memory.semantic]:
            original_len = len(memory_list)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–∞–ø–∏—Å–∏: —É–¥–∞–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ù–ï–∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞
            # –û—Å—Ç–∞–≤–ª—è–µ–º: –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ OR –±–µ–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            memory_list[:] = [
                entry for entry in memory_list
                if (entry.pinned or  # –ó–∞—â–∏—Ç–∞ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö
                    search_query not in (entry.text if case_sensitive else entry.text.lower()))
            ]
            
            # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —É–¥–∞–ª–∏–ª–∏ –∏–∑ —ç—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞
            deleted_from_list = original_len - len(memory_list)
            count += deleted_from_list
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ñ–∞–π–ª—ã
        if count > 0:
            self._save_all()
        
        return count
    
    def delete_last_n(self, n: int, memory_type: Optional[str] = None) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –∑–∞–ø–∏—Å–µ–π (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ)
        
        Args:
            n: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
            memory_type: —Ç–∏–ø –ø–∞–º—è—Ç–∏ –∏–ª–∏ None –¥–ª—è –≤—Å–µ—Ö
        
        Returns: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        count = 0
        
        if memory_type:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞
            memory_list = self._get_memory_list(memory_type)
            if memory_list:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ
                unpinned = [e for e in memory_list if not e.pinned]
                removed = min(n, len(unpinned))
                
                # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –Ω–µ–∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö
                to_remove_set = set(unpinned[-removed:])
                memory_list[:] = [e for e in memory_list if e not in to_remove_set]
                count = removed
        else:
            # –£–¥–∞–ª—è–µ–º –∏–∑ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ (—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
            all_entries = []
            for mtype in ["long_term", "short_term", "episodic", "semantic"]:
                mlist = self._get_memory_list(mtype)
                if mlist:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
                    all_entries.extend([(entry, mtype) for entry in mlist if not entry.pinned])
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ –≤ –∫–æ–Ω—Ü–µ)
            all_entries.sort(key=lambda x: x[0].timestamp)
            
            # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N
            to_remove = all_entries[-n:] if n < len(all_entries) else all_entries
            
            for entry, mtype in to_remove:
                mlist = self._get_memory_list(mtype)
                if mlist and entry in mlist:  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
                    mlist.remove(entry)
                    count += 1
        
        self._save_all()
        return count
    
    def delete_by_category(self, category: str) -> int:
        """–£–¥–∞–ª—è–µ—Ç –≤—Å–µ –∑–∞–ø–∏—Å–∏ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ)"""
        count = 0
        
        for memory_list in [self.memory.long_term, self.memory.short_term,
                           self.memory.episodic, self.memory.semantic]:
            original_len = len(memory_list)
            memory_list[:] = [
                entry for entry in memory_list
                if entry.pinned or entry.category != category  # –ó–∞—â–∏—Ç–∞ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö
            ]
            count += original_len - len(memory_list)
        
        self._save_all()
        return count
    
    def delete_old_entries(self, days: int) -> int:
        """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å–∏ —Å—Ç–∞—Ä—à–µ N –¥–Ω–µ–π (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ)"""
        count = 0
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for memory_list in [self.memory.long_term, self.memory.short_term,
                           self.memory.episodic, self.memory.semantic]:
            original_len = len(memory_list)
            
            memory_list[:] = [
                entry for entry in memory_list
                if (entry.pinned or  # –ó–∞—â–∏—Ç–∞ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã—Ö
                    datetime.fromisoformat(entry.timestamp) > cutoff_date)
            ]
            
            count += original_len - len(memory_list)
        
        self._save_all()
        return count
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏"""
        stats = {
            "total": 0,
            "by_type": {},
            "by_category": {},
            "oldest": None,
            "newest": None,
            "average_confidence": 0.0,
        }
        
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        stats["total"] = len(all_entries)
        stats["by_type"]["long_term"] = len(self.memory.long_term)
        stats["by_type"]["short_term"] = len(self.memory.short_term)
        stats["by_type"]["episodic"] = len(self.memory.episodic)
        stats["by_type"]["semantic"] = len(self.memory.semantic)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for entry in all_entries:
            cat = entry.category or "uncategorized"
            stats["by_category"][cat] = stats["by_category"].get(cat, 0) + 1
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã
        if all_entries:
            timestamps = [datetime.fromisoformat(e.timestamp) for e in all_entries]
            stats["oldest"] = min(timestamps).isoformat()
            stats["newest"] = max(timestamps).isoformat()
            
            # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidences = [e.confidence for e in all_entries if e.confidence]
            stats["average_confidence"] = sum(confidences) / len(confidences) if confidences else 0.0
        
        return stats
    
    def deduplicate(self, similarity_threshold: float = 0.95) -> int:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã (–∑–∞–ø–∏—Å–∏ —Å –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç—å—é)
        Returns: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        count = 0
        
        for memory_list in [self.memory.long_term, self.memory.short_term,
                           self.memory.episodic, self.memory.semantic]:
            seen_texts = set()
            unique_entries = []
            
            for entry in memory_list:
                text_normalized = entry.text.strip().lower()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
                if text_normalized not in seen_texts:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å —É–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–º–∏
                    is_duplicate = False
                    for unique in unique_entries:
                        similarity = self.memory._calculate_text_similarity(
                            entry.text, unique.text
                        )
                        if similarity >= similarity_threshold:
                            is_duplicate = True
                            break
                    
                    if not is_duplicate:
                        unique_entries.append(entry)
                        seen_texts.add(text_normalized)
                    else:
                        count += 1
                else:
                    count += 1
            
            memory_list[:] = unique_entries
        
        self._save_all()
        return count
    
    def create_backup(self, backup_name: Optional[str] = None) -> str:
        """–°–æ–∑–¥–∞—ë—Ç –±—ç–∫–∞–ø –≤—Å–µ–π –ø–∞–º—è—Ç–∏"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = backup_name or f"memory_backup_{timestamp}"
        backup_dir = os.path.join(self.memory.base_path, "backups")
        os.makedirs(backup_dir, exist_ok=True)
        
        backup_path = os.path.join(backup_dir, f"{backup_name}.json")
        
        backup_data = {
            "timestamp": datetime.now().isoformat(),
            "stats": self.get_stats(),
            "long_term": [e.to_dict() for e in self.memory.long_term],
            "short_term": [e.to_dict() for e in self.memory.short_term],
            "episodic": [e.to_dict() for e in self.memory.episodic],
            "semantic": [e.to_dict() for e in self.memory.semantic],
        }
        
        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2)
        
        return backup_path
    
    def _get_memory_list(self, memory_type: str) -> Optional[List[MemoryEntry]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –ø–∞–º—è—Ç–∏ –ø–æ —Ç–∏–ø—É"""
        mapping = {
            "long_term": self.memory.long_term,
            "short_term": self.memory.short_term,
            "episodic": self.memory.episodic,
            "semantic": self.memory.semantic,
        }
        return mapping.get(memory_type)
    
    def _save_all(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —Ç–∏–ø—ã –ø–∞–º—è—Ç–∏"""
        self.memory._save_memory(MemoryType.LONG_TERM, self.memory.long_term)
        self.memory._save_memory(MemoryType.SHORT_TERM, self.memory.short_term)
        self.memory._save_memory(MemoryType.EPISODIC, self.memory.episodic)
        self.memory._save_memory(MemoryType.SEMANTIC, self.memory.semantic)
    
    # ========== v0.7 –†–ê–°–®–ò–†–ï–ù–ò–Ø ==========
    
    def filter_by_confidence(self, operator: str, threshold: float) -> List[MemoryEntry]:
        """
        –§–∏–ª—å—Ç—Ä –ø–æ —É—Ä–æ–≤–Ω—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
        Args:
            operator: '<', '>', '<=', '>=', '=='
            threshold: –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0.0-1.0)
        """
        results = []
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        for entry in all_entries:
            conf = entry.confidence or 0.5
            
            if operator == '<' and conf < threshold:
                results.append(entry)
            elif operator == '>' and conf > threshold:
                results.append(entry)
            elif operator == '<=' and conf <= threshold:
                results.append(entry)
            elif operator == '>=' and conf >= threshold:
                results.append(entry)
            elif operator == '==' and abs(conf - threshold) < 0.01:
                results.append(entry)
        
        return results
    
    def filter_by_source(self, source: str) -> List[MemoryEntry]:
        """–§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        results = []
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        for entry in all_entries:
            if entry.source == source:
                results.append(entry)
        
        return results
    
    def filter_by_timerange(self, hours: int) -> List[MemoryEntry]:
        """–§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É (–ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤)"""
        results = []
        cutoff = datetime.now() - timedelta(hours=hours)
        
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        for entry in all_entries:
            try:
                entry_time = datetime.fromisoformat(entry.timestamp)
                if entry_time > cutoff:
                    results.append(entry)
            except:
                pass
        
        return results
    
    def pin_entry(self, entry_id: str) -> bool:
        """–ó–∞–∫—Ä–µ–ø–∏—Ç—å –∑–∞–ø–∏—Å—å (–∑–∞—â–∏—Ç–∞ –æ—Ç —É–¥–∞–ª–µ–Ω–∏—è)"""
        all_lists = [self.memory.long_term, self.memory.short_term,
                    self.memory.episodic, self.memory.semantic]
        
        for memory_list in all_lists:
            for entry in memory_list:
                if entry.id == entry_id:
                    entry.pinned = True
                    self._save_all()
                    return True
        
        return False
    
    def unpin_entry(self, entry_id: str) -> bool:
        """–û—Ç–∫—Ä–µ–ø–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        all_lists = [self.memory.long_term, self.memory.short_term,
                    self.memory.episodic, self.memory.semantic]
        
        for memory_list in all_lists:
            for entry in memory_list:
                if entry.id == entry_id:
                    entry.pinned = False
                    self._save_all()
                    return True
        
        return False
    
    def get_pinned(self) -> List[MemoryEntry]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∑–∞–∫—Ä–µ–ø–ª—ë–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏"""
        results = []
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        for entry in all_entries:
            if entry.pinned:
                results.append(entry)
        
        return results
    
    def add_tag(self, entry_id: str, tag: str) -> bool:
        """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–≥ –∫ –∑–∞–ø–∏—Å–∏"""
        all_lists = [self.memory.long_term, self.memory.short_term,
                    self.memory.episodic, self.memory.semantic]
        
        for memory_list in all_lists:
            for entry in memory_list:
                if entry.id == entry_id:
                    if tag not in entry.tags:
                        entry.tags.append(tag)
                        self._save_all()
                    return True
        
        return False
    
    def filter_by_tag(self, tag: str) -> List[MemoryEntry]:
        """–ù–∞–π—Ç–∏ –∑–∞–ø–∏—Å–∏ —Å –æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–º —Ç–µ–≥–æ–º"""
        results = []
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        for entry in all_entries:
            if tag in entry.tags:
                results.append(entry)
        
        return results
    
    def export_to_text(self, category: Optional[str] = None) -> str:
        """
        –≠–∫—Å–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏ –≤ —á–∏—Ç–∞–µ–º—ã–π —Ç–µ–∫—Å—Ç
        
        Args:
            category: —Ñ–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (None = –≤—Å–µ)
        """
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        if category:
            all_entries = [e for e in all_entries if e.category == category]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        all_entries.sort(key=lambda e: e.timestamp)
        
        lines = ["# –≠–∫—Å–ø–æ—Ä—Ç –ø–∞–º—è—Ç–∏ Neira", f"# –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M')}", ""]
        
        if category:
            lines.append(f"# –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category}\n")
        
        for entry in all_entries:
            timestamp = datetime.fromisoformat(entry.timestamp).strftime("%Y-%m-%d %H:%M")
            pin_mark = "üìå " if entry.pinned else ""
            tags = f" [—Ç–µ–≥–∏: {', '.join(entry.tags)}]" if entry.tags else ""
            
            lines.append(f"## {pin_mark}[{entry.category}] {timestamp}")
            lines.append(f"{entry.text}")
            lines.append(f"_–ò—Å—Ç–æ—á–Ω–∏–∫: {entry.source} | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {entry.confidence:.0%}{tags}_")
            lines.append("")
        
        return "\n".join(lines)
    
    def restore_from_backup(self, backup_name: str) -> bool:
        """
        –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–∞–º—è—Ç—å –∏–∑ –±—ç–∫–∞–ø–∞
        
        Args:
            backup_name: –∏–º—è —Ñ–∞–π–ª–∞ –±—ç–∫–∞–ø–∞ (—Å .json –∏–ª–∏ –±–µ–∑)
        """
        if not backup_name.endswith('.json'):
            backup_name += '.json'
        
        backup_dir = os.path.join(self.memory.base_path, "backups")
        backup_path = os.path.join(backup_dir, backup_name)
        
        if not os.path.exists(backup_path):
            return False
        
        try:
            with open(backup_path, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–ø –ø–∞–º—è—Ç–∏
            self.memory.long_term = [
                MemoryEntry.from_dict(e) for e in backup_data.get('long_term', [])
            ]
            self.memory.short_term = [
                MemoryEntry.from_dict(e) for e in backup_data.get('short_term', [])
            ]
            self.memory.episodic = [
                MemoryEntry.from_dict(e) for e in backup_data.get('episodic', [])
            ]
            self.memory.semantic = [
                MemoryEntry.from_dict(e) for e in backup_data.get('semantic', [])
            ]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            self._save_all()
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: {e}")
            return False
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –±—ç–∫–∞–ø–æ–≤"""
        backup_dir = os.path.join(self.memory.base_path, "backups")
        if not os.path.exists(backup_dir):
            return []
        
        backups = []
        for filename in os.listdir(backup_dir):
            if filename.endswith('.json'):
                filepath = os.path.join(backup_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    backups.append({
                        'filename': filename,
                        'timestamp': data.get('timestamp', 'unknown'),
                        'total': data.get('stats', {}).get('total', 0),
                        'size': os.path.getsize(filepath),
                    })
                except:
                    pass
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–Ω–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ)
        backups.sort(key=lambda x: x['timestamp'], reverse=True)
        return backups
    
    def semantic_search(self, query: str, top_k: int = 10) -> List[Tuple[MemoryEntry, float]]:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ Ollama)
        
        Returns: —Å–ø–∏—Å–æ–∫ (–∑–∞–ø–∏—Å—å, score) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        all_entries = (self.memory.long_term + self.memory.short_term +
                      self.memory.episodic + self.memory.semantic)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        return SemanticSearch.search(query, all_entries, top_k=top_k)

