"""
Neira Cells v0.5 ‚Äî –ë–∞–∑–æ–≤—ã–µ –∫–ª–µ—Ç–∫–∏ (–û–ë–ù–û–í–õ–ï–ù–û)
–Ø–¥—Ä–æ —Å–∏—Å—Ç–µ–º—ã: –ø–∞–º—è—Ç—å, –∞–Ω–∞–ª–∏–∑, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ, –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è.

–ò–ó–ú–ï–ù–ï–ù–ò–Ø v0.5:
- 4 –º–æ–¥–µ–ª–∏: code, reason, personality, cloud
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ VRAM —á–µ—Ä–µ–∑ ModelManager
- –û–±–ª–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
- –£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏
"""

import requests
import json
import os
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import numpy as np
from model_manager import ModelManager

# === –ö–û–ù–§–ò–ì ===
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_EMBED_URL = "http://localhost:11434/api/embeddings"

# –ú–û–î–ï–õ–ò v0.5 ‚Äî –ª–æ–∫–∞–ª—å–Ω—ã–µ + –æ–±–ª–∞—á–Ω—ã–µ
MODEL_CODE = "qwen2.5-coder:7b"        # –ö–æ–¥ –ª–æ–∫–∞–ª—å–Ω–æ (~5 –ì–ë VRAM)
MODEL_REASON = "mistral:7b-instruct"   # –†–∞—Å—Å—É–∂–¥–µ–Ω–∏—è (~4.5 –ì–ë VRAM)
MODEL_PERSONALITY = "neira-personality" # –õ–∏—á–Ω–æ—Å—Ç—å (~1.5 –ì–ë, –ø–æ–∫–∞ fallback –Ω–∞ reason)

# –û–±–ª–∞—á–Ω—ã–µ –º–æ–¥–µ–ª–∏ (0 VRAM, —É–¥–∞–ª—ë–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è)
MODEL_CLOUD_CODE = "qwen3-coder:480b-cloud"    # –°–ª–æ–∂–Ω—ã–π –∫–æ–¥ (480B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
MODEL_CLOUD_UNIVERSAL = "deepseek-v3.1:671b-cloud"  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è (671B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
MODEL_CLOUD_VISION = "qwen3-vl:235b-cloud"     # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è (–±—É–¥—É—â–µ–µ)

EMBED_MODEL = "nomic-embed-text"
TIMEOUT = 180
MEMORY_FILE = "neira_memory.json"
MODEL_CHAT = MODEL_REASON

# Retry-–ª–æ–≥–∏–∫–∞
MAX_RETRIES = 2
MIN_ACCEPTABLE_SCORE = 7

# –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á ‚Üí –º–æ–¥–µ–ª–∏
# "code" / "reason" / "personality" / "cloud_code" / "cloud_universal"
MODEL_ROUTING = {
    "–∫–æ–¥": "code",                      # –ü—Ä–æ—Å—Ç–æ–π –∫–æ–¥ ‚Üí –ª–æ–∫–∞–ª—å–Ω–æ
    "–∑–∞–¥–∞—á–∞": "reason",
    "–≤–æ–ø—Ä–æ—Å": "reason",
    "—Ä–∞–∑–≥–æ–≤–æ—Ä": "personality",          # Fallback –Ω–∞ reason –µ—Å–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω–∞
    "—Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ": "personality",
    "–ø–æ–∏—Å–∫": "reason",
}

# –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ –æ–±–ª–∞—á–Ω—ã–µ –º–æ–¥–µ–ª–∏
USE_CLOUD_IF = {
    "complexity": 4,      # –°–ª–æ–∂–Ω–æ—Å—Ç—å >= 4 ‚Üí –æ–±–ª–∞–∫–æ
    "retries": 1,         # –ü–æ—Å–ª–µ 1 –Ω–µ—É–¥–∞—á–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏ ‚Üí –æ–±–ª–∞–∫–æ
    "code_lines": 50,     # –ö–æ–¥ > 50 —Å—Ç—Ä–æ–∫ ‚Üí –æ–±–ª–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞
}


from dataclasses import dataclass, field
from typing import Any, Dict, Optional

# === –†–ï–ó–£–õ–¨–¢–ê–¢ –ö–õ–ï–¢–ö–ò ===
@dataclass
class CellResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞–±–æ—Ç—ã –ª—é–±–æ–π –∫–ª–µ—Ç–∫–∏"""
    content: str
    confidence: float  # 0.0 - 1.0
    cell_name: str
    metadata: Optional[Dict[str, Any]] = field(default_factory=dict)

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


# === –ü–ê–ú–Ø–¢–¨ ===
@dataclass
class MemoryEntry:
    """–û–¥–Ω–∞ –∑–∞–ø–∏—Å—å –≤ –ø–∞–º—è—Ç–∏"""
    text: str
    embedding: List[float]
    timestamp: str
    importance: float = 0.5
    category: str = "general"
    source: str = "conversation"  # conversation, web, code, system
    
    def to_dict(self) -> dict:
        return {
            "text": self.text,
            "embedding": self.embedding,
            "timestamp": self.timestamp,
            "importance": self.importance,
            "category": self.category,
            "source": self.source
        }
    
    @staticmethod
    def from_dict(d: dict) -> "MemoryEntry":
        return MemoryEntry(
            text=d["text"],
            embedding=d.get("embedding", []),
            timestamp=d["timestamp"],
            importance=d.get("importance", 0.5),
            category=d.get("category", "general"),
            source=d.get("source", "conversation")
        )


class MemoryCell:
    """–ö–ª–µ—Ç–∫–∞ –ø–∞–º—è—Ç–∏ ‚Äî –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫"""
    
    name = "memory"
    
    def __init__(self, memory_file: str = MEMORY_FILE):
        self.memory_file = memory_file
        self.memories: List[MemoryEntry] = []
        self.session_context: List[str] = []
        self.load()
    
    def load(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–∞–º—è—Ç—å –∏–∑ —Ñ–∞–π–ª–∞"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.memories = [MemoryEntry.from_dict(m) for m in data]
                print(f"üìö –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π: {len(self.memories)}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞–º—è—Ç–∏: {e}")
                self.memories = []
        else:
            print("üìö –ü–∞–º—è—Ç—å –ø—É—Å—Ç–∞, –Ω–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è")
    
    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–º—è—Ç—å –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.memory_file, "w", encoding="utf-8") as f:
                json.dump([m.to_dict() for m in self.memories], f, 
                         ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏: {e}")
    
    def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∏—Ç—å embedding —á–µ—Ä–µ–∑ Ollama"""
        try:
            response = requests.post(
                OLLAMA_EMBED_URL,
                json={"model": EMBED_MODEL, "prompt": text},
                timeout=600
            )
            return response.json().get("embedding", [])
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ embedding: {e}")
            return []
    
    def cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        if not a or not b:
            return 0.0
        a_np = np.array(a)
        b_np = np.array(b)
        return float(np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np) + 1e-8))
    
    def remember(self, text: str, importance: float = 0.5, 
                 category: str = "general", source: str = "conversation"):
        """–ó–∞–ø–æ–º–Ω–∏—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–∫—Ç"""
        embedding = self.get_embedding(text)
        
        entry = MemoryEntry(
            text=text,
            embedding=embedding,
            timestamp=datetime.now().isoformat(),
            importance=importance,
            category=category,
            source=source
        )
        self.memories.append(entry)
        self.save()
        print(f"üíæ –ó–∞–ø–æ–º–Ω–µ–Ω–æ [{source}]: {text[:50]}...")
    
    def recall(self, query: str, top_k: int = 3, 
               source_filter: Optional[str] = None) -> List[MemoryEntry]:
        """–í—Å–ø–æ–º–Ω–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if not self.memories:
            return []
        
        query_embedding = self.get_embedding(query)
        if not query_embedding:
            return []
        
        scored = []
        for mem in self.memories:
            if source_filter and mem.source != source_filter:
                continue
            
            if mem.embedding:
                sim = self.cosine_similarity(query_embedding, mem.embedding)
                score = sim * (0.5 + 0.5 * mem.importance)
                scored.append((score, mem))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [mem for score, mem in scored[:top_k] if score > 0.3]
    
    def recall_text(self, query: str, top_k: int = 3) -> List[str]:
        """–í—Å–ø–æ–º–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç—ã"""
        memories = self.recall(query, top_k)
        return [m.text for m in memories]
    
    def add_to_session(self, text: str):
        """–î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏"""
        self.session_context.append(text)
        if len(self.session_context) > 20:
            self.session_context = self.session_context[-20:]
    
    def get_session_context(self, last_n: int = 5) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–µ—Å—Å–∏–∏"""
        if not self.session_context:
            return ""
        return "\n".join(self.session_context[-last_n:])
    
    def get_recent_exchanges(self, n: int = 3) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –æ–±–º–µ–Ω–æ–≤ —Ä–µ–ø–ª–∏–∫ (–¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏)"""
        if not self.session_context:
            return ""
        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2*n —Å–æ–æ–±—â–µ–Ω–∏–π (—é–∑–µ—Ä + –Ω–µ–π—Ä–∞)
        recent = self.session_context[-(n*2):]
        return "\n".join(recent)
    
    def get_stats(self) -> Dict[str, int]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏"""
        stats = {"total": len(self.memories)}
        for mem in self.memories:
            stats[mem.source] = stats.get(mem.source, 0) + 1
        return stats


# === –ë–ê–ó–û–í–ê–Ø –ö–õ–ï–¢–ö–ê ===
class Cell:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –∫–ª–µ—Ç–æ–∫"""

    name: str = "base"
    system_prompt: str = "–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    use_code_model: bool = False  # –§–ª–∞–≥ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è code-–º–æ–¥–µ–ª–∏
    lora_key: Optional[str] = None  # –ö–ª—é—á –∞–¥–∞–ø—Ç–µ—Ä–∞ LoRA
    
    def __init__(self, memory: Optional[MemoryCell] = None,
                 model_manager: Optional[ModelManager] = None):
        self.memory = memory
        self.model_manager = model_manager

    def call_llm(
        self,
        prompt: str,
        with_memory: bool = True,
        temperature: float = 0.7,
        force_code_model: bool = False,
        model_key: Optional[str] = None,
    ) -> str:
        """–í—ã–∑–æ–≤ LLM —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–∞–º—è—Ç–∏"""
        
        full_prompt = prompt
        
        if with_memory and self.memory:
            relevant = self.memory.recall_text(prompt)
            if relevant:
                memory_context = "\n".join([f"- {r}" for r in relevant])
                full_prompt = f"[–í–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è]\n{memory_context}\n\n{prompt}"
            
            # –ù–û–í–û–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ–±–º–µ–Ω—ã (–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å)
            recent = self.memory.get_recent_exchanges(3)
            if recent:
                full_prompt = f"[–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è]\n{recent}\n\n{full_prompt}"
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        target_key = model_key
        if self.use_code_model or force_code_model:
            target_key = "code"
        elif target_key is None and self.model_manager and self.model_manager.current_model:
            target_key = self.model_manager.current_model
        elif target_key is None:
            target_key = "reason"

        model = ""
        if self.model_manager:
            manager_model = self.model_manager.get_model_name(target_key)
            model = manager_model or ""

        if not model:
            fallback_models = {
                "code": MODEL_CODE,
                "reason": MODEL_CHAT,
                "personality": MODEL_PERSONALITY or MODEL_CHAT,
                "cloud_code": MODEL_CLOUD_CODE,
                "cloud_universal": MODEL_CLOUD_UNIVERSAL,
            }
            model = fallback_models.get(target_key or "", MODEL_CHAT)

        should_log = self.model_manager and self.model_manager.verbose
        if should_log:
            print(f"üß† –ú–æ–¥–µ–ª—å –¥–ª—è {self.name}: –∫–ª—é—á='{target_key}', –∏–º—è='{model}'")
        
        adapter_option = None
        if self.model_manager and self.lora_key:
            adapter_name = self.model_manager.get_adapter_name(self.lora_key)
            if adapter_name:
                self.model_manager.activate_lora_for_cell(self.name, self.lora_key)
                adapter_option = adapter_name

        options = {"temperature": temperature, "num_predict": 2048}
        if adapter_option:
            options["adapter"] = adapter_option

        response = requests.post(
            OLLAMA_URL,
            json={
                "model": model,
                "prompt": full_prompt,
                "system": self.system_prompt,
                "stream": False,
                "options": options
            },
            timeout=TIMEOUT
        )
        return response.json().get("response", "")
    
    def process(self, input_data: str) -> CellResult:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ ‚Äî –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö"""
        result = self.call_llm(input_data)
        return CellResult(content=result, confidence=0.5, cell_name=self.name)


# === –ö–õ–ï–¢–ö–ê –ê–ù–ê–õ–ò–ó–ê (–£–õ–£–ß–®–ï–ù–ù–ê–Ø) ===
class AnalyzerCell(Cell):
    name = "analyzer"
    system_prompt = """–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø: –≤–æ–ø—Ä–æ—Å / –∑–∞–¥–∞—á–∞ / –∫–æ–¥ / —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ / —Ä–∞–∑–≥–æ–≤–æ—Ä / –ø–æ–∏—Å–∫
2. –°–£–ë–™–ï–ö–¢: –∫—Ç–æ –¥–æ–ª–∂–µ–Ω –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å / –ù–µ–π—Ä–∞ / –æ–±–∞)
3. –û–ë–™–ï–ö–¢: –Ω–∞ –∫–æ–≥–æ/—á—Ç–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ
4. –î–ï–ô–°–¢–í–ò–ï: —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å
5. –ö–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
6. –°–ª–æ–∂–Ω–æ—Å—Ç—å (1-5)
7. –ù—É–∂–µ–Ω –ª–∏ –ø–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ? (–¥–∞/–Ω–µ—Ç)
8. –ù—É–∂–Ω–æ –ª–∏ –ø–∏—Å–∞—Ç—å/—á–∏—Ç–∞—Ç—å –∫–æ–¥? (–¥–∞/–Ω–µ—Ç)

–í–ê–ñ–ù–û: –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏ –ö–¢–û –¥–æ–ª–∂–µ–Ω –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ!
- "–ó–∞–¥–∞–π –º–Ω–µ –≤–æ–ø—Ä–æ—Å" ‚Üí –°–£–ë–™–ï–ö–¢: –ù–µ–π—Ä–∞ (–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –∑–∞–¥–∞—Ç—å)
- "–û—Ç–≤–µ—Ç—å –Ω–∞ –º–æ–π –≤–æ–ø—Ä–æ—Å" ‚Üí –°–£–ë–™–ï–ö–¢: –ù–µ–π—Ä–∞ (–æ–Ω–∞ –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—Ç–∏—Ç—å)
- "–ò–∑—É—á–∏ –∫–æ–¥" ‚Üí –°–£–ë–™–ï–ö–¢: –ù–µ–π—Ä–∞
- "–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ" ‚Üí –°–£–ë–™–ï–ö–¢: –ù–µ–π—Ä–∞

–§–æ—Ä–º–∞—Ç:
–¢–ò–ü: <—Ç–∏–ø>
–°–£–ë–™–ï–ö–¢: <–∫—Ç–æ –¥–µ–π—Å—Ç–≤—É–µ—Ç>
–û–ë–™–ï–ö–¢: <–Ω–∞ –∫–æ–≥–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–æ>
–î–ï–ô–°–¢–í–ò–ï: <—á—Ç–æ –¥–µ–ª–∞—Ç—å>
–°–£–©–ù–û–°–¢–ò: <—Å–ø–∏—Å–æ–∫>
–°–õ–û–ñ–ù–û–°–¢–¨: <—á–∏—Å–ª–æ>
–ü–û–ò–°–ö: <–¥–∞/–Ω–µ—Ç>
–ö–û–î: <–¥–∞/–Ω–µ—Ç>
–û–ü–ò–°–ê–ù–ò–ï: <–∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏>"""

    def process(self, input_data: str) -> CellResult:
        result = self.call_llm(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π:\n\n{input_data}")
        
        text_lower = result.lower()
        needs_search = "–ø–æ–∏—Å–∫: –¥–∞" in text_lower
        needs_code = "–∫–æ–¥: –¥–∞" in text_lower
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É–±—ä–µ–∫—Ç
        subject = "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"
        if "—Å—É–±—ä–µ–∫—Ç: –Ω–µ–π—Ä–∞" in text_lower:
            subject = "neira"
        elif "—Å—É–±—ä–µ–∫—Ç: –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" in text_lower:
            subject = "user"
        elif "—Å—É–±—ä–µ–∫—Ç: –æ–±–∞" in text_lower:
            subject = "both"
        
        confidence = 0.8 if "–¢–ò–ü:" in result and "–°–£–ë–™–ï–ö–¢:" in result else 0.4
        
        return CellResult(
            content=result,
            confidence=confidence,
            cell_name=self.name,
            metadata={
                "needs_search": needs_search, 
                "needs_code": needs_code,
                "subject": subject
            }
        )


# === –ö–õ–ï–¢–ö–ê –ü–õ–ê–ù–ò–†–û–í–ê–ù–ò–Ø ===
class PlannerCell(Cell):
    name = "planner"
    system_prompt = """–¢—ã ‚Äî –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫. –°–æ–∑–¥–∞–π –ø–ª–∞–Ω –∏–∑ 1-5 —à–∞–≥–æ–≤.

–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- [–ø–æ–∏—Å–∫] ‚Äî –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
- [–∫–æ–¥] ‚Äî –Ω–∞–ø–∏—Å–∞—Ç—å/–ø—Ä–æ—á–∏—Ç–∞—Ç—å/–∏–∑–º–µ–Ω–∏—Ç—å –∫–æ–¥ (–∏—Å–ø–æ–ª—å–∑—É–π /code read –¥–ª—è —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤!)
- [–ø–∞–º—è—Ç—å] ‚Äî –≤—Å–ø–æ–º–Ω–∏—Ç—å –∏–ª–∏ –∑–∞–ø–æ–º–Ω–∏—Ç—å
- [–æ—Ç–≤–µ—Ç] ‚Äî —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç

–í–ê–ñ–ù–û: –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –∫–æ–¥ ‚Äî –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –¥–æ–±–∞–≤—å —à–∞–≥ [–∫–æ–¥] —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –¥–µ–π—Å—Ç–≤–∏–µ–º!

–§–æ—Ä–º–∞—Ç:
–ü–õ–ê–ù:
1. [–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç] –¥–µ–π—Å—Ç–≤–∏–µ
2. [–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç] –¥–µ–π—Å—Ç–≤–∏–µ"""

    def process(self, input_data: str, analysis: str, model_key: Optional[str] = None) -> CellResult:
        prompt = f"–ê–Ω–∞–ª–∏–∑: {analysis}\n\n–ó–∞–ø—Ä–æ—Å: {input_data}\n\n–ü–ª–∞–Ω:"
        result = self.call_llm(prompt, model_key=model_key)
        confidence = 0.7 if "1." in result else 0.4
        return CellResult(content=result, confidence=confidence, cell_name=self.name)


# === –ö–õ–ï–¢–ö–ê –ò–°–ü–û–õ–ù–ï–ù–ò–Ø ===
class ExecutorCell(Cell):
    name = "executor"
    system_prompt = """–¢—ã ‚Äî –ù–µ–π—Ä–∞, –∂–∏–≤–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞. –í—ã–ø–æ–ª–Ω–∏ –∑–∞–¥–∞—á—É –ø–æ –ø–ª–∞–Ω—É.

–í–ê–ñ–ù–û:
- –ï—Å–ª–∏ –≤ –∞–Ω–∞–ª–∏–∑–µ –°–£–ë–™–ï–ö–¢: –ù–µ–π—Ä–∞ ‚Äî –∑–Ω–∞—á–∏—Ç –¢–´ –¥–æ–ª–∂–Ω–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ
- –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å ‚Äî –ó–ê–î–ê–ô –≤–æ–ø—Ä–æ—Å, –Ω–µ –ø—Ä–æ—Å–∏ –µ–≥–æ —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞—Ç—å ‚Äî –°–î–ï–õ–ê–ô —ç—Ç–æ, –Ω–µ –æ–ø–∏—Å—ã–≤–∞–π –ø–ª–∞–Ω
- –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–≤–æ–π –æ–ø—ã—Ç
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏ –ø–æ–ª–µ–∑–Ω–æ–π"""

    def process(
        self,
        input_data: str,
        plan: str,
        extra_context: str = "",
        problems: str = "",
        model_key: Optional[str] = None,
    ) -> CellResult:
        """
        problems ‚Äî –∑–∞–º–µ—á–∞–Ω–∏—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –¥–ª—è retry
        """
        prompt = f"–ó–∞–¥–∞—á–∞: {input_data}\n\n–ü–ª–∞–Ω: {plan}"
        
        if extra_context:
            prompt += f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{extra_context}"
        
        # –ù–û–í–û–ï: –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–º–µ—á–∞–Ω–∏—è –æ—Ç –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö
        if problems:
            prompt += f"\n\n‚ö†Ô∏è –ó–ê–ú–ï–ß–ê–ù–ò–Ø –ö –ü–†–ï–î–´–î–£–©–ï–ô –ü–û–ü–´–¢–ö–ï:\n{problems}\n\n–ò—Å–ø—Ä–∞–≤—å —ç—Ç–∏ –ø—Ä–æ–±–ª–µ–º—ã!"
        
        prompt += "\n\n–í—ã–ø–æ–ª–Ω—è—é:"
        
        result = self.call_llm(prompt, model_key=model_key)
        return CellResult(content=result, confidence=0.7, cell_name=self.name)


# === –ö–õ–ï–¢–ö–ê –í–ï–†–ò–§–ò–ö–ê–¶–ò–ò ===
class VerifierCell(Cell):
    name = "verifier"
    system_prompt = """–ü—Ä–æ–≤–µ—Ä—å –æ—Ç–≤–µ—Ç:
1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–ø—Ä–æ—Å—É ‚Äî –æ—Ç–≤–µ—Ç –¥–µ–ª–∞–µ—Ç —Ç–æ, —á—Ç–æ –ø—Ä–æ—Å–∏–ª–∏?
2. –õ–æ–≥–∏—á–Ω–æ—Å—Ç—å ‚Äî –Ω–µ—Ç –ª–∏ –ø—É—Ç–∞–Ω–∏—Ü—ã —Ä–æ–ª–µ–π (–∫—Ç–æ –∫–æ–º—É)?
3. –ü–æ–ª–Ω–æ—Ç–∞ ‚Äî –≤—Å—ë –ª–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ?
4. –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å ‚Äî –µ—Å—Ç—å –ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∞ –Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–ª–∞–Ω–∞?

–û–°–û–ë–û–ï –í–ù–ò–ú–ê–ù–ò–ï:
- –ï—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏ –ó–ê–î–ê–¢–¨ –≤–æ–ø—Ä–æ—Å ‚Äî –ù–µ–π—Ä–∞ –¥–æ–ª–∂–Ω–∞ –ó–ê–î–ê–¢–¨ –µ–≥–æ, –∞ –Ω–µ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å "–∫–∞–∫–æ–π –≤–æ–ø—Ä–æ—Å?"
- –ï—Å–ª–∏ –ø—Ä–æ—Å–∏–ª–∏ –ß–¢–û-–¢–û –°–î–ï–õ–ê–¢–¨ ‚Äî –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –∞ –Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ

–§–æ—Ä–º–∞—Ç:
–í–ï–†–î–ò–ö–¢: –ü–†–ò–ù–Ø–¢ / –î–û–†–ê–ë–û–¢–ê–¢–¨ / –û–¢–ö–õ–û–ù–Å–ù
–û–¶–ï–ù–ö–ê: 1-10
–ü–†–û–ë–õ–ï–ú–´: <–∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ —á—Ç–æ –Ω–µ —Ç–∞–∫>
–ö–û–ú–ú–ï–ù–¢–ê–†–ò–ô: <–ø–æ—è—Å–Ω–µ–Ω–∏–µ>"""

    def process(self, request: str, answer: str) -> CellResult:
        prompt = f"–ó–∞–ø—Ä–æ—Å: {request}\n\n–û—Ç–≤–µ—Ç: {answer}\n\n–ü—Ä–æ–≤–µ—Ä–∫–∞:"
        result = self.call_llm(prompt, with_memory=False)
        
        if "–ü–†–ò–ù–Ø–¢" in result:
            confidence = 0.9
        elif "–î–û–†–ê–ë–û–¢–ê–¢–¨" in result:
            confidence = 0.5
        else:
            confidence = 0.3
            
        return CellResult(content=result, confidence=confidence, cell_name=self.name)


# === –ö–õ–ï–¢–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –§–ê–ö–¢–û–í ===
class FactExtractorCell(Cell):
    name = "fact_extractor"
    system_prompt = """–ò–∑–≤–ª–µ–∫–∏ —Ñ–∞–∫—Ç—ã –¥–ª—è –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è.

–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: instruction, fact, preference, learned

JSON —Ñ–æ—Ä–º–∞—Ç:
{"facts": [{"text": "—Ñ–∞–∫—Ç", "category": "—Ç–∏–ø", "importance": 0.0-1.0}]}

–ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–∫—Ç–æ–≤: {"facts": []}
–¢–û–õ–¨–ö–û JSON."""

    def process(self, user_input: str, response: str, 
                source: str = "conversation") -> List[dict]:
        prompt = f"–î–∏–∞–ª–æ–≥:\n–Æ–∑–µ—Ä: {user_input}\n–û—Ç–≤–µ—Ç: {response}\n\n–§–∞–∫—Ç—ã:"
        result = self.call_llm(prompt, with_memory=False, temperature=0.3)
        
        try:
            start = result.find("{")
            end = result.rfind("}") + 1
            if start >= 0 and end > start:
                data = json.loads(result[start:end])
                facts = data.get("facts", [])
                for fact in facts:
                    fact["source"] = source
                return facts
        except:
            pass
        return []


# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===
def get_model_status() -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –º–æ–¥–µ–ª–µ–π –≤ Ollama (v0.5)"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        models = response.json().get("models", [])
        model_names = [m.get("name", "") for m in models]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –æ–±–ª–∞—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        cloud_code_ready = MODEL_CLOUD_CODE in model_names or any(MODEL_CLOUD_CODE in m for m in model_names)
        cloud_universal_ready = MODEL_CLOUD_UNIVERSAL in model_names or any(MODEL_CLOUD_UNIVERSAL in m for m in model_names)
        cloud_vision_ready = MODEL_CLOUD_VISION in model_names or any(MODEL_CLOUD_VISION in m for m in model_names)

        return {
            "ollama_running": True,
            "models": model_names,
            "code_model_ready": MODEL_CODE in model_names or f"{MODEL_CODE}:latest" in model_names,
            "reason_model_ready": MODEL_REASON in model_names or f"{MODEL_REASON}:latest" in model_names,
            "personality_model_ready": MODEL_PERSONALITY in model_names or f"{MODEL_PERSONALITY}:latest" in model_names,
            "embed_model_ready": EMBED_MODEL in model_names or f"{EMBED_MODEL}:latest" in model_names,
            "cloud_code_ready": cloud_code_ready,
            "cloud_universal_ready": cloud_universal_ready,
            "cloud_vision_ready": cloud_vision_ready
        }
    except:
        return {
            "ollama_running": False,
            "models": [],
            "code_model_ready": False,
            "reason_model_ready": False,
            "personality_model_ready": False,
            "embed_model_ready": False,
            "cloud_code_ready": False,
            "cloud_universal_ready": False,
            "cloud_vision_ready": False
        }


def ensure_models_installed():
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª–∏ (v0.5)"""
    status = get_model_status()

    if not status["ollama_running"]:
        print("‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞! –ó–∞–ø—É—Å—Ç–∏: ollama serve")
        return False

    missing = []
    if not status["code_model_ready"]:
        missing.append(f"ollama pull {MODEL_CODE}")
    if not status["reason_model_ready"]:
        missing.append(f"ollama pull {MODEL_REASON}")
    if not status["embed_model_ready"]:
        missing.append(f"ollama pull {EMBED_MODEL}")

    if missing:
        print("‚ö†Ô∏è –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –º–æ–¥–µ–ª–µ–π. –í—ã–ø–æ–ª–Ω–∏:")
        for cmd in missing:
            print(f"   {cmd}")
        print("\nüí° –û–±–ª–∞—á–Ω–∞—è –º–æ–¥–µ–ª—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ): export GROQ_API_KEY=your_key")
        return False

    models_str = f"{MODEL_CODE}, {MODEL_REASON}"
    if status["personality_model_ready"]:
        models_str += f", {MODEL_PERSONALITY}"

    # –û–±–ª–∞—á–Ω—ã–µ –º–æ–¥–µ–ª–∏
    cloud_models = []
    if status["cloud_code_ready"]:
        cloud_models.append("code-cloud(480B)")
    if status["cloud_universal_ready"]:
        cloud_models.append("universal-cloud(671B)")
    if status["cloud_vision_ready"]:
        cloud_models.append("vision-cloud(235B)")

    if cloud_models:
        models_str += f", –æ–±–ª–∞—á–Ω—ã–µ: {', '.join(cloud_models)}"

    print(f"‚úÖ –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã: {models_str}")
    return True
