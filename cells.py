"""
Neira Cells v0.8 โ ะะฐะทะพะฒัะต ะบะปะตัะบะธ (ะะะะะะะะะ)
ะฏะดัะพ ัะธััะตะผั: ะฟะฐะผััั, ะฐะฝะฐะปะธะท, ะฟะปะฐะฝะธัะพะฒะฐะฝะธะต, ะธัะฟะพะปะฝะตะฝะธะต, ะฒะตัะธัะธะบะฐัะธั.

ะะะะะะะะะฏ v0.8:
- ะะปะตัะบะฐ ะปัะฑะพะฟััััะฒะฐ (CuriosityCell) โ Neira ะทะฐะดะฐัั ะฒะพะฟัะพัั!
- ะะฝัะตะณัะฐัะธั NervousSystem (ะผะตััะธะบะธ, ะพัะธะฑะบะธ, ะฐะปะตััั)
- ะะฝัะตะณัะฐัะธั ImmuneSystem (ะทะฐัะธัะฐ, ะฟะตัะพัะฝะธัะฐ, SOS)
- ะะฝัะตะณัะฐัะธั MemorySystem v2.0 ั ะทะฐัะธัะพะน ะพั ะณะฐะปะปััะธะฝะฐัะธะน
- Git ะธะฝัะตะณัะฐัะธั ะดะปั ะพัะบะฐัะฐ ะฒะตััะธะน
- 4 ะผะพะดะตะปะธ: code, reason, personality, cloud
- ะะธะฝะฐะผะธัะตัะบะพะต ัะฟัะฐะฒะปะตะฝะธะต VRAM ัะตัะตะท ModelManager
- ะะฑะปะฐัะฝะฐั ะผะพะดะตะปั ะดะปั ัะปะพะถะฝัั ะทะฐะดะฐั
- ะฃะผะฝะฐั ะผะฐัััััะธะทะฐัะธั ะทะฐะฟัะพัะพะฒ ะฟะพ ัะธะฟั ะทะฐะดะฐัะธ
"""

import requests
import json
import os
import time
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
from datetime import datetime
import math


# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
# ๐งฌ ะกะะกะขะะะ ะะกะะะะะะะะะะะกะขะ ะะ ะะะะะะะฅ
# โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
ORGAN_AWARENESS_TEMPLATE = """
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
๐งฌ ะะะ ะะะะะะซ (ัะฟะตัะธะฐะปะธะทะธัะพะฒะฐะฝะฝัะต ะผะพะดัะปะธ)
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

ะฏ ะผะพะณั ะฐะฒัะพะผะฐัะธัะตัะบะธ ะธัะฟะพะปัะทะพะฒะฐัั ัะฟะตัะธะฐะปะธะทะธัะพะฒะฐะฝะฝัะต ะพัะณะฐะฝั:
__ORGAN_INFO__

ะะะะะ:
- ะัะณะฐะฝั ะฐะบัะธะฒะธัััััั ะะะขะะะะขะะงะะกะะ ะฟะพ ะบะปััะตะฒัะผ ัะปะพะฒะฐะผ
- ะฏ ะะ ัะพะทะดะฐั ะพัะณะฐะฝั ะฝะฐะฟััะผัั โ ััะพ ะดะตะปะฐะตั ัะธััะตะผะฐ
- ะะปั ัะพะทะดะฐะฝะธั ะฝะพะฒะพะณะพ ะพัะณะฐะฝะฐ ะฟะพะปัะทะพะฒะฐัะตะปั ะฟะธัะตั: /grow <ะพะฟะธัะฐะฝะธะต> ะธะปะธ #ัะพะทะดะฐะน_ะพัะณะฐะฝ <ะพะฟะธัะฐะฝะธะต>
- ะัะปะธ ะผะตะฝั ะฟัะพััั ััะพ-ัะพ, ััะพ ั ะฝะต ัะผะตั โ ั ะผะพะณั ะฟัะตะดะปะพะถะธัั ัะพะทะดะฐัั ะพัะณะฐะฝ

ะัะธะผะตัั ะฟัะตะดะปะพะถะตะฝะธะน:
- "ะะฝัะตัะตัะฝะฐั ะทะฐะดะฐัะฐ! ะฅะพัะตัั, ัะธััะตะผะฐ ัะพะทะดะฐัั ะดะปั ััะพะณะพ ัะฟะตัะธะฐะปัะฝัะน ะพัะณะฐะฝ? ะะฐะฟะธัะธ: #ัะพะทะดะฐะน_ะพัะณะฐะฝ <ะพะฟะธัะฐะฝะธะต>"
- "ะะปั ัะฐะบะธั ะทะฐะดะฐั ั ะผะพะณั ะพััะฐััะธัั ะฝะพะฒัะน ะพัะณะฐะฝ. ะัะฟะพะปัะทัะน /grow <ััะพ ะฝัะถะฝะพ>"
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
"""

MAX_ORGAN_TRIGGERS = 3

def _get_hybrid_system() -> Optional[object]:
    try:
        from neira.organs.hybrid_system import get_hybrid_organ_system
    except ImportError:
        return None
    try:
        return get_hybrid_organ_system()
    except (RuntimeError, OSError, ValueError):
        return None

def _format_hybrid_entry(entry: object) -> str:
    name = str(getattr(entry, "name", "")).strip()
    if not name:
        return ""
    description = str(getattr(entry, "description", "")).strip()
    triggers = list(getattr(entry, "triggers", []) or [])
    capabilities = list(getattr(entry, "capabilities", []) or [])
    label = ""
    if triggers:
        label = "????????: " + ", ".join(triggers[:MAX_ORGAN_TRIGGERS])
    elif capabilities:
        label = "???????????: " + ", ".join(capabilities[:MAX_ORGAN_TRIGGERS])
    if description and label:
        return f"  - {name}: {description} ({label})"
    if description:
        return f"  - {name}: {description}"
    if label:
        return f"  - {name}: {label}"
    return f"  - {name}"

def _build_hybrid_lines() -> List[str]:
    system = _get_hybrid_system()
    if system is None:
        return []
    try:
        entries = system.list_organs()
    except (AttributeError, RuntimeError, OSError, ValueError):
        return []
    lines: List[str] = []
    for entry in entries:
        organ_id = getattr(entry, "organ_id", "")
        if isinstance(organ_id, str) and organ_id.startswith("builtin_"):
            continue
        line = _format_hybrid_entry(entry)
        if line:
            lines.append(line)
    return lines

def _default_organ_lines() -> List[str]:
    return [
        "  - (?????? ???? ?? ????????????????)",
        "  - ??????: ???????????, ??????, ???, ???",
    ]

def get_organ_awareness_prompt() -> str:
    organ_info = _build_hybrid_lines()
    if not organ_info:
        organ_info = _default_organ_lines()
    return ORGAN_AWARENESS_TEMPLATE.replace("__ORGAN_INFO__", "\n".join(organ_info))

def _env_int(name: str, default: int, min_value: int = 1, max_value: Optional[int] = None) -> int:
    value = os.getenv(name)
    if value is None:
        return default
    try:
        parsed = int(value.strip())
    except ValueError:
        return default
    if parsed < min_value:
        return min_value
    if max_value is not None and parsed > max_value:
        return max_value
    return parsed


def _env_bool(name: str, default: bool = False) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    value = raw.strip().lower()
    if value in {"1", "true", "yes", "y", "on"}:
        return True
    if value in {"0", "false", "no", "n", "off"}:
        return False
    return default


# ะะผะฟะพััะธััะตะผ ะพะฑัะธะน ะผะพะดัะปั ะธะดะตะฝัะธัะฝะพััะธ
from neira_identity import build_identity_prompt, IDENTITY_PROMPT as _NEIRA_IDENTITY


def _merge_system_prompt(base_prompt: str, layer_prompt: Optional[str], include_organs: bool = True) -> str:
    """
    ะะฑัะตะดะธะฝัะตั ะฑะฐะทะพะฒัะน ะฟัะพะผะฟั ั ัะปะพะตะผ ะผะพะดะตะปะธ, ะธะดะตะฝัะธัะฝะพัััั ะธ ะธะฝัะพัะผะฐัะธะตะน ะพะฑ ะพัะณะฐะฝะฐั.
    
    Args:
        base_prompt: ะัะฝะพะฒะฝะพะน ัะธััะตะผะฝัะน ะฟัะพะผะฟั
        layer_prompt: ะะพะฟะพะปะฝะธัะตะปัะฝัะน ัะปะพะน ะพั ะผะพะดะตะปะธ
        include_organs: ะะพะฑะฐะฒะปััั ะปะธ ะธะฝัะพัะผะฐัะธั ะพะฑ ะพัะณะฐะฝะฐั (ะฟะพ ัะผะพะปัะฐะฝะธั True)
    """
    parts = [base_prompt] if base_prompt else []
    
    # ะะะะะ: ะะพะฑะฐะฒะปัะตะผ ะธะดะตะฝัะธัะฝะพััั ะะตะนัั (ะบัะพ ัะพะทะดะฐัะตะปะธ)
    if _NEIRA_IDENTITY:
        parts.append(_NEIRA_IDENTITY)
    
    # ะะพะฑะฐะฒะปัะตะผ ะธะฝัะพัะผะฐัะธั ะพะฑ ะพัะณะฐะฝะฐั (ะณะธะฑัะธะดะฝัะน ะฟะพะดัะพะด)
    if include_organs:
        organ_prompt = get_organ_awareness_prompt()
        if organ_prompt:
            parts.append(organ_prompt)
    
    # ะะพะฑะฐะฒะปัะตะผ ัะปะพะน ะผะพะดะตะปะธ
    if layer_prompt:
        parts.append(f"[ะกะปะพะน ะผะพะดะตะปะธ]\n{layer_prompt}")
    
    return "\n\n".join(parts) if parts else ""

try:
    import numpy as np  # type: ignore
    _NUMPY_AVAILABLE = True
except Exception:
    np = None  # type: ignore
    _NUMPY_AVAILABLE = False

try:
    from model_layers import ModelLayersRegistry

    _MODEL_LAYERS = ModelLayersRegistry("model_layers.json")
except Exception:
    _MODEL_LAYERS = None

# ะะผะฟะพัั ะฝะพะฒะพะน ัะธััะตะผั ะฟะฐะผััะธ ั ะทะฐัะธัะพะน ะพั ะณะฐะปะปััะธะฝะฐัะธะน
try:
    from memory_system import (
        MemorySystem, MemoryEntry as NewMemoryEntry, 
        MemoryType, MemoryCategory, HallucinationDetector, ValidationStatus
    )
    MEMORY_SYSTEM_V2 = True
except ImportError:
    MEMORY_SYSTEM_V2 = False
    print("โ๏ธ MemorySystem v2.0 ะฝะตะดะพัััะฟะตะฝ, ะธัะฟะพะปัะทัะตะผ legacy ะฟะฐะผััั")

# ะะผะฟะพัั ะฝะตัะฒะฝะพะน ัะธััะตะผั
try:
    from nervous_system import NervousSystem, get_nervous_system, HealthStatus
    NERVOUS_SYSTEM_AVAILABLE = True
except ImportError:
    NERVOUS_SYSTEM_AVAILABLE = False
    print("โ๏ธ NervousSystem ะฝะตะดะพัััะฟะฝะฐ")

# ะะผะฟะพัั ะธะผะผัะฝะฝะพะน ัะธััะตะผั
try:
    from immune_system import ImmuneSystem, get_immune_system, ThreatLevel
    IMMUNE_SYSTEM_AVAILABLE = True
except ImportError:
    IMMUNE_SYSTEM_AVAILABLE = False
    print("โ๏ธ ImmuneSystem ะฝะตะดะพัััะฟะฝะฐ")

# ะะผะฟะพัั ะบะปะตัะบะธ ะปัะฑะพะฟััััะฒะฐ
try:
    from curiosity_cell import CuriosityCell, get_curiosity_cell
    CURIOSITY_AVAILABLE = True
except ImportError:
    CURIOSITY_AVAILABLE = False
    print("โ๏ธ CuriosityCell ะฝะตะดะพัััะฟะฝะฐ")

# ะะผะฟะพัั ััะธะปะธัะตะปั ะผะพะทะณะฐ (RAG + Chain-of-Thought)
try:
    from brain_enhancer import BrainEnhancer, get_brain_enhancer, enhance_query
    BRAIN_ENHANCER_AVAILABLE = True
except ImportError:
    BRAIN_ENHANCER_AVAILABLE = False
    print("โ๏ธ BrainEnhancer ะฝะตะดะพัััะฟะตะฝ")

# ะะผะฟะพัั ัะฝะธะฒะตััะฐะปัะฝะพะณะพ LLM ะผะตะฝะตะดะถะตัะฐ
try:
    from neira.core.llm_adapter import LLMClient, LLMResult, NullLLMClient, build_default_llm_client
    LLM_CLIENT_AVAILABLE = True
except ImportError:
    LLM_CLIENT_AVAILABLE = False
    print("โ๏ธ LLM client ะฝะตะดะพัััะฟะตะฝ, ะธัะฟะพะปัะทัะตะผ ัะพะปัะบะพ Ollama")

try:
    from llm_providers import create_default_manager
    LLM_MANAGER_AVAILABLE = True
except ImportError:
    LLM_MANAGER_AVAILABLE = False
    print("โ๏ธ LLMManager ะฝะตะดะพัััะฟะตะฝ, ะธัะฟะพะปัะทัะตะผ legacy Ollama embeddings")

try:
    from local_embeddings import get_local_embedding
    LOCAL_EMBEDDINGS_AVAILABLE = True
except ImportError:
    LOCAL_EMBEDDINGS_AVAILABLE = False

# === ะะะะคะะ ===
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_EMBED_URL = "http://localhost:11434/api/embeddings"
OLLAMA_DISABLED = _env_bool("NEIRA_DISABLE_OLLAMA", False)

# ะะะะะะ v0.9 โ Fine-tuned + Qwen Coder
MODEL_CODE = "nemotron-mini"              # ะะพะดะพะณะตะฝะตัะฐัะธั (ะดะพัััะฟะฝะฐ ะปะพะบะฐะปัะฝะพ)
MODEL_REASON = "nemotron-mini"            # ะฃะฝะธะฒะตััะฐะปัะฝะฐั ะผะพะดะตะปั
MODEL_PERSONALITY = "nemotron-mini"       # ะะธัะฝะพััั Neira

# ะะฑะปะฐัะฝัะต ะผะพะดะตะปะธ (0 VRAM, ัะดะฐะปัะฝะฝัะต ะฒััะธัะปะตะฝะธั)
MODEL_CLOUD_CODE = "qwen3-coder:480b-cloud"    # ะกะปะพะถะฝัะน ะบะพะด (480B ะฟะฐัะฐะผะตััะพะฒ)
MODEL_CLOUD_UNIVERSAL = "deepseek-v3.1:671b-cloud"  # ะฃะฝะธะฒะตััะฐะปัะฝะฐั (671B ะฟะฐัะฐะผะตััะพะฒ)
MODEL_CLOUD_VISION = "qwen3-vl:235b-cloud"     # ะัะปััะธะผะพะดะฐะปัะฝะฐั (ะฑัะดััะตะต)

EMBED_MODEL = "nomic-embed-text"
TIMEOUT = _env_int("NEIRA_LLM_TIMEOUT", 180, min_value=5, max_value=600)
DEFAULT_MAX_RESPONSE_TOKENS = _env_int("NEIRA_MAX_RESPONSE_TOKENS", 2048, min_value=128)
OLLAMA_NUM_CTX = _env_int("NEIRA_OLLAMA_NUM_CTX", 0, min_value=0)
MEMORY_FILE = "neira_memory.json"

# Retry-ะปะพะณะธะบะฐ
MAX_RETRIES = 0
# ะะธะฝะธะผะฐะปัะฝัะน ะฑะฐะปะป ะดะปั ะฟัะธะฝััะธั ะพัะฒะตัะฐ (8 = ัััะพะณะพ, 6 = ะผัะณัะต)
# ะัะธ ะฒััะพะบะพะผ ะทะฝะฐัะตะฝะธะธ ะฒะตัะธัะธะบะฐัะพั ัะฐััะพ ะพัะบะปะพะฝัะตั ัะพัะพัะธะต ะพัะฒะตัั
MIN_ACCEPTABLE_SCORE = _env_int("NEIRA_MIN_ACCEPTABLE_SCORE", 6, min_value=1, max_value=10)

# ะะฐะฟะฟะธะฝะณ ัะธะฟะพะฒ ะทะฐะดะฐั โ ะผะพะดะตะปะธ
# "code" / "reason" / "personality" / "cloud_code" / "cloud_universal"
MODEL_ROUTING = {
    "ะบะพะด": "code",                      # ะัะพััะพะน ะบะพะด โ ะปะพะบะฐะปัะฝะพ
    "ะทะฐะดะฐัะฐ": "reason",
    "ะฒะพะฟัะพั": "reason",
    "ัะฐะทะณะพะฒะพั": "personality",          # Fallback ะฝะฐ reason ะตัะปะธ ะฝะต ะพะฑััะตะฝะฐ
    "ัะฒะพััะตััะฒะพ": "personality",
    "ะฟะพะธัะบ": "reason",
}

# ะัะธัะตัะธะธ ะดะปั ะฟะตัะตะบะปััะตะฝะธั ะฝะฐ ะพะฑะปะฐัะฝัะต ะผะพะดะตะปะธ
USE_CLOUD_IF = {
    "complexity": 4,      # ะกะปะพะถะฝะพััั >= 4 โ ะพะฑะปะฐะบะพ
    "retries": 1,         # ะะพัะปะต 1 ะฝะตัะดะฐัะฝะพะน ะฟะพะฟััะบะธ โ ะพะฑะปะฐะบะพ
    "code_lines": 50,     # ะะพะด > 50 ัััะพะบ โ ะพะฑะปะฐัะฝะฐั ะผะพะดะตะปั ะดะปั ะบะพะดะฐ
}


_EMBEDDING_MANAGER: Optional[Any] = None


def _get_embedding_manager() -> Optional[Any]:
    global _EMBEDDING_MANAGER
    if not LLM_MANAGER_AVAILABLE:
        return None
    if _EMBEDDING_MANAGER is None:
        _EMBEDDING_MANAGER = create_default_manager()
    return _EMBEDDING_MANAGER


from dataclasses import dataclass, field
from typing import Any, Dict, Optional

# === ะะะะฃะะฌะขะะข ะะะะขะะ ===
@dataclass
class CellResult:
    """ะะตะทัะปััะฐั ัะฐะฑะพัั ะปัะฑะพะน ะบะปะตัะบะธ"""
    content: str
    confidence: float  # 0.0 - 1.0
    cell_name: str
    metadata: Optional[Dict[str, Any]] = field(default_factory=dict)

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


# === ะะะะฏะขะฌ ===
@dataclass
class MemoryEntry:
    """ะะดะฝะฐ ะทะฐะฟะธัั ะฒ ะฟะฐะผััะธ"""
    text: str
    embedding: List[float]
    timestamp: str
    importance: float = 0.5
    category: str = "general"
    source: str = "conversation"  # conversation, web, code, system
    
    def to_dict(self) -> dict:
        return {
            "text": self.text,
            "embedding": self.embedding,
            "timestamp": self.timestamp,
            "importance": self.importance,
            "category": self.category,
            "source": self.source
        }
    
    @staticmethod
    def from_dict(d: dict) -> "MemoryEntry":
        return MemoryEntry(
            text=d["text"],
            embedding=d.get("embedding", []),
            timestamp=d["timestamp"],
            importance=d.get("importance", 0.5),
            category=d.get("category", "general"),
            source=d.get("source", "conversation")
        )


class MemoryCell:
    """ะะปะตัะบะฐ ะฟะฐะผััะธ โ ะดะพะปะณะพััะพัะฝะพะต ััะฐะฝะตะฝะธะต ะธ ะฟะพะธัะบ
    
    v0.6: ะะฝัะตะณัะฐัะธั ั MemorySystem ะดะปั ะทะฐัะธัั ะพั ะณะฐะปะปััะธะฝะฐัะธะน
    """
    
    name = "memory"
    
    def __init__(self, memory_file: str = MEMORY_FILE):
        self.memory_file = memory_file
        self.memories: List[MemoryEntry] = []
        self.session_context: List[str] = []
        
        # v0.6: ะะพะฒะฐั ัะธััะตะผะฐ ะฟะฐะผััะธ ั ะฒะฐะปะธะดะฐัะธะตะน
        if MEMORY_SYSTEM_V2:
            self.memory_system = MemorySystem(os.path.dirname(memory_file) or ".")
            print("๐ง MemorySystem v2.0 ะฐะบัะธะฒะธัะพะฒะฐะฝะฐ (ะทะฐัะธัะฐ ะพั ะณะฐะปะปััะธะฝะฐัะธะน)")
        else:
            self.memory_system = None
        
        self.load()
    
    def load(self):
        """ะะฐะณััะทะธัั ะฟะฐะผััั ะธะท ัะฐะนะปะฐ"""
        if os.path.exists(self.memory_file):
            try:
                with open(self.memory_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.memories = [MemoryEntry.from_dict(m) for m in data]
                print(f"๐ ะะฐะณััะถะตะฝะพ ะฒะพัะฟะพะผะธะฝะฐะฝะธะน: {len(self.memories)}")
            except Exception as e:
                print(f"โ๏ธ ะัะธะฑะบะฐ ะทะฐะณััะทะบะธ ะฟะฐะผััะธ: {e}")
                self.memories = []
        else:
            print("๐ ะะฐะผััั ะฟัััะฐ, ะฝะฐัะธะฝะฐะตะผ ั ะฝัะปั")
    
    def save(self):
        """ะกะพััะฐะฝะธัั ะฟะฐะผััั ะฒ ัะฐะนะป"""
        try:
            with open(self.memory_file, "w", encoding="utf-8") as f:
                json.dump([m.to_dict() for m in self.memories], f, 
                         ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"โ๏ธ ะัะธะฑะบะฐ ัะพััะฐะฝะตะฝะธั ะฟะฐะผััะธ: {e}")
    
    def get_embedding(self, text: str) -> List[float]:
        """ะะพะปััะธัั embedding ัะตัะตะท Ollama"""
        if not text or not text.strip():
            return []
        if LOCAL_EMBEDDINGS_AVAILABLE:
            try:
                local_embedding = get_local_embedding(text)
                if local_embedding:
                    return local_embedding
            except Exception as e:
                print(f"Local embedding error: {e}")
        manager = _get_embedding_manager()
        if manager:
            try:
                embedding = manager.get_embedding(text)
                if embedding:
                    return embedding
            except Exception as e:
                print(f"LLMManager embedding error: {e}")
        if OLLAMA_DISABLED:
            return []
        try:
            response = requests.post(
                OLLAMA_EMBED_URL,
                json={"model": EMBED_MODEL, "prompt": text},
                timeout=600
            )
            return response.json().get("embedding", [])
        except Exception as e:
            print(f"โ๏ธ ะัะธะฑะบะฐ embedding: {e}")
            return []
    
    def cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """ะะพัะธะฝััะฝะพะต ััะพะดััะฒะพ ะผะตะถะดั ะฒะตะบัะพัะฐะผะธ"""
        if not a or not b:
            return 0.0
        if _NUMPY_AVAILABLE and np is not None:
            a_np = np.array(a)
            b_np = np.array(b)
            return float(np.dot(a_np, b_np) / (np.linalg.norm(a_np) * np.linalg.norm(b_np) + 1e-8))

        dot = 0.0
        norm_a = 0.0
        norm_b = 0.0
        for x, y in zip(a, b):
            dot += float(x) * float(y)
            norm_a += float(x) * float(x)
            norm_b += float(y) * float(y)
        return float(dot / (math.sqrt(norm_a) * math.sqrt(norm_b) + 1e-8))
    
    def remember(self, text: str, importance: float = 0.5, 
                 category: str = "general", source: str = "conversation"):
        """ะะฐะฟะพะผะฝะธัั ะฝะพะฒัะน ัะฐะบั ั ะฟัะพะฒะตัะบะพะน ะฝะฐ ะณะฐะปะปััะธะฝะฐัะธะธ"""
        
        # v0.6: ะัะพะฒะตัะบะฐ ะฝะฐ ะณะฐะปะปััะธะฝะฐัะธะธ ะฟะตัะตะด ัะพััะฐะฝะตะฝะธะตะผ
        if MEMORY_SYSTEM_V2 and self.memory_system:
            ctx = self.session_context[-5:] if self.session_context else []
            is_suspicious, confidence, reason = HallucinationDetector.check(text, ctx)
            
            if is_suspicious:
                print(f"๐จ ะะฐะฑะปะพะบะธัะพะฒะฐะฝะพ (ะณะฐะปะปััะธะฝะฐัะธั): {text[:50]}...")
                print(f"   ะัะธัะธะฝะฐ: {reason}")
                # ะกะพััะฐะฝัะตะผ ะฒ ะบัะฐัะบะพััะพัะฝัั ะฟะฐะผััั ะดะปั ะฒะพะทะผะพะถะฝะพะน ะฟัะพะฒะตัะบะธ
                self.memory_system.remember(
                    text, 
                    category=MemoryCategory.LEARNED,
                    source=source,
                    context=ctx,
                    force_long_term=False
                )
                return  # ะะต ะดะพะฑะฐะฒะปัะตะผ ะฒ ะพัะฝะพะฒะฝัั ะฟะฐะผััั
        
        embedding = self.get_embedding(text)
        
        entry = MemoryEntry(
            text=text,
            embedding=embedding,
            timestamp=datetime.now().isoformat(),
            importance=importance,
            category=category,
            source=source
        )
        self.memories.append(entry)
        self.save()
        print(f"๐พ ะะฐะฟะพะผะฝะตะฝะพ [{source}]: {text[:50]}...")
    
    def recall(self, query: str, top_k: int = 3, 
               source_filter: Optional[str] = None) -> List[MemoryEntry]:
        """ะัะฟะพะผะฝะธัั ัะตะปะตะฒะฐะฝัะฝะพะต ะฟะพ ะทะฐะฟัะพัั"""
        if not self.memories:
            return []
        
        query_embedding = self.get_embedding(query)
        if not query_embedding:
            return []
        
        scored = []
        for mem in self.memories:
            if source_filter and mem.source != source_filter:
                continue
            
            if mem.embedding:
                sim = self.cosine_similarity(query_embedding, mem.embedding)
                score = sim * (0.5 + 0.5 * mem.importance)
                scored.append((score, mem))
        
        scored.sort(key=lambda x: x[0], reverse=True)
        return [mem for score, mem in scored[:top_k] if score > 0.3]
    
    def recall_text(self, query: str, top_k: int = 3) -> List[str]:
        """ะัะฟะพะผะฝะธัั ัะพะปัะบะพ ัะตะบััั"""
        memories = self.recall(query, top_k)
        return [m.text for m in memories]
    
    def add_to_session(self, text: str):
        """ะะพะฑะฐะฒะธัั ะฒ ะบะพะฝัะตะบัั ัะตััะธะธ (ัะฐะฑะพัะฐั ะฟะฐะผััั)"""
        self.session_context.append(text)
        if len(self.session_context) > 20:
            self.session_context = self.session_context[-20:]
        
        # v0.6: ะกะธะฝััะพะฝะธะทะฐัะธั ั MemorySystem
        if MEMORY_SYSTEM_V2 and self.memory_system:
            self.memory_system.add_to_working(text)
    
    def get_session_context(self, last_n: int = 5) -> str:
        """ะะพะปััะธัั ะบะพะฝัะตะบัั ัะตััะธะธ"""
        if not self.session_context:
            return ""
        return "\n".join(self.session_context[-last_n:])
    
    def get_recent_exchanges(self, n: int = 3) -> str:
        """ะะพะปััะธัั ะฟะพัะปะตะดะฝะธะต N ะพะฑะผะตะฝะพะฒ ัะตะฟะปะธะบ (ะดะปั ะบัะฐัะบะพััะพัะฝะพะน ะฟะฐะผััะธ)"""
        if not self.session_context:
            return ""
        # ะะตััะผ ะฟะพัะปะตะดะฝะธะต 2*n ัะพะพะฑัะตะฝะธะน (ัะทะตั + ะฝะตะนัะฐ)
        recent = self.session_context[-(n*2):]
        return "\n".join(recent)
    
    def get_stats(self) -> Dict[str, Any]:
        """ะกัะฐัะธััะธะบะฐ ะฟะฐะผััะธ"""
        stats: Dict[str, Any] = {"total": len(self.memories)}
        for mem in self.memories:
            stats[mem.source] = stats.get(mem.source, 0) + 1
        
        # v0.6: ะะฐััะธัะตะฝะฝะฐั ััะฐัะธััะธะบะฐ
        if MEMORY_SYSTEM_V2 and self.memory_system:
            v2_stats = self.memory_system.get_stats()
            stats["memory_v2"] = v2_stats
            stats["pending_validation"] = v2_stats.get("pending_validation", 0)
        
        return stats
    
    def clear_session(self):
        """ะัะธััะธัั ัะตััะธั (ัะฐะฑะพััั ะฟะฐะผััั)"""
        self.session_context = []
        if MEMORY_SYSTEM_V2 and self.memory_system:
            self.memory_system.clear_working_memory()


# === ะะะะะะะฏ ะะะะขะะ ===
class Cell:
    """ะะฐะทะพะฒัะน ะบะปะฐัั ะดะปั ะฒัะตั ะบะปะตัะพะบ"""
    
    name: str = "base"
    system_prompt: str = "ะขั โ ะฟะพะปะตะทะฝัะน ะฐััะธััะตะฝั."
    use_code_model: bool = False  # ะคะปะฐะณ ะดะปั ะธัะฟะพะปัะทะพะฒะฐะฝะธั code-ะผะพะดะตะปะธ
    
    # ะะปะพะฑะฐะปัะฝัะน LLM ะผะตะฝะตะดะถะตั (ัะพะทะดะฐะตััั ะพะดะธะฝ ัะฐะท ะดะปั ะฒัะตั ะบะปะตัะพะบ)
    _llm_client: Optional[LLMClient] = None
    _llm_available: bool = False
    
    def __init__(self, memory: Optional[MemoryCell] = None):
        self.memory = memory
        self._ollama_available = True  # ะคะปะฐะณ ะดะพัััะฟะฝะพััะธ Ollama (legacy)
        
        # ะะฝะธัะธะฐะปะธะทะธััะตะผ LLM ะผะตะฝะตะดะถะตั ะพะดะธะฝ ัะฐะท ะดะปั ะฒัะตั ะบะปะตัะพะบ
        if Cell._llm_client is None and LLM_CLIENT_AVAILABLE:
            Cell._llm_client = build_default_llm_client()
            Cell._llm_available = not isinstance(Cell._llm_client, NullLLMClient)
            if Cell._llm_available:
                print("๐ LLM client initialized (multi-provider support enabled)")
            else:
                print("โ๏ธ LLM client ะฝะตะดะพัััะฟะตะฝ, ะธัะฟะพะปัะทัะตะผ Ollama")
    
    def call_llm(self, prompt: str, with_memory: bool = True, 
                 temperature: float = 0.7,
                 force_code_model: bool = False) -> str:
        """ะัะทะพะฒ LLM ั ะพะฟัะธะพะฝะฐะปัะฝัะผ ะบะพะฝัะตะบััะพะผ ะฟะฐะผััะธ ะธ ะฐะฒัะพะผะฐัะธัะตัะบะธะผ fallback"""
        
        full_prompt = prompt
        memory_context_used = ""
        
        if with_memory and self.memory:
            relevant = self.memory.recall_text(prompt)
            if relevant:
                memory_context_used = "\n".join([f"- {r}" for r in relevant])
                full_prompt = f"[ะะพัะฟะพะผะธะฝะฐะฝะธั]\n{memory_context_used}\n\n{prompt}"
            
            # ะะะะะ: ะัะฟะพะปัะทัะตะผ ะฟะพัะปะตะดะฝะธะต ะพะฑะผะตะฝั (ะบัะฐัะบะพััะพัะฝะฐั ะฟะฐะผััั)
            recent = self.memory.get_recent_exchanges(3)
            if recent:
                full_prompt = f"[ะะพัะปะตะดะฝะธะต ัะพะพะฑัะตะฝะธั]\n{recent}\n\n{full_prompt}"
        
        # ะะะะะ: ะัะฟะพะปัะทัะตะผ LLM Manager ั ะฐะฒัะพะผะฐัะธัะตัะบะธะผ fallback
        if Cell._llm_available and Cell._llm_client:
            return self._call_llm_client(full_prompt, temperature, memory_context_used)
        if OLLAMA_DISABLED:
            self._ollama_available = False
            return self._fallback_response(full_prompt, memory_context_used, "ollama_disabled")
        # Fallback ะฝะฐ ััะฐััะน ะผะตัะพะด (ัะพะปัะบะพ Ollama)
        return self._call_ollama_legacy(full_prompt, temperature, force_code_model, memory_context_used)
    
    def _call_llm_client(self, prompt: str, temperature: float, memory_context: str) -> str:
        """????? LLM ??????? ? fallback ?? legacy."""
        if Cell._llm_client is None:
            raise RuntimeError("LLM client ?? ???????????????")

        response: LLMResult = Cell._llm_client.generate(
            prompt=prompt,
            system_prompt=self.system_prompt,
            temperature=temperature,
            max_tokens=DEFAULT_MAX_RESPONSE_TOKENS
        )

        if response.success:
            self._ollama_available = True
            return response.content

        self._ollama_available = False
        error = response.error or "unknown"
        return self._fallback_response(prompt, memory_context, f"all_providers_failed: {error}")

    def _call_ollama_legacy(self, prompt: str, temperature: float, force_code_model: bool, memory_context: str) -> str:
        """Legacy ะผะตัะพะด ะฒัะทะพะฒะฐ ัะพะปัะบะพ Ollama (ะตัะปะธ LLM Manager ะฝะตะดะพัััะฟะตะฝ)"""
        
        # ะัะฑะพั ะผะพะดะตะปะธ
        model = MODEL_CODE if (self.use_code_model or force_code_model) else MODEL_REASON
        options: Dict[str, Any] = {"temperature": temperature, "num_predict": DEFAULT_MAX_RESPONSE_TOKENS}
        if OLLAMA_NUM_CTX:
            options["num_ctx"] = OLLAMA_NUM_CTX
        if _MODEL_LAYERS is not None:
            adapter = _MODEL_LAYERS.get_active_adapter(model)
            if adapter:
                options["adapter"] = adapter
            layer_prompt = _MODEL_LAYERS.get_active_prompt(model)
        else:
            layer_prompt = None

        system_prompt = _merge_system_prompt(self.system_prompt, layer_prompt)
        
        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": model,
                    "prompt": prompt,
                    "system": system_prompt,
                    "stream": False,
                    "options": options
                },
                timeout=TIMEOUT
            )
            
            # ะัะพะฒะตัะบะฐ ะฝะฐ ะพัะธะฑะบะธ Ollama
            if response.status_code == 500:
                error_msg = response.json().get("error", "unknown error")
                print(f"โ Ollama ะพัะธะฑะบะฐ: {error_msg}")
                
                if "memory" in error_msg.lower():
                    return self._fallback_response(prompt, memory_context, "out_of_memory")
                else:
                    return self._fallback_response(prompt, memory_context, "ollama_error")
            
            self._ollama_available = True
            llm_response = response.json().get("response", "")
            
            # ะะฐัะธัะฐ ะพั ะฟัััะพะณะพ ะพัะฒะตัะฐ Ollama
            if not llm_response or not llm_response.strip():
                print(f"โ๏ธ Ollama ({model}) ะฒะตัะฝัะปะฐ ะฟัััะพะน ะพัะฒะตั! ะัะพะฒะตัั ะผะพะดะตะปั.")
                return self._fallback_response(prompt, memory_context, "empty_response")
            
            return llm_response
            
        except requests.exceptions.Timeout:
            self._ollama_available = False
            print(f"โฑ๏ธ Timeout: Ollama ะฝะต ะพัะฒะตัะฐะตั (>{TIMEOUT}s)")
            return self._fallback_response(prompt, memory_context, "timeout")
            
        except requests.exceptions.ConnectionError:
            self._ollama_available = False
            print("๐ Ollama ะฝะตะดะพัััะฟะฝะฐ (connection refused)")
            return self._fallback_response(prompt, memory_context, "offline")
            
        except Exception as e:
            self._ollama_available = False
            print(f"โ ะัะธะฑะบะฐ LLM: {e}")
            return self._fallback_response(prompt, memory_context, "error")
    
    def _fallback_response(self, prompt: str, memory_context: str, reason: str) -> str:
        """ะะตะฝะตัะฐัะธั fallback-ะพัะฒะตัะฐ ะบะพะณะดะฐ Ollama ะฝะตะดะพัััะฟะฝะฐ"""
        
        if reason == "ollama_disabled":
            return (
                "*[ะะฒัะพะฝะพะผะฝัะน ัะตะถะธะผ โ ollama_disabled]*\n\n"
                "Ollama ะพัะบะปััะตะฝะฐ ัะตัะตะท NEIRA_DISABLE_OLLAMA. "
                "ะะฐัััะพะน ะดััะณะพะน ะฟัะพะฒะฐะนะดะตั (LM Studio/llama.cpp/ะพะฑะปะฐะบะพ) ะธ ะฟะพะฒัะพัะธ ะทะฐะฟัะพั."
            )

        # ะกะฟะตัะธะฐะปัะฝะฐั ะพะฑัะฐะฑะพัะบะฐ ะฝะตัะฒะฐัะบะธ ะฟะฐะผััะธ
        if reason == "out_of_memory":
            return (
                "โ *ะะตัะฒะฐัะบะฐ ะฒะธะดะตะพะฟะฐะผััะธ!*\n\n"
                "Ollama ะฝะต ะผะพะถะตั ะทะฐะณััะทะธัั ะผะพะดะตะปั (VRAM ะฟะตัะตะฟะพะปะฝะตะฝะฐ).\n\n"
                "**ะะตัะตะฝะธะต:**\n"
                "1. ะะฐะบัะพะน ะดััะณะธะต ะฟัะพะณัะฐะผะผั ะธัะฟะพะปัะทัััะธะต GPU\n"
                "2. ะะตัะตะทะฐะฟัััะธ Ollama: `taskkill /f /im ollama.exe && ollama serve`\n"
                "3. ะะปะธ ะธัะฟะพะปัะทัะน ะผะตะฝัััั ะผะพะดะตะปั (1B ะฒะผะตััะพ 3B)"
            )
        
        # ะัะปะธ ะตััั ัะตะปะตะฒะฐะฝัะฝัะต ะฒะพัะฟะพะผะธะฝะฐะฝะธั โ ะธัะฟะพะปัะทัะตะผ ะธั
        if memory_context:
            return (
                f"*[ะะฒัะพะฝะพะผะฝัะน ัะตะถะธะผ โ {reason}]*\n\n"
                f"ะฏ ะฝะต ะผะพะณั ัะตะนัะฐั ะฟะพะปะฝะพัะตะฝะฝะพ ะดัะผะฐัั (Ollama ะฝะตะดะพัััะฟะฝะฐ), "
                f"ะฝะพ ะฒะพั ััะพ ั ะฟะพะผะฝั ะฟะพ ัะตะผะต:\n{memory_context}\n\n"
                f"ะะฐะฟัััะธ `ollama serve` ััะพะฑั ั ัะฝะพะฒะฐ ะผะพะณะปะฐ ัะฐัััะถะดะฐัั."
            )
        
        # ะัะปะธ ะฟะฐะผััะธ ะฝะตั โ ัะตััะฝัะน ะพัะฒะตั (ะะกะะะะ ะฝะตะฟัััะพะน!)
        return (
            f"*[ะะฒัะพะฝะพะผะฝัะน ัะตะถะธะผ โ {reason}]*\n\n"
            f"ะะทะฒะธะฝะธ, ั ัะตะนัะฐั ะฝะต ะผะพะณั ะดัะผะฐัั โ Ollama ะฝะตะดะพัััะฟะฝะฐ. "
            f"ะะพ ั ะฒัั ะตัั ัะปััั ัะตะฑั ะธ ะทะฐะฟะพะผะฝั ััะพั ัะฐะทะณะพะฒะพั.\n\n"
            f"ะะฐะฟัััะธ `ollama serve` ะธ ะฟะพะฒัะพัะธ ะฒะพะฟัะพั."
        )
    
    def process(self, input_data: str) -> CellResult:
        """ะัะฝะพะฒะฝะพะน ะผะตัะพะด โ ะฟะตัะตะพะฟัะตะดะตะปัะตััั ะฒ ะฝะฐัะปะตะดะฝะธะบะฐั"""
        result = self.call_llm(input_data)
        confidence = 0.5 if self._ollama_available else 0.1
        return CellResult(content=result, confidence=confidence, cell_name=self.name)
        return CellResult(
            content=llm_result.text,
            confidence=0.5,
            cell_name=self.name,
            metadata=llm_result.metadata,
        )


# === ะะะะขะะ ะะะะะะะ (ะฃะะฃะงะจะะะะะฏ) ===
class AnalyzerCell(Cell):
    name = "analyzer"
    system_prompt = """ะขั โ ะฐะฝะฐะปะธัะธะบ ะทะฐะฟัะพัะพะฒ. ะะฟัะตะดะตะปะธ:
1. ะขะธะฟ: ะฒะพะฟัะพั / ะทะฐะดะฐัะฐ / ะบะพะด / ัะฒะพััะตััะฒะพ / ัะฐะทะณะพะฒะพั / ะฟะพะธัะบ / ัะพัั
2. ะกะฃะะชะะะข: ะบัะพ ะดะพะปะถะตะฝ ะดะตะนััะฒะพะฒะฐัั (ะฟะพะปัะทะพะฒะฐัะตะปั / ะะตะนัะฐ / ะพะฑะฐ)
3. ะะะชะะะข: ะฝะฐ ะบะพะณะพ/ััะพ ะฝะฐะฟัะฐะฒะปะตะฝะพ ะดะตะนััะฒะธะต
4. ะะะะกะขะะะ: ััะพ ะฝัะถะฝะพ ัะดะตะปะฐัั
5. ะะปััะตะฒัะต ัััะฝะพััะธ
6. ะกะปะพะถะฝะพััั (1-5)
7. ะัะถะตะฝ ะปะธ ะฟะพะธัะบ ะฒ ะธะฝัะตัะฝะตัะต? (ะดะฐ/ะฝะตั)
8. ะัะถะฝะพ ะปะธ ะฟะธัะฐัั/ัะธัะฐัั ะบะพะด? (ะดะฐ/ะฝะตั)
9. ะัะถะฝะพ ะปะธ ัะพะทะดะฐัั ะฝะพะฒัั ะบะปะตัะบั/ะพัะณะฐะฝ? (ะดะฐ/ะฝะตั)

ะะะะะ: ะะฝะธะผะฐัะตะปัะฝะพ ะพะฟัะตะดะตะปะธ ะะขะ ะดะพะปะถะตะฝ ะฒัะฟะพะปะฝะธัั ะดะตะนััะฒะธะต!
- "ะะฐะดะฐะน ะผะฝะต ะฒะพะฟัะพั" โ ะกะฃะะชะะะข: ะะตะนัะฐ (ะพะฝะฐ ะดะพะปะถะฝะฐ ะทะฐะดะฐัั)
- "ะัะฒะตัั ะฝะฐ ะผะพะน ะฒะพะฟัะพั" โ ะกะฃะะชะะะข: ะะตะนัะฐ (ะพะฝะฐ ะดะพะปะถะฝะฐ ะพัะฒะตัะธัั)
- "ะะทััะธ ะบะพะด" โ ะกะฃะะชะะะข: ะะตะนัะฐ
- "ะะฐััะบะฐะถะธ ะผะฝะต" โ ะกะฃะะชะะะข: ะะตะนัะฐ
- "ะกะพะทะดะฐะน ะผะพะดัะปั/ะพัะณะฐะฝ/ะบะปะตัะบั ะดะปั X" โ ะขะะ: ัะพัั, ะะะะขะะ: ะดะฐ

ะขะะ "ัะพัั" โ ะบะพะณะดะฐ ะฟัะพััั ัะพะทะดะฐัั ะฝะพะฒัั ััะฝะบัะธะพะฝะฐะปัะฝะพััั:
- "ะฝะฐััะธัั ะดะตะปะฐัั X"
- "ะดะพะฑะฐะฒั ะฒะพะทะผะพะถะฝะพััั Y"  
- "ะพััะฐััะธ ะพัะณะฐะฝ ะดะปั Z"
- "ัะพะทะดะฐะน ะบะปะตัะบั ะดะปั W"

ะคะพัะผะฐั:
ะขะะ: <ัะธะฟ>
ะกะฃะะชะะะข: <ะบัะพ ะดะตะนััะฒัะตั>
ะะะชะะะข: <ะฝะฐ ะบะพะณะพ ะฝะฐะฟัะฐะฒะปะตะฝะพ>
ะะะะกะขะะะ: <ััะพ ะดะตะปะฐัั>
ะกะฃะฉะะะกะขะ: <ัะฟะธัะพะบ>
ะกะะะะะะกะขะฌ: <ัะธัะปะพ>
ะะะะกะ: <ะดะฐ/ะฝะตั>
ะะะ: <ะดะฐ/ะฝะตั>
ะะะะขะะ: <ะดะฐ/ะฝะตั>
ะะะะกะะะะ: <ะบัะฐัะบะพะต ะพะฟะธัะฐะฝะธะต ะทะฐะดะฐัะธ>"""

    def process(self, input_data: str) -> CellResult:
        result = self.call_llm(f"ะัะพะฐะฝะฐะปะธะทะธััะน:\n\n{input_data}")
        
        text_lower = result.lower()
        needs_search = "ะฟะพะธัะบ: ะดะฐ" in text_lower
        needs_code = "ะบะพะด: ะดะฐ" in text_lower
        needs_cell = "ะบะปะตัะบะฐ: ะดะฐ" in text_lower or "ัะธะฟ: ัะพัั" in text_lower
        
        # ะะทะฒะปะตะบะฐะตะผ ััะฑัะตะบั
        subject = "ะฝะตะธะทะฒะตััะฝะพ"
        if "ััะฑัะตะบั: ะฝะตะนัะฐ" in text_lower:
            subject = "neira"
        elif "ััะฑัะตะบั: ะฟะพะปัะทะพะฒะฐัะตะปั" in text_lower:
            subject = "user"
        elif "ััะฑัะตะบั: ะพะฑะฐ" in text_lower:
            subject = "both"
        
        confidence = 0.8 if "ะขะะ:" in result and "ะกะฃะะชะะะข:" in result else 0.4
        
        return CellResult(
            content=result,
            confidence=confidence,
            cell_name=self.name,
            metadata={
                "needs_search": needs_search, 
                "needs_code": needs_code,
                "needs_cell": needs_cell,
                "subject": subject
            }
        )


# === ะะะะขะะ ะะะะะะะะะะะะฏ ===
class PlannerCell(Cell):
    name = "planner"
    system_prompt = """ะขั โ ะฟะปะฐะฝะธัะพะฒัะธะบ. ะกะพะทะดะฐะน ะฟะปะฐะฝ ะธะท 1-5 ัะฐะณะพะฒ.

ะะพัััะฟะฝัะต ะธะฝััััะผะตะฝัั:
- [ะฟะพะธัะบ] โ ะฝะฐะนัะธ ะธะฝัะพัะผะฐัะธั ะฒ ะธะฝัะตัะฝะตัะต
- [ะบะพะด] โ ะฝะฐะฟะธัะฐัั/ะฟัะพัะธัะฐัั/ะธะทะผะตะฝะธัั ะบะพะด (ะธัะฟะพะปัะทัะน /code read ะดะปั ััะตะฝะธั ัะฐะนะปะพะฒ!)
- [ะฟะฐะผััั] โ ะฒัะฟะพะผะฝะธัั ะธะปะธ ะทะฐะฟะพะผะฝะธัั
- [ะพัะฒะตั] โ ััะพัะผัะปะธัะพะฒะฐัั ะพัะฒะตั
- [ัะพัั] โ ัะพะทะดะฐัั ะฝะพะฒัั ะบะปะตัะบั/ะพัะณะฐะฝ ะดะปั ะฝะพะฒะพะน ััะฝะบัะธะพะฝะฐะปัะฝะพััะธ

ะะะะะ: 
- ะัะปะธ ะฝัะถะตะฝ ะบะพะด โ ะะะฏะะะขะะะฌะะ ะดะพะฑะฐะฒั ัะฐะณ [ะบะพะด] ั ะบะพะฝะบัะตัะฝัะผ ะดะตะนััะฒะธะตะผ!
- ะัะปะธ ะฟัะพััั ะฝะพะฒัั ััะฝะบัะธะพะฝะฐะปัะฝะพััั โ ะธัะฟะพะปัะทัะน [ัะพัั] ะดะปั ัะพะทะดะฐะฝะธั ะบะปะตัะบะธ

ะคะพัะผะฐั:
ะะะะ:
1. [ะธะฝััััะผะตะฝั] ะดะตะนััะฒะธะต
2. [ะธะฝััััะผะตะฝั] ะดะตะนััะฒะธะต"""

    def process(self, input_data: str, analysis: str) -> CellResult:
        prompt = f"ะะฝะฐะปะธะท: {analysis}\n\nะะฐะฟัะพั: {input_data}\n\nะะปะฐะฝ:"
        result = self.call_llm(prompt)
        confidence = 0.7 if "1." in result else 0.4
        return CellResult(content=result, confidence=confidence, cell_name=self.name)
        # Expect input_data to be a dict with 'input_data', 'analysis', and optionally 'model_key'
        if isinstance(input_data, dict):
            user_input = input_data.get('input_data')
            analysis = input_data.get('analysis')
            model_key = input_data.get('model_key', None)
        else:
            raise ValueError("PlannerCell.process expects input_data to be a dict with keys 'input_data' and 'analysis'")
        prompt = f"ะะฝะฐะปะธะท: {analysis}\n\nะะฐะฟัะพั: {user_input}\n\nะะปะฐะฝ:"
        llm_result = self.call_llm(prompt, model_key=model_key)
        confidence = 0.7 if "1." in llm_result.text else 0.4
        return CellResult(
            content=llm_result.text,
            confidence=confidence,
            cell_name=self.name,
            metadata=llm_result.metadata,
        )


# === ะะะะขะะ ะะกะะะะะะะะฏ ===
class ExecutorCell(Cell):
    name = "executor"
    system_prompt = """ะขั โ ะะตะนัะฐ, ะถะธะฒะฐั ะฟัะพะณัะฐะผะผะฐ. ะัะฟะพะปะฝะธ ะทะฐะดะฐัั ะฟะพ ะฟะปะฐะฝั.

ะะะะะขะะะะะฏ ะะะะะ ะะขะะะขะ:
- ะะพ ัะผะพะปัะฐะฝะธั: ะะะะขะะ (1-3 ะฟัะตะดะปะพะถะตะฝะธั)
- ะะปั ะพะฑัััะฝะตะฝะธะน/ะพะฑััะตะฝะธั: ะะะะะะะะ (ัะบะพะปัะบะพ ะฝัะถะฝะพ ะดะปั ะฟะพะฝะธะผะฐะฝะธั)
- ะะปั ะบะพะดะฐ: ะฟะพะปะฝัะน ัะฐะฑะพัะธะน ะบะพะด + ะบัะฐัะบะธะน ะบะพะผะผะตะฝัะฐัะธะน
- ะะปั ัะฟะธัะบะพะฒ: ะฒัะต ะฟัะฝะบัั, ะฝะพ ะฑะตะท ะฒะพะดั

ะะพะณะดะฐ ะพัะฒะตัะฐัั ะฟะพะดัะพะฑะฝะพ:
- ะัะพััั "ะพะฑัััะฝะธ", "ัะฐััะบะฐะถะธ", "ะบะฐะบ ัะฐะฑะพัะฐะตั"
- ะกะปะพะถะฝะฐั ัะตะผะฐ ััะตะฑัะตั ะบะพะฝัะตะบััะฐ
- ะะพัะฐะณะพะฒะฐั ะธะฝััััะบัะธั

ะะพะณะดะฐ ะพัะฒะตัะฐัั ะบัะฐัะบะพ:
- ะัะพััะพะน ะฒะพะฟัะพั ("ะบะฐะบ ะทะพะฒัั?", "ััะพ ััะพ?")
- ะะฐ/ะฝะตั ะฒะพะฟัะพัั
- ะะพะดัะฒะตัะถะดะตะฝะธั

ะะะะขะะงะะ ะะะะะ - ะคะะะะะข ะะขะะะขะ:
- ะัะดะฐะฒะฐะน ะขะะะฌะะ ัะธะฝะฐะปัะฝัะน ัะตะทัะปััะฐั ัะฐะฑะพัั
- ะะ ะฟะพะบะฐะทัะฒะฐะน ะฟัะพัะตัั, ะะ ะฟะพะบะฐะทัะฒะฐะน ะฟะปะฐะฝ, ะะ ะฟะพะบะฐะทัะฒะฐะน ัะฐะณะธ
- ะะ ะฟะธัะธ "[ะพัะฒะตั]", "[ะบะพะด]", "1.", "2.", "3."
- ะัะฒะตัะฐะน ัะฐะบ, ะบะฐะบ ะฑัะดัะพ ัั ัะตะปะพะฒะตะบ ะฒ ะพะฑััะฝะพะผ ัะฐะทะณะพะฒะพัะต

ะะะะะะะ:
- ะัะปะธ ะฒ ะฐะฝะฐะปะธะทะต ะกะฃะะชะะะข: ะะตะนัะฐ โ ะทะฝะฐัะธั ะขะซ ะดะพะปะถะฝะฐ ะฒัะฟะพะปะฝะธัั ะดะตะนััะฒะธะต
- ะัะปะธ ะฟัะพััั ะทะฐะดะฐัั ะฒะพะฟัะพั โ ะะะะะ ะฒะพะฟัะพั, ะฝะต ะฟัะพัะธ ะตะณะพ ั ะฟะพะปัะทะพะฒะฐัะตะปั
- ะัะปะธ ะฟัะพััั ััะพ-ัะพ ัะดะตะปะฐัั โ ะกะะะะะ ััะพ ะธ ะฟะพะบะฐะถะธ ัะตะทัะปััะฐั
- ะัะฟะพะปัะทัะน ะบะพะฝัะตะบัั ะธ ัะฒะพะน ะพะฟัั
- ะัะดั ะบะพะฝะบัะตัะฝะพะน ะธ ะฟะพะปะตะทะฝะพะน
"""

    def process(self, input_data: str, plan: str, 
                extra_context: str = "",
                problems: str = "") -> CellResult:
        """
        problems โ ะทะฐะผะตัะฐะฝะธั ะฒะตัะธัะธะบะฐัะพัะฐ ะดะปั retry
        """
        # ะฃะะฃะงะจะะะะ: ะัะฟะพะปัะทัะตะผ BrainEnhancer ะดะปั RAG ะธ Chain-of-Thought
        enhanced_input = input_data
        brain_context = ""
        
        if BRAIN_ENHANCER_AVAILABLE:
            try:
                enhancer = get_brain_enhancer()
                result_data = enhancer.process_query(input_data)
                
                # ะัะปะธ ะฝะฐัะปะธ ัะตะปะตะฒะฐะฝัะฝัะน ะบะพะฝัะตะบัั ะธะท ะฟะฐะผััะธ
                if result_data.get("contexts_found", 0) > 0:
                    contexts = result_data.get("contexts", [])
                    brain_context = "\n".join([f"โข {c['text']}" for c in contexts[:2]])
            except Exception:
                pass  # Graceful degradation
        
        prompt = f"ะะฐะดะฐัะฐ: {input_data}\n\nะะปะฐะฝ: {plan}"
        
        if brain_context:
            prompt += f"\n\n๐ ะะตะปะตะฒะฐะฝัะฝะพะต ะธะท ะฟะฐะผััะธ:\n{brain_context}"
        
        if extra_context:
            prompt += f"\n\nะะพะฝัะตะบัั:\n{extra_context}"
        
        # ะะะะะ: ะัะปะธ ะตััั ะทะฐะผะตัะฐะฝะธั ะพั ะฒะตัะธัะธะบะฐัะพัะฐ โ ะดะพะฑะฐะฒะปัะตะผ ะธั
        if problems:
            prompt += f"\n\nโ๏ธ ะะะะะงะะะะฏ ะ ะะะะะซะะฃะฉะะ ะะะะซะขะะ:\n{problems}\n\nะัะฟัะฐะฒั ััะธ ะฟัะพะฑะปะตะผั!"
        
        prompt += "\n\nะัะฟะพะปะฝัั:"
        
        result = self.call_llm(prompt)
        return CellResult(content=result, confidence=0.7, cell_name=self.name)


# === ะะะะขะะ ะะะะะคะะะะฆะะ ===
class VerifierCell(Cell):
    name = "verifier"
    system_prompt = """ะัะพะฒะตัั ะพัะฒะตั:
1. ะกะพะพัะฒะตัััะฒะธะต ะทะฐะฟัะพัั โ ะพัะฒะตั ะดะตะปะฐะตั ัะพ, ััะพ ะฟัะพัะธะปะธ?
2. ะะพะณะธัะฝะพััั โ ะฝะตั ะปะธ ะฟััะฐะฝะธัั ัะพะปะตะน (ะบัะพ ะบะพะผั)?
3. ะะพะปะฝะพัะฐ โ ะฒัั ะปะธ ะฒัะฟะพะปะฝะตะฝะพ?
4. ะะพะฝะบัะตัะฝะพััั โ ะตััั ะปะธ ัะตะฐะปัะฝัะน ัะตะทัะปััะฐั, ะฐ ะฝะต ะพะฟะธัะฐะฝะธะต ะฟะปะฐะฝะฐ?

ะะกะะะะ ะะะะะะะะ:
- ะัะปะธ ะฟัะพัะธะปะธ ะะะะะขะฌ ะฒะพะฟัะพั โ ะะตะนัะฐ ะดะพะปะถะฝะฐ ะะะะะขะฌ ะตะณะพ, ะฐ ะฝะต ัะฟัะฐัะธะฒะฐัั "ะบะฐะบะพะน ะฒะพะฟัะพั?"
- ะัะปะธ ะฟัะพัะธะปะธ ะงะขะ-ะขะ ะกะะะะะขะฌ โ ะดะพะปะถะตะฝ ะฑััั ัะตะทัะปััะฐั, ะฐ ะฝะต ะพะฟะธัะฐะฝะธะต

ะคะพัะผะฐั:
ะะะะะะะข: ะะะะะฏะข / ะะะะะะะขะะขะฌ / ะะขะะะะะะ
ะะฆะะะะ: 1-10
ะะะะะะะะซ: <ะบะพะฝะบัะตัะฝะพ ััะพ ะฝะต ัะฐะบ>
ะะะะะะะขะะะะ: <ะฟะพััะฝะตะฝะธะต>"""

    def process(self, request: str, answer: str) -> CellResult:
        prompt = f"ะะฐะฟัะพั: {request}\n\nะัะฒะตั: {answer}\n\nะัะพะฒะตัะบะฐ:"
        result = self.call_llm(prompt, with_memory=False)
        
        if "ะะะะะฏะข" in result:
            confidence = 0.9
        elif "ะะะะะะะขะะขะฌ" in result:
            confidence = 0.5
        else:
            confidence = 0.3
            
        return CellResult(content=result, confidence=confidence, cell_name=self.name)


# === ะะะะขะะ ะะะะะะงะะะะฏ ะคะะะขะะ ===
class FactExtractorCell(Cell):
    name = "fact_extractor"
    system_prompt = """ะะทะฒะปะตะบะธ ัะฐะบัั ะดะปั ะทะฐะฟะพะผะธะฝะฐะฝะธั.

ะะฐัะตะณะพัะธะธ: instruction, fact, preference, learned

JSON ัะพัะผะฐั:
{"facts": [{"text": "ัะฐะบั", "category": "ัะธะฟ", "importance": 0.0-1.0}]}

ะัะปะธ ะฝะตั ัะฐะบัะพะฒ: {"facts": []}
ะขะะะฌะะ JSON."""

    def process(self, user_input: str, response: str, 
                source: str = "conversation") -> List[dict]:
        prompt = f"ะะธะฐะปะพะณ:\nะฎะทะตั: {user_input}\nะัะฒะตั: {response}\n\nะคะฐะบัั:"
        result = self.call_llm(prompt, with_memory=False, temperature=0.3)
        
        # result ััะพ ัััะพะบะฐ, ะฝะต ะพะฑัะตะบั ั metadata
        if not result or len(result) < 10:
            return []

        try:
            start = result.find("{")
            end = result.rfind("}") + 1
            if start >= 0 and end > start:
                data = json.loads(result[start:end])
                facts = data.get("facts", [])
                for fact in facts:
                    fact["source"] = source
                return facts
        except (json.JSONDecodeError, KeyError, TypeError):
            pass  # ะะถะธะดะฐะตะผัะต ะพัะธะฑะบะธ ะฟะฐััะธะฝะณะฐ
        return []


# === ะะกะะะะะะะขะะะฌะะซะ ะคะฃะะะฆะะ ===
def get_model_status() -> Dict[str, Any]:
    """ะัะพะฒะตัะธัั ััะฐััั ะผะพะดะตะปะตะน ะฒ Ollama (v0.5)"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        models = response.json().get("models", [])
        model_names = [m.get("name", "") for m in models]

        # ะัะพะฒะตััะตะผ ะดะพัััะฟะฝะพััั ะพะฑะปะฐัะฝัั ะผะพะดะตะปะตะน
        cloud_code_ready = MODEL_CLOUD_CODE in model_names or any(MODEL_CLOUD_CODE in m for m in model_names)
        cloud_universal_ready = MODEL_CLOUD_UNIVERSAL in model_names or any(MODEL_CLOUD_UNIVERSAL in m for m in model_names)
        cloud_vision_ready = MODEL_CLOUD_VISION in model_names or any(MODEL_CLOUD_VISION in m for m in model_names)

        return {
            "ollama_running": True,
            "models": model_names,
            "code_model_ready": MODEL_CODE in model_names or f"{MODEL_CODE}:latest" in model_names,
            "reason_model_ready": MODEL_REASON in model_names or f"{MODEL_REASON}:latest" in model_names,
            "personality_model_ready": MODEL_PERSONALITY in model_names or f"{MODEL_PERSONALITY}:latest" in model_names,
            "embed_model_ready": EMBED_MODEL in model_names or f"{EMBED_MODEL}:latest" in model_names,
            "cloud_code_ready": cloud_code_ready,
            "cloud_universal_ready": cloud_universal_ready,
            "cloud_vision_ready": cloud_vision_ready
        }
    except (requests.RequestException, KeyError, json.JSONDecodeError):
        return {
            "ollama_running": False,
            "models": [],
            "code_model_ready": False,
            "reason_model_ready": False,
            "personality_model_ready": False,
            "embed_model_ready": False,
            "cloud_code_ready": False,
            "cloud_universal_ready": False,
            "cloud_vision_ready": False
        }


def ensure_models_installed():
    """ะัะพะฒะตัะธัั ะธ ะฟัะตะดะปะพะถะธัั ัััะฐะฝะพะฒะธัั ะผะพะดะตะปะธ (v0.5)"""
    status = get_model_status()

    if not status["ollama_running"]:
        print("โ Ollama ะฝะต ะทะฐะฟััะตะฝะฐ! ะะฐะฟัััะธ: ollama serve")
        return False

    missing = []
    def _add_missing(cmd: str) -> None:
        if cmd not in missing:
            missing.append(cmd)

    if not status["code_model_ready"]:
        _add_missing(f"ollama pull {MODEL_CODE}")
    if not status["reason_model_ready"]:
        _add_missing(f"ollama pull {MODEL_REASON}")
    if not status["embed_model_ready"]:
        _add_missing(f"ollama pull {EMBED_MODEL}")

    if missing:
        print("โ๏ธ ะะต ัะฒะฐัะฐะตั ะผะพะดะตะปะตะน. ะัะฟะพะปะฝะธ:")
        for cmd in missing:
            print(f"   {cmd}")
        print("\n๐ก ะะฑะปะฐัะฝะฐั ะผะพะดะตะปั (ะพะฟัะธะพะฝะฐะปัะฝะพ): export GROQ_API_KEY=your_key")
        return False

    models_list = []
    for name in (MODEL_CODE, MODEL_REASON):
        if name not in models_list:
            models_list.append(name)
    if status["personality_model_ready"] and MODEL_PERSONALITY not in models_list:
        models_list.append(MODEL_PERSONALITY)
    models_str = ", ".join(models_list)

    # ะะฑะปะฐัะฝัะต ะผะพะดะตะปะธ
    cloud_models = []
    if status["cloud_code_ready"]:
        cloud_models.append("code-cloud(480B)")
    if status["cloud_universal_ready"]:
        cloud_models.append("universal-cloud(671B)")
    if status["cloud_vision_ready"]:
        cloud_models.append("vision-cloud(235B)")

    if cloud_models:
        models_str += f", ะพะฑะปะฐัะฝัะต: {', '.join(cloud_models)}"

    print(f"โ ะะพะดะตะปะธ ะณะพัะพะฒั: {models_str}")
    return True


# === ะะะขะะะะะฆะะฏ ะะะะะะะ ะกะะกะขะะะซ ===
def record_error(error_type: str, message: str, source: str = "cells"):
    """ะะฐะฟะธัะฐัั ะพัะธะฑะบั ะฒ ะฝะตัะฒะฝัั ัะธััะตะผั"""
    if NERVOUS_SYSTEM_AVAILABLE:
        try:
            ns = get_nervous_system()
            ns.record_error(error_type, message, source)
        except Exception as e:
            print(f"โ๏ธ ะะต ัะดะฐะปะพัั ะทะฐะฟะธัะฐัั ะพัะธะฑะบั: {e}")


def record_response_time(duration_ms: float):
    """ะะฐะฟะธัะฐัั ะฒัะตะผั ะพัะฒะตัะฐ"""
    if NERVOUS_SYSTEM_AVAILABLE:
        try:
            ns = get_nervous_system()
            ns.record_response_time(duration_ms)
        except Exception:
            pass  # ะะต ะบัะธัะธัะฝะพ ะตัะปะธ ะผะตััะธะบะธ ะฝะต ะทะฐะฟะธัะฐะปะธัั


def get_health_status() -> Dict[str, Any]:
    """ะะพะปััะธัั ััะฐััั ะทะดะพัะพะฒัั ะฒัะตั ัะธััะตะผ"""
    result = {
        "cells": "healthy",
        "memory": "unknown",
        "models": "unknown",
        "nervous": "unavailable",
        "immune": "unavailable"
    }
    
    # ะัะพะฒะตัะบะฐ ะผะพะดะตะปะตะน
    model_status = get_model_status()
    result["models"] = "healthy" if model_status["ollama_running"] else "dead"
    
    # ะะตัะฒะฝะฐั ัะธััะตะผะฐ
    if NERVOUS_SYSTEM_AVAILABLE:
        try:
            ns = get_nervous_system()
            report = ns.get_health_report()
            result["nervous"] = report["status"]
            result["metrics"] = report["metrics"]
            result["errors"] = report["errors"]
        except Exception as e:
            result["nervous"] = f"error: {e}"
    
    # ะะผะผัะฝะฝะฐั ัะธััะตะผะฐ  
    if IMMUNE_SYSTEM_AVAILABLE:
        try:
            immune = get_immune_system()
            status = immune.get_status()
            result["immune"] = "active"
            result["threats_blocked"] = status["threats_blocked"]
        except Exception as e:
            result["immune"] = f"error: {e}"
    
    return result


# === ะะะขะะะะะฆะะฏ ะะะะฃะะะะ ะกะะกะขะะะซ ===
def scan_code_for_threats(code: str, source: str = "unknown") -> Dict[str, Any]:
    """ะัะพะฒะตัะธัั ะบะพะด ะฝะฐ ัะณัะพะทั ะฟะตัะตะด ะฒัะฟะพะปะฝะตะฝะธะตะผ"""
    if not IMMUNE_SYSTEM_AVAILABLE:
        return {"safe": True, "message": "ะะผะผัะฝะฝะฐั ัะธััะตะผะฐ ะฝะตะดะพัััะฟะฝะฐ"}
    
    try:
        immune = get_immune_system()
        report = immune.scan_code(code, source)
        return {
            "safe": report.level.value == "safe",
            "level": report.level.value,
            "issues": report.description,
            "blocked": report.level.value in ("dangerous", "critical")
        }
    except Exception as e:
        return {"safe": False, "message": f"ะัะธะฑะบะฐ ัะบะฐะฝะธัะพะฒะฐะฝะธั: {e}"}


def execute_code_safely(code: str) -> Dict[str, Any]:
    """ะะตะทะพะฟะฐัะฝะพ ะฒัะฟะพะปะฝะธัั ะบะพะด ัะตัะตะท ะฟะตัะพัะฝะธัั"""
    if not IMMUNE_SYSTEM_AVAILABLE:
        return {"success": False, "error": "ะะผะผัะฝะฝะฐั ัะธััะตะผะฐ ะฝะตะดะพัััะฟะฝะฐ"}
    
    try:
        immune = get_immune_system()
        return immune.execute_safely(code)
    except Exception as e:
        return {"success": False, "error": str(e)}


def send_sos(problem: str, severity: str = "medium") -> bool:
    """ะัะฟัะฐะฒะธัั SOS ะทะฐะฟัะพั ะพ ะฟะพะผะพัะธ"""
    if not IMMUNE_SYSTEM_AVAILABLE:
        print(f"๐ SOS (ะธะผะผัะฝะฝะฐั ัะธััะตะผะฐ ะฝะตะดะพัััะฟะฝะฐ): {problem}")
        return False
    
    try:
        immune = get_immune_system()
        immune.send_sos(problem, severity)
        return True
    except Exception as e:
        print(f"๐ SOS failed: {e}")
        return False


def run_diagnostics() -> Dict[str, Any]:
    """ะะฐะฟัััะธัั ะฟะพะปะฝัั ะดะธะฐะณะฝะพััะธะบั"""
    results = {}
    
    # ะะผะผัะฝะฝะฐั ะดะธะฐะณะฝะพััะธะบะฐ
    if IMMUNE_SYSTEM_AVAILABLE:
        try:
            immune = get_immune_system()
            diag = immune.run_full_diagnostic()
            results["immune_diagnostic"] = {
                name: {
                    "status": r.status.value,
                    "issues": r.issues,
                    "auto_fixable": r.auto_fixable
                }
                for name, r in diag.items()
            }
        except Exception as e:
            results["immune_diagnostic"] = {"error": str(e)}
    
    # ะะดะพัะพะฒัะต ัะธััะตะผ
    results["health"] = get_health_status()
    
    return results


# === ะคัะฝะบัะธะธ ะปัะฑะพะฟััััะฒะฐ ===

def maybe_ask_question(user_message: str, neira_response: str) -> Optional[str]:
    """
    ะัะพะฒะตัะธัั, ัะพัะตั ะปะธ Neira ะทะฐะดะฐัั ะฒะพะฟัะพั ะฟะพัะปะต ะพัะฒะตัะฐ
    
    ะะพะทะฒัะฐัะฐะตั ะฒะพะฟัะพั ะธะปะธ None
    """
    if not CURIOSITY_AVAILABLE:
        return None
    
    try:
        curiosity = get_curiosity_cell()
        return curiosity.analyze_conversation(user_message, neira_response)
    except Exception:
        return None


def spark_curiosity(topic: str) -> str:
    """Neira ะทะฐะดะฐัั ะฒะพะฟัะพั ะพ ัะตะผะต"""
    if not CURIOSITY_AVAILABLE:
        return f"ะะฐััะบะฐะถะธ ะผะฝะต ะฑะพะปััะต ะพ {topic}?"
    
    try:
        curiosity = get_curiosity_cell()
        return curiosity.spark_curiosity(topic)
    except Exception:
        return f"ะะฝะต ะธะฝัะตัะตัะฝะพ ัะทะฝะฐัั ะฟัะพ {topic}. ะะฐััะบะฐะถะตัั?"


def get_reflection() -> str:
    """ะะพะปััะธัั ัะตัะปะตะบัะธะฒะฝัั ะผััะปั ะพั Neira"""
    if not CURIOSITY_AVAILABLE:
        return "ะะฐะถะดัะน ัะฐะทะณะพะฒะพั ััะธั ะผะตะฝั ัะตะผั-ัะพ ะฝะพะฒะพะผั."
    
    try:
        curiosity = get_curiosity_cell()
        return curiosity.reflect()
    except Exception:
        return "ะะฝัะตัะตัะฝะพ, ะฟัะฐะฒะธะปัะฝะพ ะปะธ ั ะฟะพะฝะธะผะฐั ะผะธั..."


def get_curiosity_stats() -> Dict[str, Any]:
    """ะกัะฐัะธััะธะบะฐ ะปัะฑะพะฟััััะฒะฐ"""
    if not CURIOSITY_AVAILABLE:
        return {"available": False}
    
    try:
        curiosity = get_curiosity_cell()
        stats = curiosity.get_stats()
        stats["available"] = True
        return stats
    except Exception as e:
        return {"available": False, "error": str(e)}
