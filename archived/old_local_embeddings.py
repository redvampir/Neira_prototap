"""
Local Embeddings v2.0 ‚Äî –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

–†–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é offline –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ—Ç –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤.
–í–∫–ª—é—á–∞–µ—Ç:
- N-gram —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (–±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥)
- –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ (–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
- –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- TF-–ø–æ–¥–æ–±–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
"""

import hashlib
import math
import os
import re
from functools import lru_cache
from typing import Any, Dict, List, Optional, Set, Tuple


_TRUE_VALUES = {"1", "true", "yes", "y", "on"}
_FALSE_VALUES = {"0", "false", "no", "n", "off"}


# ============== –°—Ç–æ–ø-—Å–ª–æ–≤–∞ ==============

RUSSIAN_STOPWORDS = {
    '–∏', '–≤', '–≤–æ', '–Ω–µ', '–Ω–∞', '—Å', '—Å–æ', '–∫', '–ø–æ', '–∑–∞', '–∏–∑', '–æ', '–æ–±', '–∞',
    '–Ω–æ', '–¥–∞', '—Ç–æ', '–∫–∞–∫', '—á—Ç–æ', '—ç—Ç–æ', '–≤—Å–µ', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–∏', '–º—ã', '–≤—ã',
    '—è', '—Ç—ã', '–µ–µ', '–µ–≥–æ', '–∏—Ö', '–±—ã', '–∂–µ', '–ª–∏', '—É', '–¥–ª—è', '–¥–æ', '–æ—Ç', '–ø—Ä–∏',
    '—Ç–∞–∫', '–±—ã–ª', '–±—ã–ª–∞', '–±—ã–ª–∏', '–±—ã—Ç—å', '–µ—Å—Ç—å', '–±—É–¥–µ—Ç', '–±—É–¥—É—Ç', '–∫–æ—Ç–æ—Ä—ã–π',
    '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—Ç–æ—Ç', '—Ç–∞', '—Ç–µ', '—Ç–æ–ª—å–∫–æ',
    '—É–∂–µ', '–µ—â–µ', '–µ—â—ë', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '—Ç–∞–º', '–∑–¥–µ—Å—å', '—Ç—É—Ç', '–æ—á–µ–Ω—å', '–º–æ–∂–Ω–æ',
    '–Ω—É–∂–Ω–æ', '–Ω–∞–¥–æ', '—á—Ç–æ–±—ã', '–µ—Å–ª–∏', '—Ç–æ–≥–¥–∞', '–ø–æ—Ç–æ–º', '–≤–æ—Ç', '–∏–ª–∏', '–Ω—É',
    '–≤–µ–¥—å', '–¥–∞–∂–µ', '—Ç–æ–∂–µ', '—Ç–∞–∫–∂–µ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–∫–∞–∂–¥—ã–π', '—Å–≤–æ–π',
    '—Å–≤–æ—è', '—Å–≤–æ–µ', '—Å–≤–æ–∏', '–≤–µ—Å—å', '–≤—Å—è', '–≤—Å—ë', '–≤—Å–µ–º', '–≤—Å–µ—Ö', '–º–æ–π', '–º–æ—è',
    '–º–æ–µ', '–Ω–∞—à', '–Ω–∞—à–∞', '–Ω–∞—à–µ', '–≤–∞—à', '–≤–∞—à–∞', '–≤–∞—à–µ', '—á–µ—Ä–µ–∑', '–º–µ–∂–¥—É', '–ø–æ–¥',
    '–Ω–∞–¥', '–ø–æ—Å–ª–µ', '–ø–µ—Ä–µ–¥', '–±–µ–∑', '–æ–∫–æ–ª–æ', '–≤–æ–∫—Ä—É–≥', '–∫—Ä–æ–º–µ', '–∫—Ç–æ', '–∫—É–¥–∞',
    '–æ—Ç–∫—É–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '—Å–∫–æ–ª—å–∫–æ', '–ø–æ–∫–∞', '—á–µ–º', '–Ω–∏–∫—Ç–æ', '–Ω–∏—á—Ç–æ',
    '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏–≥–¥–µ', '—Å–µ–±—è', '—Å–∞–º', '—Å–∞–º–∞', '—Å–∞–º–æ', '—Å–∞–º–∏', '–º–µ–Ω—è', '—Ç–µ–±—è',
    '–Ω–∞—Å', '–≤–∞—Å', '–µ–º—É', '–µ–π', '–∏–º', '–º–Ω–µ', '—Ç–µ–±–µ', '–Ω–∞–º', '–≤–∞–º', '–Ω–µ–≥–æ', '–Ω–µ–µ',
    '–Ω–∏—Ö', '–Ω–µ–º—É', '–Ω–µ–π', '–Ω–∏–º', '—Å–æ–±–æ–π', '–¥—Ä—É–≥–æ–π', '–¥—Ä—É–≥–∞—è', '–¥—Ä—É–≥–∏–µ',
}

ENGLISH_STOPWORDS = {
    'a', 'an', 'the', 'and', 'or', 'but', 'if', 'then', 'else', 'when', 'where',
    'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is',
    'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',
    'do', 'does', 'did', 'doing', 'will', 'would', 'could', 'should', 'may',
    'might', 'must', 'shall', 'can', 'for', 'of', 'to', 'from', 'in', 'out', 'on',
    'off', 'over', 'under', 'up', 'down', 'with', 'at', 'by', 'about', 'into',
    'through', 'during', 'before', 'after', 'above', 'below', 'between', 'because',
    'as', 'until', 'while', 'each', 'all', 'both', 'most', 'other', 'some', 'such',
    'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 'just',
    'also', 'now', 'how', 'why', 'any', 'here', 'there', 'it', 'its', 'i', 'me',
    'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',
    'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',
    'herself', 'they', 'them', 'their', 'theirs', 'themselves', 'more', 'few',
}

ALL_STOPWORDS = RUSSIAN_STOPWORDS | ENGLISH_STOPWORDS


# ============== –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ==============
# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º—ã –∑–∞–ø—Ä–æ—Å–∞

SEMANTIC_CATEGORIES = {
    'code': {
        'keywords': ['–∫–æ–¥', '—Ñ—É–Ω–∫—Ü–∏—è', '–∫–ª–∞—Å—Å', '–ø—Ä–æ–≥—Ä–∞–º–º', '—Å–∫—Ä–∏–ø—Ç', 'python', 'javascript',
                     'code', 'function', 'class', 'script', 'variable', '–ø–µ—Ä–µ–º–µ–Ω–Ω', '–º–µ—Ç–æ–¥',
                     'method', 'import', 'return', 'def ', 'async', 'await', 'loop', '—Ü–∏–∫–ª',
                     '–º–∞—Å—Å–∏–≤', 'array', 'list', 'dict', 'json', 'api', 'http', 'request'],
        'weight': 2.0
    },
    'ui': {
        'keywords': ['–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', 'ui', '–∫–Ω–æ–ø–∫', '—Ñ–æ—Ä–º', 'html', 'css', '–¥–∏–∑–∞–π–Ω', 'layout',
                     'button', 'input', 'canvas', '–∏–≥—Ä', 'game', '–≤–∏–∑—É–∞–ª–∏–∑', 'chart', '–≥—Ä–∞—Ñ–∏–∫',
                     'dashboard', '–¥–∞—à–±–æ—Ä–¥', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', 'calculator', '–∞–Ω–∏–º–∞—Ü', 'animation'],
        'weight': 2.0
    },
    'analysis': {
        'keywords': ['–∞–Ω–∞–ª–∏–∑', '–ø—Ä–æ–≤–µ—Ä—å', '–Ω–∞–π–¥–∏ –æ—à–∏–±–∫', '–æ–ø—Ç–∏–º–∏–∑', '—Ä–µ–≤—å—é', 'review',
                     'analyze', 'check', 'debug', '–∏—Å–ø—Ä–∞–≤', 'fix', 'bug', '–±–∞–≥', '–ø—Ä–æ–±–ª–µ–º',
                     'error', '–æ—à–∏–±–∫', 'exception', '—Ä–µ—Ñ–∞–∫—Ç–æ—Ä', 'refactor'],
        'weight': 1.8
    },
    'memory': {
        'keywords': ['–∑–∞–ø–æ–º–Ω–∏', '–ø–æ–º–Ω–∏', '–≤—Å–ø–æ–º–Ω–∏', '–ø–∞–º—è—Ç—å', 'remember', 'memory', 'forget',
                     '–∑–∞–±—É–¥—å', '–∑–Ω–∞–µ—à—å', '–∑–Ω–∞–ª', '—É–∑–Ω–∞–ª', 'learned', '—É—á–∏–ª', '–≤—ã—É—á–∏–ª'],
        'weight': 2.0
    },
    'creative': {
        'keywords': ['—Å–æ–∑–¥–∞–π', '–ø—Ä–∏–¥—É–º–∞–π', '–Ω–∞–ø–∏—à–∏', '—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π', 'generate', 'create',
                     'make', 'build', '–∏—Å—Ç–æ—Ä–∏—è', 'story', '—Å–∫–∞–∑–∫', '—Å—Ç–∏—Ö', 'poem'],
        'weight': 1.5
    },
    'question': {
        'keywords': ['—á—Ç–æ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '–∫—Ç–æ', '–∫–∞–∫–æ–π',
                     'what', 'how', 'why', 'when', 'where', 'who', 'which', 'explain',
                     '–æ–±—ä—è—Å–Ω–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏', 'tell'],
        'weight': 1.3
    },
    'action': {
        'keywords': ['—Å–¥–µ–ª–∞–π', '–≤—ã–ø–æ–ª–Ω–∏', '–∑–∞–ø—É—Å—Ç–∏', '–æ—Å—Ç–∞–Ω–æ–≤–∏', '—É–¥–∞–ª–∏', '–¥–æ–±–∞–≤—å',
                     '–∏–∑–º–µ–Ω–∏', '–æ–±–Ω–æ–≤–∏', 'do', 'run', 'execute', 'stop', 'delete', 'add',
                     'change', 'update', 'install', '—É—Å—Ç–∞–Ω–æ–≤–∏'],
        'weight': 1.7
    }
}


def _env_int(name: str, default: int, min_value: int = 1, max_value: Optional[int] = None) -> int:
    raw = os.getenv(name)
    if raw is None:
        return default
    try:
        value = int(raw.strip())
    except ValueError:
        return default
    if value < min_value:
        return min_value
    if max_value is not None and value > max_value:
        return max_value
    return value


def _env_bool(name: str, default: bool = False) -> bool:
    raw = os.getenv(name)
    if raw is None:
        return default
    value = raw.strip().lower()
    if value in _TRUE_VALUES:
        return True
    if value in _FALSE_VALUES:
        return False
    return default


def _env_tri_state(name: str) -> Optional[bool]:
    raw = os.getenv(name)
    if raw is None:
        return None
    value = raw.strip().lower()
    if value in _TRUE_VALUES:
        return True
    if value in _FALSE_VALUES:
        return False
    return None


_LOCAL_EMBED_DIM = _env_int("NEIRA_LOCAL_EMBED_DIM", 384, min_value=32, max_value=4096)
_LOCAL_MIN_NGRAM = _env_int("NEIRA_LOCAL_EMBED_MIN_NGRAM", 3, min_value=1, max_value=8)
_LOCAL_MAX_NGRAM = _env_int("NEIRA_LOCAL_EMBED_MAX_NGRAM", 5, min_value=1, max_value=10)
_LOCAL_MAX_NGRAMS = _env_int("NEIRA_LOCAL_EMBED_MAX_NGRAMS", 8000, min_value=128, max_value=200000)
_LOCAL_MAX_TEXT_CHARS = _env_int("NEIRA_LOCAL_EMBED_MAX_TEXT_CHARS", 20000, min_value=256, max_value=200000)

if _LOCAL_MAX_NGRAM < _LOCAL_MIN_NGRAM:
    _LOCAL_MAX_NGRAM = _LOCAL_MIN_NGRAM

# –í–∫–ª—é—á–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–∞)
_USE_SEMANTIC_FEATURES = _env_bool("NEIRA_LOCAL_EMBED_SEMANTIC", True)

# –†–∞–∑–º–µ—Ä LRU –∫—ç—à–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
_EMBED_CACHE_SIZE = _env_int("NEIRA_LOCAL_EMBED_CACHE_SIZE", 1000, min_value=100, max_value=10000)


def local_embeddings_enabled() -> bool:
    tri_state = _env_tri_state("NEIRA_LOCAL_EMBEDDINGS")
    if tri_state is not None:
        return tri_state
    return _env_bool("NEIRA_DISABLE_OLLAMA", False)


# ============== Text Processing ==============

def _normalize_text(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç: lowercase, —É–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã"""
    text = text.strip().lower()
    if not text:
        return ""
    return " ".join(text.split())


def _tokenize(text: str) -> List[str]:
    """–†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã"""
    # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –Ω–µ-–±—É–∫–≤–µ–Ω–Ω—ã–º —Å–∏–º–≤–æ–ª–∞–º, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –ª–∞—Ç–∏–Ω–∏—Ü—É
    tokens = re.findall(r'[a-zA-Z–∞-—è–ê-–Ø—ë–Å0-9]+', text.lower())
    return tokens


def _remove_stopwords(tokens: List[str]) -> List[str]:
    """–£–¥–∞–ª–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞"""
    return [t for t in tokens if t not in ALL_STOPWORDS and len(t) > 1]


def _simple_stem_russian(word: str) -> str:
    """
    –ü—Ä–æ—Å—Ç–æ–µ —Å—Ç–µ–º–º–∏–Ω–≥ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    –£–¥–∞–ª—è–µ—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
    """
    if len(word) < 4:
        return word
    
    # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
    endings = [
        '–æ—Å—Ç—å', '–µ–Ω–∏–µ', '–∞–Ω–∏–µ', '—Ç—å—Å—è', '–∞—é—Ç', '—É–µ—Ç', '–∏—Ç—å', '–∞—Ç—å', '—è—Ç—å',
        '–æ–≥–æ', '–µ–≥–æ', '–æ–º—É', '–µ–º—É', '—ã–º', '–∏–º', '–æ–π', '–µ–π', '—É—é', '—é—é',
        '—ã–µ', '–∏–µ', '—ã—Ö', '–∏—Ö', '–∞—è', '—è—è', '–æ–µ', '–µ–µ',
        '–æ–≤', '–µ–≤', '–∞–º', '—è–º', '–∞—Ö', '—è—Ö', '–º–∏',
        '—Å—è', '—Å—å', '–µ—Ç', '–∏—Ç', '—É—Ç', '—é—Ç', '–ª–∞', '–ª–æ', '–ª–∏',
    ]
    
    for ending in endings:
        if word.endswith(ending) and len(word) - len(ending) >= 2:
            return word[:-len(ending)]
    
    return word


def _simple_stem_english(word: str) -> str:
    """
    –ü—Ä–æ—Å—Ç–æ–π —Å—Ç–µ–º–º–∏–Ω–≥ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ
    Porter-–ø–æ–¥–æ–±–Ω—ã–π, –Ω–æ —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π
    """
    if len(word) < 4:
        return word
    
    # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
    endings = [
        'ment', 'ness', 'tion', 'sion', 'able', 'ible', 'ful', 'less',
        'ive', 'ous', 'ity', 'ing', 'ed', 'er', 'es', 'ly', 's'
    ]
    
    for ending in endings:
        if word.endswith(ending) and len(word) - len(ending) >= 2:
            return word[:-len(ending)]
    
    return word


def _stem_word(word: str) -> str:
    """–°—Ç–µ–º–º–∏–Ω–≥ —Å–ª–æ–≤–∞ (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞)"""
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –ø–æ –ø–µ—Ä–≤—ã–º –±—É–∫–≤–∞–º
    if re.match(r'[–∞-—è—ë]', word):
        return _simple_stem_russian(word)
    else:
        return _simple_stem_english(word)


def _extract_keywords(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    tokens = _tokenize(text)
    tokens = _remove_stopwords(tokens)
    stems = [_stem_word(t) for t in tokens]
    return stems


# ============== Semantic Features ==============

def _detect_categories(text: str) -> Dict[str, float]:
    """
    –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ–∫—Å—Ç–∞
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å {–∫–∞—Ç–µ–≥–æ—Ä–∏—è: score}
    """
    text_lower = text.lower()
    detected = {}
    
    for category, data in SEMANTIC_CATEGORIES.items():
        score = 0.0
        for keyword in data['keywords']:
            if keyword in text_lower:
                score += data['weight']
        if score > 0:
            detected[category] = score
    
    return detected


def _category_to_vector(categories: Dict[str, float], dim: int) -> List[float]:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –≤–µ–∫—Ç–æ—Ä —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    """
    vector = [0.0] * dim
    
    if not categories:
        return vector
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–º—ë–Ω –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
    for category, score in categories.items():
        digest = hashlib.blake2b(category.encode('utf-8'), digest_size=8).digest()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (distributed representation)
        for i in range(4):
            offset = i * 2
            index = int.from_bytes(digest[offset:offset+2], 'little') % dim
            sign = 1.0 if (digest[offset] & 1) == 0 else -1.0
            vector[index] += sign * score
    
    return vector


# ============== Main Embedding Function ==============

@lru_cache(maxsize=_EMBED_CACHE_SIZE)
def _get_embedding_cached(text: str) -> Optional[Tuple[float, ...]]:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç tuple –¥–ª—è hashable)"""
    result = _compute_embedding(text)
    return tuple(result) if result else None


def _compute_embedding(text: str) -> Optional[List[float]]:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
    
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
    1. N-gram —Ö—ç—à–∏ (–±–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥)
    2. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    3. TF-–ø–æ–¥–æ–±–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
    """
    normalized = _normalize_text(text)
    if not normalized:
        return None
    
    if len(normalized) > _LOCAL_MAX_TEXT_CHARS:
        normalized = normalized[:_LOCAL_MAX_TEXT_CHARS]
    
    dim = _LOCAL_EMBED_DIM
    
    # –ß–∞—Å—Ç—å 1: N-gram —Ö—ç—à–∏ (70% –≤–µ–∫—Ç–æ—Ä–∞)
    ngram_dim = int(dim * 0.7)
    ngram_vector = _compute_ngram_vector(normalized, ngram_dim)
    
    if _USE_SEMANTIC_FEATURES:
        # –ß–∞—Å—Ç—å 2: –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (15% –≤–µ–∫—Ç–æ—Ä–∞)
        category_dim = int(dim * 0.15)
        categories = _detect_categories(text)
        category_vector = _category_to_vector(categories, category_dim)
        
        # –ß–∞—Å—Ç—å 3: Keyword stems (15% –≤–µ–∫—Ç–æ—Ä–∞)
        keyword_dim = dim - ngram_dim - category_dim
        keywords = _extract_keywords(text)
        keyword_vector = _compute_keyword_vector(keywords, keyword_dim)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
        vector = ngram_vector + category_vector + keyword_vector
    else:
        # –¢–æ–ª—å–∫–æ n-gram
        vector = ngram_vector + [0.0] * (dim - ngram_dim)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    norm = math.sqrt(sum(v * v for v in vector))
    if norm <= 1e-12:
        return None
    
    return [v / norm for v in vector]


def _compute_ngram_vector(text: str, dim: int) -> List[float]:
    """–í—ã—á–∏—Å–ª–∏—Ç—å n-gram –≤–µ–∫—Ç–æ—Ä"""
    vector = [0.0] * dim
    count = 0
    length = len(text)
    min_n = _LOCAL_MIN_NGRAM
    max_n = _LOCAL_MAX_NGRAM
    max_ngrams = _LOCAL_MAX_NGRAMS
    
    for i in range(length):
        for n in range(min_n, max_n + 1):
            end = i + n
            if end > length:
                break
            ngram = text[i:end]
            digest = hashlib.blake2b(ngram.encode("utf-8"), digest_size=8).digest()
            index = int.from_bytes(digest[:4], "little") % dim
            sign = 1.0 if (digest[4] & 1) == 0 else -1.0
            
            # TF-–ø–æ–¥–æ–±–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ: –∫–æ—Ä–æ—Ç–∫–∏–µ n-gram –º–µ–Ω–µ–µ –≤–∞–∂–Ω—ã
            weight = math.log(n + 1)
            vector[index] += sign * weight
            
            count += 1
            if count >= max_ngrams:
                break
        if count >= max_ngrams:
            break
    
    return vector


def _compute_keyword_vector(keywords: List[str], dim: int) -> List[float]:
    """–í—ã—á–∏—Å–ª–∏—Ç—å –≤–µ–∫—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
    vector = [0.0] * dim
    
    if not keywords:
        return vector
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
    keyword_counts: Dict[str, int] = {}
    for kw in keywords:
        keyword_counts[kw] = keyword_counts.get(kw, 0) + 1
    
    # –•—ç—à–∏—Ä—É–µ–º –∫–∞–∂–¥–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ
    for keyword, count in keyword_counts.items():
        digest = hashlib.blake2b(keyword.encode('utf-8'), digest_size=8).digest()
        index = int.from_bytes(digest[:4], 'little') % dim
        sign = 1.0 if (digest[4] & 1) == 0 else -1.0
        
        # TF-–ø–æ–¥–æ–±–Ω–æ–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ
        weight = 1 + math.log(count)
        vector[index] += sign * weight
    
    return vector


def get_local_embedding(text: str) -> Optional[List[float]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—Å—Ç–∞
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    """
    if not local_embeddings_enabled():
        return None
    if not isinstance(text, str):
        return None
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
    cached = _get_embedding_cached(text)
    return list(cached) if cached else None


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    –í—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏
    
    –û–±–∞ –≤–µ–∫—Ç–æ—Ä–∞ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (–Ω–æ—Ä–º–∞ = 1)
    """
    if len(vec1) != len(vec2):
        return 0.0
    
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    return dot_product


def find_similar(
    query: str,
    candidates: List[Tuple[str, List[float]]],
    top_k: int = 5,
    threshold: float = 0.3
) -> List[Tuple[str, float]]:
    """
    –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–µ–∫—Å—Ç—ã
    
    Args:
        query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        candidates: –°–ø–∏—Å–æ–∫ (—Ç–µ–∫—Å—Ç, —ç–º–±–µ–¥–¥–∏–Ω–≥)
        top_k: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
    
    Returns:
        –°–ø–∏—Å–æ–∫ (—Ç–µ–∫—Å—Ç, score) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
    """
    query_emb = get_local_embedding(query)
    if not query_emb:
        return []
    
    results = []
    for text, emb in candidates:
        if emb:
            score = cosine_similarity(query_emb, emb)
            if score >= threshold:
                results.append((text, score))
    
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]


def batch_embeddings(texts: List[str]) -> List[Optional[List[float]]]:
    """
    –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    return [get_local_embedding(t) for t in texts]


def clear_embedding_cache():
    """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    _get_embedding_cached.cache_clear()


def get_cache_stats() -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫—ç—à–∞"""
    info = _get_embedding_cached.cache_info()
    return {
        'hits': info.hits,
        'misses': info.misses,
        'size': info.currsize,
        'maxsize': info.maxsize
    }


# ============== Test ==============

if __name__ == "__main__":
    # –í–∫–ª—é—á–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
    os.environ["NEIRA_LOCAL_EMBEDDINGS"] = "true"
    
    print("üß™ –¢–µ—Å—Ç Local Embeddings v2.0")
    print("=" * 50)
    
    test_texts = [
        "–°–æ–∑–¥–∞–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∏–≥—Ä—ã",
        "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –º–∞—Å—Å–∏–≤–∞",
        "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?",
        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –∫–æ–¥ –Ω–∞ –æ—à–∏–±–∫–∏",
        "–ó–∞–ø–æ–º–Ω–∏ —á—Ç–æ –º–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π",
    ]
    
    embeddings = []
    for text in test_texts:
        emb = get_local_embedding(text)
        if emb:
            embeddings.append((text, emb))
            print(f"‚úÖ '{text[:30]}...' ‚Üí dim={len(emb)}")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è '{text}'")
    
    print("\n" + "=" * 50)
    print("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ö–æ–¥—Å—Ç–≤–∞:")
    
    query = "–°–¥–µ–ª–∞–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏–≥—Ä—ã"
    print(f"\n–ó–∞–ø—Ä–æ—Å: '{query}'")
    
    similar = find_similar(query, embeddings, top_k=3)
    for text, score in similar:
        print(f"  {score:.3f}: {text}")
    
    print("\n" + "=" * 50)
    print("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
    
    for text in test_texts[:3]:
        cats = _detect_categories(text)
        print(f"'{text[:30]}...' ‚Üí {cats}")
    
    print("\n" + "=" * 50)
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—ç—à–∞: {get_cache_stats()}")
    print("\nüéâ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
