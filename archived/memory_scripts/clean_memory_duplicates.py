#!/usr/bin/env python3
"""
–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏ –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –∏–∑ –ø–∞–º—è—Ç–∏ Neira
"""
import json
from datetime import datetime
from collections import defaultdict

def clean_memory():
    """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞–º—è—Ç—å
    with open('neira_memory.json', 'r', encoding='utf-8') as f:
        memories = json.load(f)
    
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–æ –æ—á–∏—Å—Ç–∫–∏: {len(memories)}\n")
    
    # 1. –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
    seen_texts = {}
    duplicates = []
    
    for i, entry in enumerate(memories):
        text_normalized = entry.get('text', '').strip().lower()
        
        if text_normalized in seen_texts:
            # –î—É–±–ª–∏–∫–∞—Ç –Ω–∞–π–¥–µ–Ω
            duplicates.append((i, seen_texts[text_normalized], text_normalized[:70]))
        else:
            seen_texts[text_normalized] = i
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Ç–æ—á–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
    
    # 2. –ù–∞—Ö–æ–¥–∏–º –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è (–º–Ω–æ–≥–æ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π –∑–∞ –∫–æ—Ä–æ—Ç–∫–∏–π –ø–µ—Ä–∏–æ–¥)
    looped_entries = []
    time_buckets = defaultdict(list)  # timestamp_minute -> [indices]
    
    for i, entry in enumerate(memories):
        ts = entry.get('timestamp', '')
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–∏–Ω—É—Ç–∞–º
        if ts:
            minute_key = ts[:16]  # YYYY-MM-DDTHH:MM
            time_buckets[minute_key].append(i)
    
    # –ò—â–µ–º –º–∏–Ω—É—Ç—ã —Å –∞–Ω–æ–º–∞–ª—å–Ω–æ –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∑–∞–ø–∏—Å–µ–π (>5)
    for minute, indices in time_buckets.items():
        if len(indices) > 5:
            print(f"‚ö†Ô∏è –ó–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –≤ {minute}: {len(indices)} –∑–∞–ø–∏—Å–µ–π")
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ—Ö–æ–∂–µ—Å—Ç—å
            texts_in_minute = [memories[i].get('text', '') for i in indices]
            
            # –ï—Å–ª–∏ –≤—Å–µ –ø—Ä–æ –æ–¥–Ω–æ –∏ —Ç–æ –∂–µ - –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ
            keywords = ['telegram', 'c++', 'analyzer', 'logger', 'json', 'chatid']
            keyword_counts = {kw: sum(1 for t in texts_in_minute if kw in t.lower()) for kw in keywords}
            
            # –ï—Å–ª–∏ >80% –∑–∞–ø–∏—Å–µ–π —Å–æ–¥–µ—Ä–∂–∞—Ç –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ - –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏–µ
            for kw, count in keyword_counts.items():
                if count > len(indices) * 0.8:
                    print(f"   ‚Üí –¢–µ–º–∞ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–Ω–∏—è: {kw} ({count}/{len(indices)} –∑–∞–ø–∏—Å–µ–π)")
                    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—É—é –∏ –ø–æ—Å–ª–µ–¥–Ω—é—é –∑–∞–ø–∏—Å—å
                    looped_entries.extend(indices[1:-1])
                    break
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(looped_entries)}\n")
    
    # 3. –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    to_remove = set()
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ)
    for dup_idx, original_idx, text in duplicates:
        to_remove.add(dup_idx)
        if len(to_remove) <= 5:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç #{dup_idx}: {text}...")
    
    # –£–¥–∞–ª—è–µ–º –∑–∞—Ü–∏–∫–ª–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
    to_remove.update(looped_entries)
    
    print(f"\nüìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –∫ —É–¥–∞–ª–µ–Ω–∏—é: {len(to_remove)}")
    
    # 4. –°–æ–∑–¥–∞–µ–º –æ—á–∏—â–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å
    cleaned = [entry for i, entry in enumerate(memories) if i not in to_remove]
    
    print(f"‚úÖ –ó–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {len(cleaned)}")
    print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(memories) - len(cleaned)}\n")
    
    # 5. –°–æ–∑–¥–∞–µ–º –±—ç–∫–∞–ø
    backup_name = f"neira_memory_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(backup_name, 'w', encoding='utf-8') as f:
        json.dump(memories, f, ensure_ascii=False, indent=2)
    print(f"üíæ –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {backup_name}")
    
    # 6. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å
    with open('neira_memory.json', 'w', encoding='utf-8') as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    
    # 7. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π
    print("\n" + "=" * 80)
    print("–ü–û–°–õ–ï–î–ù–ò–ï 10 –ó–ê–ü–ò–°–ï–ô –ü–û–°–õ–ï –û–ß–ò–°–¢–ö–ò:")
    print("=" * 80)
    for i, entry in enumerate(cleaned[-10:], len(cleaned)-9):
        ts = entry.get('timestamp', '–Ω–µ—Ç')[:19]
        text = entry.get('text', '')[:70]
        print(f"{i}. {ts} - {text}")

if __name__ == "__main__":
    clean_memory()
