"""
–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ç–æ—Ä –ø–∞–º—è—Ç–∏ –¥–ª—è Neira
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏, –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —á–µ—Ä–µ–∑ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
"""

import json
from typing import List, Dict, Set, Tuple
from datetime import datetime
from pathlib import Path
from dataclasses import asdict


class MemoryConsolidator:
    """–£–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏"""
    
    def __init__(self, similarity_threshold: float = 0.85):
        """
        Args:
            similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è (0-1)
        """
        self.similarity_threshold = similarity_threshold
    
    def consolidate_similar(self, memories: List[dict]) -> Tuple[List[dict], dict]:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏ (>threshold —Å—Ö–æ–∂–µ—Å—Ç–∏) –≤ –æ–¥–Ω—É —Å –ø–æ–≤—ã—à–µ–Ω–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        
        Args:
            memories: –°–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏
        
        Returns:
            (consolidated_memories, stats)
        """
        if not memories:
            return [], {"clusters": 0, "merged": 0, "kept": 0}
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π
        clusters = self._cluster_similar(memories)
        
        consolidated = []
        merged_count = 0
        
        for cluster in clusters:
            if len(cluster) > 1:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä –≤ –æ–¥–Ω—É –∑–∞–ø–∏—Å—å
                merged = self._merge_cluster(cluster)
                consolidated.append(merged)
                merged_count += len(cluster) - 1
            else:
                # –û–¥–∏–Ω–æ—á–Ω–∞—è –∑–∞–ø–∏—Å—å - –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                consolidated.append(cluster[0])
        
        stats = {
            "original_count": len(memories),
            "consolidated_count": len(consolidated),
            "clusters": len(clusters),
            "merged": merged_count,
            "reduction_percent": round((1 - len(consolidated) / len(memories)) * 100, 1) if memories else 0
        }
        
        return consolidated, stats
    
    def _cluster_similar(self, memories: List[dict]) -> List[List[dict]]:
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏"""
        clusters = []
        used_indices = set()
        
        for i, mem1 in enumerate(memories):
            if i in used_indices:
                continue
            
            cluster = [mem1]
            used_indices.add(i)
            
            for j, mem2 in enumerate(memories[i+1:], i+1):
                if j in used_indices:
                    continue
                
                similarity = self._semantic_similarity(
                    mem1.get("text", ""),
                    mem2.get("text", "")
                )
                
                if similarity >= self.similarity_threshold:
                    cluster.append(mem2)
                    used_indices.add(j)
            
            clusters.append(cluster)
        
        return clusters
    
    def _merge_cluster(self, cluster: List[dict]) -> dict:
        """
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π –≤ –æ–¥–Ω—É
        
        –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
        - –¢–µ–∫—Å—Ç: —Å–∞–º–∞—è —Å–≤–µ–∂–∞—è –∑–∞–ø–∏—Å—å
        - Confidence: –±–∞–∑–æ–≤–∞—è + –±–æ–Ω—É—Å –∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ
        - Related: –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–≤—è–∑–µ–π
        - Access count: —Å—É–º–º–∞ –æ–±—Ä–∞—â–µ–Ω–∏–π
        - Validation: –µ—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–Ω–∞ user_confirmed ‚Üí confirmed
        """
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (—Å–∞–º–∞—è —Å–≤–µ–∂–∞—è –ø–µ—Ä–≤–æ–π)
        sorted_cluster = sorted(
            cluster,
            key=lambda x: x.get("timestamp", ""),
            reverse=True
        )
        
        latest = sorted_cluster[0]
        
        # –ü–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —á–µ—Ä–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ
        base_confidence = latest.get("confidence", 0.5)
        confirmation_boost = min(0.3, 0.05 * len(cluster))  # +0.05 –∑–∞ –∫–∞–∂–¥—É—é –ø–æ—Ö–æ–∂—É—é
        new_confidence = min(1.0, base_confidence + confirmation_boost)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–≤—è–∑–∏
        all_related = set()
        for mem in cluster:
            related = mem.get("related_ids", [])
            if isinstance(related, list):
                all_related.update(related)
        
        # –°—É–º–º–∏—Ä—É–µ–º –æ–±—Ä–∞—â–µ–Ω–∏—è
        total_access = sum(mem.get("access_count", 0) for mem in cluster)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è: –µ—Å–ª–∏ —Ö–æ—Ç—å –æ–¥–Ω–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        validation_priority = {
            "user_confirmed": 4,
            "validated": 3,
            "pending": 2,
            "rejected": 1
        }
        
        best_validation = max(
            cluster,
            key=lambda x: validation_priority.get(x.get("validation_status", "pending"), 0)
        ).get("validation_status", "validated")
        
        # –°–æ–∑–¥–∞—ë–º –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—É—é –∑–∞–ø–∏—Å—å
        merged = latest.copy()
        merged.update({
            "confidence": new_confidence,
            "validation_status": best_validation,
            "related_ids": list(all_related),
            "access_count": total_access,
            "consolidation_info": {
                "merged_count": len(cluster),
                "merged_at": datetime.now().isoformat(),
                "source_ids": [mem.get("id", "") for mem in cluster if mem.get("id")]
            }
        })
        
        return merged
    
    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """
        –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤ (Jaccard + n-grams)
        
        –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç:
        1. Jaccard similarity —Å–ª–æ–≤ (50%)
        2. Character n-gram similarity (50%)
        """
        if not text1 or not text2:
            return 0.0
        
        # 1. Jaccard similarity —Å–ª–æ–≤
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if words1 and words2:
            jaccard = len(words1 & words2) / len(words1 | words2)
        else:
            jaccard = 0.0
        
        # 2. Character n-gram similarity (n=3)
        ngrams1 = self._get_ngrams(text1.lower(), n=3)
        ngrams2 = self._get_ngrams(text2.lower(), n=3)
        
        if ngrams1 and ngrams2:
            ngram_sim = len(ngrams1 & ngrams2) / len(ngrams1 | ngrams2)
        else:
            ngram_sim = 0.0
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º (50/50)
        return (jaccard + ngram_sim) / 2
    
    def _get_ngrams(self, text: str, n: int = 3) -> Set[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç character n-grams"""
        text = text.replace(" ", "")  # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
        return {text[i:i+n] for i in range(len(text) - n + 1)}
    
    def consolidate_by_category(self, memories: List[dict]) -> Tuple[List[dict], dict]:
        """
        –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        
        –°–Ω–∞—á–∞–ª–∞ –≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, –ø–æ—Ç–æ–º –∫–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ—Ç –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π
        """
        if not memories:
            return [], {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        by_category = {}
        for mem in memories:
            category = mem.get("category", "conversation")
            if category not in by_category:
                by_category[category] = []
            by_category[category].append(mem)
        
        # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –æ—Ç–¥–µ–ª—å–Ω–æ
        consolidated = []
        total_stats = {
            "original_count": len(memories),
            "by_category": {}
        }
        
        for category, cat_memories in by_category.items():
            cat_consolidated, cat_stats = self.consolidate_similar(cat_memories)
            consolidated.extend(cat_consolidated)
            total_stats["by_category"][category] = cat_stats
        
        total_stats["consolidated_count"] = len(consolidated)
        total_stats["reduction_percent"] = round(
            (1 - len(consolidated) / len(memories)) * 100, 1
        ) if memories else 0
        
        return consolidated, total_stats


# –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞ –ø–∞–º—è—Ç–∏
def consolidate_memory_file(
    input_file: str,
    output_file: str = None,
    threshold: float = 0.85,
    by_category: bool = True
) -> dict:
    """
    –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –ø–∞–º—è—Ç–∏
    
    Args:
        input_file: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        output_file: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ None - –ø–µ—Ä–µ–∑–∞–ø–∏—à–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π)
        threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
        by_category: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –ø–µ—Ä–µ–¥ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–µ–π
    
    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞–º—è—Ç—å
    with open(input_file, 'r', encoding='utf-8') as f:
        memories = json.load(f)
    
    if not isinstance(memories, list):
        raise ValueError("–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å–ø–∏—Å–æ–∫ –∑–∞–ø–∏—Å–µ–π")
    
    print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(memories)}")
    
    # –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä—É–µ–º
    consolidator = MemoryConsolidator(similarity_threshold=threshold)
    
    if by_category:
        consolidated, stats = consolidator.consolidate_by_category(memories)
    else:
        consolidated, stats = consolidator.consolidate_similar(memories)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    if output_file is None:
        output_file = input_file
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(consolidated, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(consolidated)}")
    print(f"üìâ –°–∂–∞—Ç–∏–µ: {stats['reduction_percent']}%")
    
    if "by_category" in stats:
        print("\nüìä –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, cat_stats in stats["by_category"].items():
            print(f"  {category}: {cat_stats['original_count']} ‚Üí {cat_stats['consolidated_count']} "
                  f"(-{cat_stats['reduction_percent']}%)")
    
    return stats


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    test_memories = [
        {
            "id": "1",
            "text": "–õ—é–±–ª—é –≥—É–ª—è—Ç—å –≤ –ø–∞—Ä–∫–µ –ø–æ —É—Ç—Ä–∞–º",
            "category": "preference",
            "confidence": 0.7,
            "validation_status": "pending",
            "timestamp": "2025-12-15T08:00:00",
            "access_count": 5,
            "related_ids": []
        },
        {
            "id": "2",
            "text": "–û–±–æ–∂–∞—é —É—Ç—Ä–µ–Ω–Ω–∏–µ –ø—Ä–æ–≥—É–ª–∫–∏ –≤ –ø–∞—Ä–∫–µ",
            "category": "preference",
            "confidence": 0.6,
            "validation_status": "validated",
            "timestamp": "2025-12-15T09:00:00",
            "access_count": 3,
            "related_ids": []
        },
        {
            "id": "3",
            "text": "–ú–æ–π –ª—é–±–∏–º—ã–π —Ü–≤–µ—Ç - —Å–∏–Ω–∏–π",
            "category": "preference",
            "confidence": 0.8,
            "validation_status": "user_confirmed",
            "timestamp": "2025-12-14T10:00:00",
            "access_count": 10,
            "related_ids": []
        }
    ]
    
    consolidator = MemoryConsolidator(similarity_threshold=0.80)
    
    print("üß™ –¢–µ—Å—Ç –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏:")
    print(f"–ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {len(test_memories)}\n")
    
    consolidated, stats = consolidator.consolidate_similar(test_memories)
    
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
    print(f"  –ó–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ: {len(consolidated)}")
    print(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {stats['clusters']}")
    print(f"  –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ: {stats['merged']}")
    print(f"  –°–∂–∞—Ç–∏–µ: {stats['reduction_percent']}%")
    
    print(f"\nüìã –ö–æ–Ω—Å–æ–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏:")
    for mem in consolidated:
        print(f"\n  [{mem['id']}] {mem['text'][:50]}...")
        print(f"    Confidence: {mem['confidence']:.2f}")
        print(f"    Validation: {mem['validation_status']}")
        if "consolidation_info" in mem:
            info = mem["consolidation_info"]
            print(f"    Merged: {info['merged_count']} –∑–∞–ø–∏—Å–µ–π")
            print(f"    Sources: {info['source_ids']}")
