"""
üéì Neira Autonomous Learning System v1.0
–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π

–ü–†–ò–ù–¶–ò–ü–´:
1. –£—á–∏–º—Å—è –¢–û–õ–¨–ö–û –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (whitelist)
2. –í—Å–µ –∑–Ω–∞–Ω–∏—è –ø—Ä–æ—Ö–æ–¥—è—Ç –∫–∞—Ä–∞–Ω—Ç–∏–Ω (quarantine zone)
3. –ú—É–ª—å—Ç–∏—Å–æ—Ä—Å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è (2+ –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø—Ä–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è—Ö
5. Human-in-the-loop –¥–ª—è —Å–ø–æ—Ä–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤

–ó–ê–©–ò–¢–ê –û–¢ –ì–ê–õ–õ–Æ–¶–ò–ù–ê–¶–ò–ô:
‚úÖ Whitelist –Ω–∞–¥—ë–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
‚úÖ Blacklist –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è —Å –ø–∞–º—è—Ç—å—é
‚úÖ –ü–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π (–∏–∑ immune_system.py)
‚úÖ –ö–∞—Ä–∞–Ω—Ç–∏–Ω –ø–µ—Ä–µ–¥ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –≤ –ø–∞–º—è—Ç—å
‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ confidence (0.7)
"""

import asyncio
import logging
import json
import os
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import hashlib

try:
    import aiohttp
    import requests
    from bs4 import BeautifulSoup  # type: ignore
except ImportError:
    print("‚ö†Ô∏è –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install aiohttp beautifulsoup4 lxml")


class SourceTrust(Enum):
    """–£—Ä–æ–≤–µ–Ω—å –¥–æ–≤–µ—Ä–∏—è –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É"""
    VERIFIED = 1.0      # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
    HIGH = 0.9          # Wikipedia, arXiv
    MEDIUM = 0.7        # –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –±–ª–æ–≥–∏
    LOW = 0.5           # –û–±—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
    UNTRUSTED = 0.0     # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏


@dataclass
class CuratedSource:
    """–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∑–Ω–∞–Ω–∏–π"""
    name: str
    url_pattern: str      # Regex –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ URL
    trust_level: float    # 0.0-1.0
    categories: List[str] # –¢–µ–º—ã –∏–∑ —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    rate_limit: int = 10  # –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —á–∞—Å
    
    def matches(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ URL –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        return bool(re.match(self.url_pattern, url))


@dataclass
class QuarantineEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–µ (–æ–∂–∏–¥–∞–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏)"""
    id: str
    text: str
    source_url: str
    source_trust: float
    category: str
    timestamp: str
    
    # –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è
    verification_count: int = 0  # –°–∫–æ–ª—å–∫–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—Ç
    confidence: float = 0.0
    contradictions: List[str] = field(default_factory=list)
    
    # –°—Ç–∞—Ç—É—Å
    status: str = "pending"  # pending, approved, rejected
    reviewed_by: Optional[str] = None
    review_timestamp: Optional[str] = None


class SourceWhitelist:
    """Whitelist –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    CURATED_SOURCES = [
        # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –¥–æ–≤–µ—Ä–∏–µ)
        CuratedSource(
            name="Python.org",
            url_pattern=r"https?://docs\.python\.org/.*",
            trust_level=SourceTrust.VERIFIED.value,
            categories=["programming", "python", "technology"]
        ),
        CuratedSource(
            name="MDN Web Docs",
            url_pattern=r"https?://developer\.mozilla\.org/.*",
            trust_level=SourceTrust.VERIFIED.value,
            categories=["web", "javascript", "technology"]
        ),
        
        # –ù–∞—É—á–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–≤—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ)
        CuratedSource(
            name="Wikipedia (Russian)",
            url_pattern=r"https?://ru\.wikipedia\.org/wiki/.*",
            trust_level=SourceTrust.HIGH.value,
            categories=["general", "science", "history", "culture"]
        ),
        CuratedSource(
            name="Wikipedia (English)",
            url_pattern=r"https?://en\.wikipedia\.org/wiki/.*",
            trust_level=SourceTrust.HIGH.value,
            categories=["general", "science", "history", "culture"]
        ),
        CuratedSource(
            name="arXiv.org",
            url_pattern=r"https?://arxiv\.org/abs/.*",
            trust_level=SourceTrust.HIGH.value,
            categories=["science", "ai", "research"]
        ),
        
        # GitHub (–≤—ã—Å–æ–∫–æ–µ –¥–æ–≤–µ—Ä–∏–µ, –Ω–æ —Ç–æ–ª—å–∫–æ README)
        CuratedSource(
            name="GitHub README",
            url_pattern=r"https?://github\.com/.*/.*/(blob|raw)/.*/README\.md",
            trust_level=SourceTrust.HIGH.value,
            categories=["programming", "opensource", "technology"]
        ),
        
        # Stack Overflow (—Å—Ä–µ–¥–Ω–µ–µ –¥–æ–≤–µ—Ä–∏–µ - —Ç–æ–ª—å–∫–æ accepted answers)
        CuratedSource(
            name="Stack Overflow",
            url_pattern=r"https?://stackoverflow\.com/questions/.*",
            trust_level=SourceTrust.MEDIUM.value,
            categories=["programming", "troubleshooting"]
        ),
    ]
    
    # Blacklist –Ω–µ–Ω–∞–¥—ë–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    BLACKLIST_PATTERNS = [
        r".*forum.*",           # –§–æ—Ä—É–º—ã (–Ω–µ–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
        r".*\.blogspot\..*",    # –ë–ª–æ–≥–∏
        r".*medium\.com.*",     # Medium (—Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ)
        r".*reddit\.com.*",     # Reddit (UGC)
        r".*quora\.com.*",      # Quora (—Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ)
        r".*\.xyz$",            # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        r".*\.tk$",             # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
        r".*torrent.*",         # –¢–æ—Ä—Ä–µ–Ω—Ç—ã
    ]
    
    @classmethod
    def is_trusted(cls, url: str) -> Tuple[bool, float, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–¥—ë–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        
        Returns:
            (trusted, trust_level, source_name)
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ blacklist
        for pattern in cls.BLACKLIST_PATTERNS:
            if re.match(pattern, url, re.IGNORECASE):
                return (False, 0.0, "blacklisted")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ whitelist
        for source in cls.CURATED_SOURCES:
            if source.matches(url):
                return (True, source.trust_level, source.name)
        
        # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ - –Ω–µ –¥–æ–≤–µ—Ä—è–µ–º
        return (False, SourceTrust.UNTRUSTED.value, "unknown")


class KnowledgeValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –∑–Ω–∞–Ω–∏–π —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"""
    
    def __init__(self, memory_system):
        self.memory = memory_system
        self.min_sources = 2  # –ú–∏–Ω–∏–º—É–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        self.min_confidence = 0.7  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è
    
    async def verify_fact(self, text: str, source_url: str, source_trust: float) -> Tuple[bool, float, List[str]]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∞–∫—Ç –Ω–∞ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å
        
        Returns:
            (approved, confidence, contradictions)
        """
        contradictions = []
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ø–∞–º—è—Ç—å—é
        existing_memories = self.memory.long_term + self.memory.short_term
        for entry in existing_memories:
            if self._check_contradiction(text, entry.text):
                contradictions.append(f"–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—Ç: {entry.text[:100]}")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è - –æ—Ç–∫–ª–æ–Ω—è–µ–º
        if contradictions:
            return (False, 0.0, contradictions)
        
        # 2. –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = –¥–æ–≤–µ—Ä–∏–µ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        confidence = source_trust
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
        if self._has_hallucination_patterns(text):
            return (False, 0.0, ["–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"])
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if self._too_specific_without_context(text):
            confidence *= 0.5  # –°–Ω–∏–∂–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        
        # 5. –†–µ—à–µ–Ω–∏–µ: –ø—Ä–∏–Ω–∏–º–∞–µ–º –µ—Å–ª–∏ confidence >= threshold
        approved = confidence >= self.min_confidence
        
        return (approved, confidence, contradictions)
    
    def _check_contradiction(self, text1: str, text2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º –æ—Ç—Ä–∏—Ü–∞–Ω–∏—è
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ + –æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ
        negations = {"–Ω–µ", "–Ω–µ—Ç", "never", "no", "not", "–±–µ–∑"}
        
        common_words = words1.intersection(words2)
        has_negation_1 = bool(words1.intersection(negations))
        has_negation_2 = bool(words2.intersection(negations))
        
        # –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ –µ—Å–ª–∏ –æ–¥–Ω–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ, –¥—Ä—É–≥–æ–µ –Ω–µ—Ç
        if len(common_words) >= 3 and (has_negation_1 != has_negation_2):
            return True
        
        return False
    
    def _has_hallucination_patterns(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π (–∏–∑ immune_system.py)"""
        suspicious_patterns = [
            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏
            r'\$\d+\s*(–º–ª–Ω|—Ç—ã—Å|billion)',           # –î–µ–Ω–µ–∂–Ω—ã–µ —Å—É–º–º—ã –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            r'–ø—Ä–∏–±—ã–ª—å|–¥–æ—Ö–æ–¥|–∑–∞—Ä–∞–±–æ—Ç.*\d+',          # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ü–∏—Ñ—Ä—ã
            r'–ø—Ä–æ–¥–∞–∂[–∞–∏]? (–º–æ—â–Ω–æ—Å—Ç|—Ä–µ—Å—É—Ä—Å)',        # –ü—Ä–æ–¥–∞–∂–∞ –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–π
            
            # –ò–≥—Ä–æ–≤—ã–µ –º–µ—Ö–∞–Ω–∏–∫–∏
            r'–∏–≥—Ä[–∞—ã]? (—Ç—Ä–µ–±—É–µ—Ç|–≤–∫–ª—é—á–∞–µ—Ç)',
            r'—Ö–æ–¥ (–º–æ–∂–µ—Ç|–≤–∫–ª—é—á–∞–µ—Ç)',
            
            # –¶–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ
            r'—Ü–µ–Ω–∞ (–º–æ–¥–µ–ª–∏|–º–æ—â–Ω–æ—Å—Ç–∏)',
            r'—Å—Ç–æ–∏–º–æ—Å—Ç—å (–Ω–µ–π—Ä–æ–Ω|–∫–ª–µ—Ç–∫)',
            
            # –§–∏–∑–∏—á–µ—Å–∫–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ–µ
            r'—Å–∫–æ—Ä–æ—Å—Ç—å.*—Å–≤–µ—Ç.*–ø—Ä–µ–≤—ã—à–∞',
            r'–≤–µ—á–Ω—ã–π –¥–≤–∏–≥–∞—Ç–µ–ª—å',
            
            # –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
            r'(–≤—Å–µ–≥–¥–∞|–Ω–∏–∫–æ–≥–¥–∞|–≤—Å–µ|–Ω–∏ –æ–¥–∏–Ω).*100%',
        ]
        
        for pattern in suspicious_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                return True
        
        return False
    
    def _too_specific_without_context(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±–µ–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        # –¢–æ—á–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if re.search(r'\d{4,}', text) and not re.search(r'(–≥–æ–¥|–≤–µ—Ä—Å–∏—è|—Å—Ç—Ä–∞–Ω–∏—Ü)', text):
            return True
        
        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        if re.search(r'[A-Z–ê-–Ø][a-z–∞-—è]+\s[A-Z–ê-–Ø][a-z–∞-—è]+', text):
            # –ï—Å—Ç—å –∏–º—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ, –Ω–æ –Ω–µ—Ç –≥–ª–∞–≥–æ–ª–æ–≤ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            if not re.search(r'(—Å–æ–∑–¥–∞–ª|–∏–∑–æ–±—Ä—ë–ª|–Ω–∞–ø–∏—Å–∞–ª|—Å–∫–∞–∑–∞–ª)', text):
                return True
        
        return False


class LearningCurriculum:
    """–£—á–µ–±–Ω—ã–π –ø–ª–∞–Ω - —á—Ç–æ –∏–∑—É—á–∞—Ç—å –∏ –≤ –∫–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ"""
    
    TOPICS = {
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –°–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏–µ
        "self_awareness": {
            "priority": 1,
            "keywords": ["AI", "LLM", "neural_network", "machine_learning", "Ollama"],
            "sources": ["Wikipedia", "arXiv.org"],
            "max_entries": 20
        },
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –¢–µ–∫—É—â–∏–π —Å—Ç–µ–∫
        "current_stack": {
            "priority": 2,
            "keywords": ["Python_asyncio", "Telegram_Bot_API", "FastAPI", "pytest"],
            "sources": ["Python.org", "MDN Web Docs", "GitHub README"],
            "max_entries": 30
        },
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –û–±—â–∏–µ –∑–Ω–∞–Ω–∏—è
        "general_knowledge": {
            "priority": 3,
            "keywords": ["–∏—Å—Ç–æ—Ä–∏—è_–Ω–∞—É–∫–∏", "—Ñ–∏–∑–∏–∫–∞", "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ"],
            "sources": ["Wikipedia (Russian)"],
            "max_entries": 50
        },
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 4: –¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
        "tech_trends": {
            "priority": 4,
            "keywords": ["Python_3.13", "AI_–Ω–æ–≤–æ—Å—Ç–∏", "GitHub_Copilot"],
            "sources": ["Python.org", "GitHub"],
            "max_entries": 10
        }
    }
    
    @classmethod
    def get_next_topic(cls) -> Tuple[str, Dict[str, Any]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–µ–¥—É—é—â—É—é —Ç–µ–º—É –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è"""
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        sorted_topics = sorted(cls.TOPICS.items(), key=lambda x: x[1]["priority"])
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é
        return sorted_topics[0]


class AutonomousLearningSystem:
    """–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, memory_system, idle_threshold_minutes: int = 30, admin_telegram_id: Optional[int] = None):
        self.memory = memory_system
        self.idle_threshold = idle_threshold_minutes
        self.admin_telegram_id = admin_telegram_id
        self.validator = KnowledgeValidator(memory_system)
        
        # –ö–∞—Ä–∞–Ω—Ç–∏–Ω–Ω–∞—è –∑–æ–Ω–∞
        self.quarantine_path = "neira_quarantine.json"
        self.quarantine: List[QuarantineEntry] = self._load_quarantine()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "learning_sessions": 0,
            "facts_learned": 0,
            "facts_rejected": 0,
            "sources_checked": 0,
            "quarantine_approved": 0,
            "quarantine_rejected": 0,
        }
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        self.last_activity = datetime.now()
        
        # –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞
        self.learning_task: Optional[asyncio.Task] = None
        self.running = False
        
        logging.info("üéì Autonomous Learning System –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def _load_quarantine(self) -> List[QuarantineEntry]:
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Ä–∞–Ω—Ç–∏–Ω –∏–∑ —Ñ–∞–π–ª–∞"""
        if not os.path.exists(self.quarantine_path):
            return []
        
        try:
            with open(self.quarantine_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            entries = []
            for entry_dict in data:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ –¥–ª—è contradictions
                if 'contradictions' not in entry_dict:
                    entry_dict['contradictions'] = []
                entries.append(QuarantineEntry(**entry_dict))
            logging.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞: {len(entries)}")
            return entries
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞: {e}")
            return []
    
    def _save_quarantine(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞—Ä–∞–Ω—Ç–∏–Ω –≤ —Ñ–∞–π–ª"""
        try:
            data = [asdict(entry) for entry in self.quarantine]
            with open(self.quarantine_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞: {e}")
    
    def mark_activity(self):
        """–û—Ç–º–µ—Ç–∏—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–¥–∏–∞–ª–æ–≥ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)"""
        self.last_activity = datetime.now()
    
    def is_idle(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞: Neira –Ω–µ –∑–∞–Ω—è—Ç–∞?"""
        elapsed = (datetime.now() - self.last_activity).total_seconds() / 60
        return elapsed >= self.idle_threshold
    
    async def start_autonomous_learning(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Ñ–æ–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        if self.running:
            logging.warning("–û–±—É—á–µ–Ω–∏–µ —É–∂–µ –∑–∞–ø—É—â–µ–Ω–æ")
            return
        
        self.running = True
        self.learning_task = asyncio.create_task(self._learning_loop())
        logging.info("üéì –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—â–µ–Ω–æ")
    
    async def stop_autonomous_learning(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ñ–æ–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        self.running = False
        if self.learning_task:
            self.learning_task.cancel()
            try:
                await self.learning_task
            except asyncio.CancelledError:
                pass
        logging.info("üõë –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    async def _learning_loop(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"""
        while self.running:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º idle
                if self.is_idle():
                    logging.info("üí§ Neira –≤ —Ä–µ–∂–∏–º–µ idle - –Ω–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
                    await self._run_learning_session()
                
                # –ñ–¥—ë–º 1 —á–∞—Å –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                await asyncio.sleep(3600)
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
                await asyncio.sleep(60)  # Retry —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É
    
    async def _run_learning_session(self):
        """–ó–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Å—Å–∏—é –æ–±—É—á–µ–Ω–∏—è"""
        self.stats["learning_sessions"] += 1
        logging.info(f"üìö –ù–∞—á–∞–ª–æ —Å–µ—Å—Å–∏–∏ –æ–±—É—á–µ–Ω–∏—è #{self.stats['learning_sessions']}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é —Ç–µ–º—É
        topic_name, topic_config = LearningCurriculum.get_next_topic()
        logging.info(f"üìñ –¢–µ–º–∞: {topic_name}")
        
        # –£—á–∏–º—Å—è –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        for keyword in topic_config["keywords"][:3]:  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
            await self._learn_from_keyword(keyword, topic_config)
            await asyncio.sleep(5)  # Rate limiting
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ä–∞–Ω—Ç–∏–Ω
        await self._review_quarantine()
        
        logging.info(f"‚úÖ –°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ò–∑—É—á–µ–Ω–æ: {self.stats['facts_learned']}")
    
    async def _learn_from_keyword(self, keyword: str, topic_config: Dict):
        """–ò–∑—É—á–∏—Ç—å —Ç–µ–º—É –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É"""
        logging.info(f"üîç –ü–æ–∏—Å–∫: {keyword}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Wikipedia –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫
        keyword_clean = keyword.replace('_', ' ')
        url = f"https://ru.wikipedia.org/wiki/{keyword.replace(' ', '_')}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–≤–µ—Ä–∏–µ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        trusted, trust_level, source_name = SourceWhitelist.is_trusted(url)
        
        if not trusted:
            logging.warning(f"‚ö†Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –≤ whitelist: {url}")
            return
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ (summary) –∏–∑ Wikipedia
        try:
            fact = await self._extract_wikipedia_summary(keyword_clean, url)
            if not fact:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è '{keyword}'")
                return
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            return
        
        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º
        approved, confidence, contradictions = await self.validator.verify_fact(
            fact, url, trust_level
        )
        
        self.stats["sources_checked"] += 1
        
        if approved:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–∞—Ä–∞–Ω—Ç–∏–Ω
            entry_id = hashlib.md5(fact.encode()).hexdigest()[:12]
            
            quarantine_entry = QuarantineEntry(
                id=entry_id,
                text=fact,
                source_url=url,
                source_trust=trust_level,
                category="learned",
                timestamp=datetime.now().isoformat(),
                confidence=confidence,
                status="pending"
            )
            
            self.quarantine.append(quarantine_entry)
            self._save_quarantine()
            self.stats["facts_learned"] += 1
            
            logging.info(f"‚úÖ –§–∞–∫—Ç –≤ –∫–∞—Ä–∞–Ω—Ç–∏–Ω: {fact[:60]}...")
        else:
            self.stats["facts_rejected"] += 1
            logging.warning(f"‚ùå –§–∞–∫—Ç –æ—Ç–∫–ª–æ–Ω—ë–Ω: {fact[:60]}... | {contradictions}")
    
    async def _extract_wikipedia_summary(self, keyword: str, url: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ Wikipedia —á–µ—Ä–µ–∑ API —Å fallback –Ω–∞ en."""
        async def fetch_summary(session, api_url: str) -> Optional[str]:
            try:
                async with session.get(api_url, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                    if resp.status != 200:
                        return None
                    data = await resp.json()
                    summary = data.get("extract")
                    if summary:
                        summary = summary.strip()
                        if len(summary) > 500:
                            summary = summary[:500]
                        return f"{keyword}: {summary}"
            except Exception as e:  # pragma: no cover - —Å–µ—Ç—å –Ω–µ–ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞
                logging.warning(f"Wiki API error for {api_url}: {e}")
            return None

        title = keyword.replace(" ", "_")
        api_ru = f"https://ru.wikipedia.org/api/rest_v1/page/summary/{title}"
        api_en = f"https://en.wikipedia.org/api/rest_v1/page/summary/{title}"

        async with aiohttp.ClientSession() as session:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º ru
            summary = await fetch_summary(session, api_ru)
            if summary:
                return summary
            # Fallback –Ω–∞ en
            summary = await fetch_summary(session, api_en)
            if summary:
                return summary

        return None
    
    async def _review_quarantine(self):
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—Ä–∞–Ω—Ç–∏–Ω –∏ –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –æ–¥–æ–±—Ä–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –≤ –ø–∞–º—è—Ç—å"""
        approved_count = 0
        
        for entry in self.quarantine[:]:  # –ö–æ–ø–∏—è —Å–ø–∏—Å–∫–∞
            # –ê–≤—Ç–æ–æ–¥–æ–±—Ä–µ–Ω–∏–µ –µ—Å–ª–∏ confidence –≤—ã—Å–æ–∫–∞—è –∏ –Ω–µ—Ç –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–π
            if entry.confidence >= 0.9 and not entry.contradictions and entry.status == "pending":
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤ –ø–∞–º—è—Ç—å
                self.memory.remember(
                    text=entry.text,
                    source=f"autonomous_learning:{entry.source_url}",
                    category="learned",
                    force_long_term=True
                )
                
                # –£–¥–∞–ª—è–µ–º –∏–∑ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞
                self.quarantine.remove(entry)
                approved_count += 1
                self.stats["quarantine_approved"] += 1
        
        if approved_count > 0:
            self._save_quarantine()
            logging.info(f"‚úÖ –û–¥–æ–±—Ä–µ–Ω–æ –∏–∑ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞: {approved_count}")
    
    def manual_approve(self, entry_id: str) -> bool:
        """–†—É—á–Ω–æ–µ –æ–¥–æ–±—Ä–µ–Ω–∏–µ –∏–∑ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞ (–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–º)"""
        for entry in self.quarantine:
            if entry.id == entry_id:
                # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤ –ø–∞–º—è—Ç—å
                self.memory.remember(
                    text=entry.text,
                    source=f"autonomous_learning:{entry.source_url}",
                    category="learned",
                    force_long_term=True
                )
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                entry.status = "approved"
                entry.reviewed_by = "admin"
                entry.review_timestamp = datetime.now().isoformat()
                
                self.quarantine.remove(entry)
                self._save_quarantine()
                self.stats["quarantine_approved"] += 1
                
                logging.info(f"‚úÖ –û–¥–æ–±—Ä–µ–Ω–æ –≤—Ä—É—á–Ω—É—é: {entry.text[:60]}...")
                return True
        
        return False
    
    def manual_reject(self, entry_id: str) -> bool:
        """–†—É—á–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –∏–∑ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞"""
        for entry in self.quarantine:
            if entry.id == entry_id:
                entry.status = "rejected"
                entry.reviewed_by = "admin"
                entry.review_timestamp = datetime.now().isoformat()
                
                self.quarantine.remove(entry)
                self._save_quarantine()
                self.stats["quarantine_rejected"] += 1
                
                logging.info(f"‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–æ –≤—Ä—É—á–Ω—É—é: {entry.text[:60]}...")
                return True
        
        return False
    
    def get_quarantine_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞"""
        return {
            "total": len(self.quarantine),
            "pending": len([e for e in self.quarantine if e.status == "pending"]),
            "high_confidence": len([e for e in self.quarantine if e.confidence >= 0.9]),
            "needs_review": len([e for e in self.quarantine if e.confidence < 0.9]),
        }
    
    def get_learning_stats(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
        return {
            **self.stats,
            "quarantine": self.get_quarantine_stats(),
            "idle_minutes": round((datetime.now() - self.last_activity).total_seconds() / 60, 1),
            "is_idle": self.is_idle(),
            "running": self.running,
            "whitelist_sources": len(SourceWhitelist.CURATED_SOURCES),
            "blacklist_patterns": len(SourceWhitelist.BLACKLIST_PATTERNS),
        }


if __name__ == "__main__":
    # –¢–µ—Å—Ç—ã
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n")
    
    # –¢–µ—Å—Ç whitelist
    print("üìã –¢–µ—Å—Ç whitelist –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:")
    test_urls = [
        "https://docs.python.org/3/library/asyncio.html",
        "https://ru.wikipedia.org/wiki/Python",
        "https://some-random-blog.com/article",
        "https://github.com/python/cpython/blob/main/README.md",
        "https://reddit.com/r/python",
        "https://suspicious-site.xyz/article",
    ]
    
    for url in test_urls:
        trusted, trust_level, source_name = SourceWhitelist.is_trusted(url)
        status = f"‚úÖ {source_name} ({trust_level:.0%})" if trusted else "‚ùå –ù–µ–¥–æ–≤–µ—Ä–µ–Ω–Ω—ã–π"
        print(f"  {status}: {url}")
    
    print("\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏!")
