"""
AutonomyEngine v1.0 ‚Äî –î–≤–∏–∂–æ–∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏ Neira

Phase 3: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å –±–µ–∑ LLM

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
1. SemanticClusterer ‚Äî –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤/pathways
2. QualityPredictor ‚Äî –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
3. ContextAwareCache ‚Äî –∫—ç—à —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
4. AutonomyDecider ‚Äî —Ä–µ—à–µ–Ω–∏–µ: –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏–ª–∏ LLM?
5. SelfMonitor ‚Äî –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏

–¶–µ–ª—å: –î–æ—Å—Ç–∏—á—å 70%+ –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞
"""

import hashlib
import json
import logging
import math
import re
import time
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Set, Tuple

from neira_brain import get_brain, NeiraBrain
from local_embeddings import get_local_embedding, cosine_similarity, find_similar

logger = logging.getLogger("AutonomyEngine")


# ============== Semantic Clusterer ==============

class SemanticClusterer:
    """
    –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ pathways
    
    –¶–µ–ª–∏:
    - –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è pathways
    - –ù–∞–π—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö
    - –£–ª—É—á—à–∏—Ç—å coverage –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    CLUSTER_SIMILARITY_THRESHOLD = 0.85
    MIN_CLUSTER_SIZE = 2
    
    def __init__(self, brain: Optional[NeiraBrain] = None):
        self.brain = brain or get_brain()
        self._clusters: Dict[str, List[str]] = {}  # cluster_id -> [pathway_ids]
        self._query_patterns: Dict[str, Dict] = {}  # pattern -> {count, examples}
        
        logger.info("üîó SemanticClusterer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def cluster_pathways(self) -> Dict[str, Any]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞—Ç—å –≤—Å–µ pathways –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        """
        pathways = self.brain.query("SELECT * FROM pathways WHERE confidence_threshold > 0.3")
        
        if not pathways:
            return {"clusters": 0, "merged": 0}
        
        # –ü–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö triggers
        embeddings = {}
        for p in pathways:
            triggers = json.loads(p['triggers']) if p['triggers'] else []
            if triggers:
                trigger_text = " ".join(triggers)
                emb = get_local_embedding(trigger_text)
                if emb:
                    embeddings[p['id']] = {
                        'embedding': emb,
                        'pathway': dict(p)
                    }
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º
        clusters: List[List[str]] = []
        used = set()
        
        pathway_ids = list(embeddings.keys())
        
        for i, pid1 in enumerate(pathway_ids):
            if pid1 in used:
                continue
            
            cluster = [pid1]
            used.add(pid1)
            
            for pid2 in pathway_ids[i+1:]:
                if pid2 in used:
                    continue
                
                sim = cosine_similarity(
                    embeddings[pid1]['embedding'],
                    embeddings[pid2]['embedding']
                )
                
                if sim >= self.CLUSTER_SIMILARITY_THRESHOLD:
                    cluster.append(pid2)
                    used.add(pid2)
            
            if len(cluster) >= self.MIN_CLUSTER_SIZE:
                clusters.append(cluster)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ –º–µ—Ä–∂–∏–º –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        merged_count = 0
        for i, cluster in enumerate(clusters):
            cluster_id = f"cluster_{i}"
            self._clusters[cluster_id] = cluster
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ä–∂ –µ—Å–ª–∏ –≤—Å–µ –≤ –æ–¥–Ω–æ–º tier
            if self._should_merge(cluster, embeddings):
                self._merge_cluster(cluster, embeddings)
                merged_count += 1
        
        stats = {
            "total_pathways": len(pathways),
            "clustered": sum(len(c) for c in clusters),
            "clusters": len(clusters),
            "merged": merged_count
        }
        
        logger.info(f"üîó –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {stats}")
        return stats
    
    def _should_merge(self, cluster: List[str], embeddings: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –∫–ª–∞—Å—Ç–µ—Ä"""
        if len(cluster) < 2:
            return False
        
        tiers = set()
        for pid in cluster:
            p = embeddings[pid]['pathway']
            tiers.add(p.get('tier', 'cold'))
        
        # –ú–µ—Ä–∂–∏–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—Å–µ cold –∏–ª–∏ –≤—Å–µ warm
        return len(tiers) == 1 and 'hot' not in tiers
    
    def _merge_cluster(self, cluster: List[str], embeddings: Dict):
        """–û–±—ä–µ–¥–∏–Ω–∏—Ç—å pathways –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –≤ –æ–¥–∏–Ω"""
        if len(cluster) < 2:
            return
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π pathway (–ø–æ success_count)
        best_id = max(
            cluster, 
            key=lambda pid: embeddings[pid]['pathway'].get('success_count', 0)
        )
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º triggers –∏ success_count
        all_triggers = set()
        total_success = 0
        total_fail = 0
        
        for pid in cluster:
            p = embeddings[pid]['pathway']
            triggers = json.loads(p['triggers']) if p['triggers'] else []
            all_triggers.update(triggers)
            total_success += p.get('success_count', 0) or 0
            total_fail += p.get('failure_count', 0) or 0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π
        self.brain.execute("""
            UPDATE pathways 
            SET triggers = ?,
                success_count = ?,
                failure_count = ?,
                last_used = CURRENT_TIMESTAMP
            WHERE id = ?
        """, (json.dumps(list(all_triggers)), total_success, total_fail, best_id))
        
        # –£–¥–∞–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        for pid in cluster:
            if pid != best_id:
                self.brain.execute("DELETE FROM pathways WHERE id = ?", (pid,))
        
        logger.info(f"üîó Merged cluster: {cluster} ‚Üí {best_id}")
    
    def find_query_patterns(self, min_frequency: int = 3) -> List[Dict]:
        """
        –ù–∞–π—Ç–∏ —á–∞—Å—Ç—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ –º–µ—Ç—Ä–∏–∫
        recent = self.brain.query("""
            SELECT data FROM metrics 
            WHERE event_type = 'request' 
            AND timestamp > datetime('now', '-7 days')
            LIMIT 1000
        """)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns: Dict[str, Dict] = defaultdict(lambda: {'count': 0, 'examples': []})
        
        for row in recent:
            try:
                data = json.loads(row['data']) if row['data'] else {}
                query = data.get('message_preview', '')
                
                if len(query) < 10:
                    continue
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω (–ø–µ—Ä–≤—ã–µ 3 –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤–∞)
                words = re.findall(r'[–∞-—èa-z]{4,}', query.lower())[:3]
                if len(words) >= 2:
                    pattern = " ".join(sorted(words))
                    patterns[pattern]['count'] += 1
                    if len(patterns[pattern]['examples']) < 3:
                        patterns[pattern]['examples'].append(query)
            except:
                continue
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        result = [
            {'pattern': p, **data}
            for p, data in patterns.items()
            if data['count'] >= min_frequency
        ]
        
        return sorted(result, key=lambda x: x['count'], reverse=True)


# ============== Quality Predictor ==============

class ResponseQuality(Enum):
    """–£—Ä–æ–≤–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
    EXCELLENT = "excellent"  # 90%+ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    GOOD = "good"            # 70-90% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    ACCEPTABLE = "acceptable" # 50-70% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    UNCERTAIN = "uncertain"   # <50% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    UNKNOWN = "unknown"       # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö


@dataclass
class QualityPrediction:
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
    quality: ResponseQuality
    confidence: float  # 0.0 - 1.0
    source: str  # –æ—Ç–∫—É–¥–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    factors: Dict[str, float] = field(default_factory=dict)
    recommendation: str = ""  # "use_autonomous" / "use_llm" / "hybrid"


class QualityPredictor:
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞ –î–û –µ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    
    –§–∞–∫—Ç–æ—Ä—ã:
    - –ü–æ—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    - –ù–∞–ª–∏—á–∏–µ pathway/cache
    - –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
    - –ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    
    # –í–µ—Å–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤
    WEIGHTS = {
        'pathway_match': 0.35,
        'cache_match': 0.25,
        'query_complexity': 0.20,
        'user_history': 0.20
    }
    
    def __init__(self, brain: Optional[NeiraBrain] = None):
        self.brain = brain or get_brain()
        self._complexity_cache: Dict[str, float] = {}
        
        logger.info("üìä QualityPredictor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def predict(
        self, 
        query: str, 
        user_id: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> QualityPrediction:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        
        Args:
            query: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            
        Returns:
            QualityPrediction —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–µ–π
        """
        factors = {}
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º pathway match
        factors['pathway_match'] = self._check_pathway_match(query)
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º cache match
        factors['cache_match'] = self._check_cache_match(query)
        
        # 3. –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
        factors['query_complexity'] = self._assess_complexity(query)
        
        # 4. –ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        factors['user_history'] = self._check_user_history(user_id) if user_id else 0.5
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        confidence = sum(
            factors[k] * self.WEIGHTS[k] 
            for k in self.WEIGHTS
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if confidence >= 0.85:
            quality = ResponseQuality.EXCELLENT
            recommendation = "use_autonomous"
        elif confidence >= 0.70:
            quality = ResponseQuality.GOOD
            recommendation = "use_autonomous"
        elif confidence >= 0.50:
            quality = ResponseQuality.ACCEPTABLE
            recommendation = "hybrid"  # –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π + –ø—Ä–æ–≤–µ—Ä–∫–∞
        else:
            quality = ResponseQuality.UNCERTAIN
            recommendation = "use_llm"
        
        return QualityPrediction(
            quality=quality,
            confidence=confidence,
            source="quality_predictor",
            factors=factors,
            recommendation=recommendation
        )
    
    def _check_pathway_match(self, query: str) -> float:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –∑–∞–ø—Ä–æ—Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç pathways"""
        pathways = self.brain.search_pathways(query)
        
        if not pathways:
            return 0.0
        
        best = pathways[0]
        tier = best.get('tier', 'cold')
        success = best.get('success_count', 0) or 0
        confidence = best.get('confidence', 0.5) or 0.5
        
        # –ë–∞–∑–æ–≤—ã–π score –æ—Ç tier
        tier_score = {'hot': 1.0, 'warm': 0.7, 'cold': 0.4}.get(tier, 0.3)
        
        # –ë–æ–Ω—É—Å –∑–∞ success
        success_bonus = min(0.3, success * 0.03)
        
        return min(1.0, tier_score * confidence + success_bonus)
    
    def _check_cache_match(self, query: str) -> float:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –µ—Å—Ç—å –ª–∏ –ø–æ—Ö–æ–∂–∏–π –∑–∞–ø—Ä–æ—Å –≤ –∫—ç—à–µ"""
        query_emb = get_local_embedding(query)
        if not query_emb:
            return 0.0
        
        # –ò—â–µ–º –≤ –∫—ç—à–µ
        cached = self.brain.query("""
            SELECT query, hit_count FROM cache 
            WHERE hit_count > 0
            ORDER BY created_at DESC
            LIMIT 100
        """)
        
        best_similarity = 0.0
        best_hit_count = 0
        
        for row in cached:
            cached_emb = get_local_embedding(row['query'])
            if cached_emb:
                sim = cosine_similarity(query_emb, cached_emb)
                if sim > best_similarity:
                    best_similarity = sim
                    best_hit_count = row['hit_count'] or 1
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º hit_count –≤ confidence (–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º)
        # –ß–µ–º –±–æ–ª—å—à–µ –æ–±—Ä–∞—â–µ–Ω–∏–π –∫ –∫—ç—à—É, —Ç–µ–º –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        cache_confidence = min(1.0, best_hit_count / 10.0) * 0.7 + 0.3
        
        if best_similarity >= 0.9:
            return cache_confidence
        elif best_similarity >= 0.8:
            return cache_confidence * 0.7
        elif best_similarity >= 0.7:
            return cache_confidence * 0.4
        
        return 0.0
    
    def _assess_complexity(self, query: str) -> float:
        """
        –û—Ü–µ–Ω–∏—Ç—å —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ (–∏–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: –ø—Ä–æ—Å—Ç–æ–π = –≤—ã—Å–æ–∫–∏–π score)
        """
        query_hash = hashlib.md5(query.encode()).hexdigest()
        
        if query_hash in self._complexity_cache:
            return self._complexity_cache[query_hash]
        
        # –§–∞–∫—Ç–æ—Ä—ã —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        word_count = len(query.split())
        has_code = bool(re.search(r'```|def |class |function ', query))
        has_questions = query.count('?')
        has_multiple_topics = len(re.findall(r'(?:–∏|—Ç–∞–∫–∂–µ|–µ—â—ë|–ø–ª—é—Å|–∫—Ä–æ–º–µ)', query.lower()))
        
        # –î–ª–∏–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å–ª–æ–∂–Ω–µ–µ
        length_penalty = min(1.0, word_count / 50)
        
        # –ö–æ–¥ —Å–ª–æ–∂–Ω–µ–µ
        code_penalty = 0.3 if has_code else 0.0
        
        # –ú–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å–ª–æ–∂–Ω–µ–µ
        question_penalty = min(0.3, has_questions * 0.1)
        
        # –ú–Ω–æ–≥–æ —Ç–µ–º —Å–ª–æ–∂–Ω–µ–µ
        topic_penalty = min(0.3, has_multiple_topics * 0.1)
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (–∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º: –ø—Ä–æ—Å—Ç–æ–π = 1.0)
        complexity = 1.0 - (length_penalty * 0.4 + code_penalty + question_penalty + topic_penalty)
        complexity = max(0.1, min(1.0, complexity))
        
        self._complexity_cache[query_hash] = complexity
        return complexity
    
    def _check_user_history(self, user_id: str) -> float:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Å —ç—Ç–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
        stats = self.brain.query("""
            SELECT 
                COUNT(*) as total,
                SUM(CASE WHEN json_extract(data, '$.success') = 1 THEN 1 ELSE 0 END) as success
            FROM metrics
            WHERE source = ?
            AND event_type = 'feedback'
            AND timestamp > datetime('now', '-30 days')
        """, (f"telegram_{user_id}",))
        
        if not stats or stats[0]['total'] == 0:
            return 0.5  # –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö ‚Äî –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ
        
        total = stats[0]['total']
        success = stats[0]['success'] or 0
        
        return success / total if total > 0 else 0.5


# ============== Context-Aware Cache ==============

@dataclass
class ConversationContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
    user_id: str
    messages: List[Dict[str, str]] = field(default_factory=list)
    topics: Set[str] = field(default_factory=set)
    mood: str = "neutral"
    started_at: datetime = field(default_factory=datetime.now)
    last_activity: datetime = field(default_factory=datetime.now)


class ContextAwareCache:
    """
    –ö—ç—à —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
    
    –£–ª—É—á—à–µ–Ω–∏—è –Ω–∞–¥ –æ–±—ã—á–Ω—ã–º –∫—ç—à–µ–º:
    - –£—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    - –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç —Ç–µ–º—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
    - –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    """
    
    CONTEXT_TIMEOUT_MINUTES = 30
    MAX_CONTEXT_MESSAGES = 10
    
    def __init__(self, brain: Optional[NeiraBrain] = None):
        self.brain = brain or get_brain()
        self._contexts: Dict[str, ConversationContext] = {}
        
        logger.info("üóÇÔ∏è ContextAwareCache –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def get_context(self, user_id: str) -> ConversationContext:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        now = datetime.now()
        
        if user_id in self._contexts:
            ctx = self._contexts[user_id]
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º timeout
            if (now - ctx.last_activity).total_seconds() > self.CONTEXT_TIMEOUT_MINUTES * 60:
                # –ö–æ–Ω—Ç–µ–∫—Å—Ç —É—Å—Ç–∞—Ä–µ–ª, —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
                ctx = ConversationContext(user_id=user_id)
                self._contexts[user_id] = ctx
            else:
                ctx.last_activity = now
        else:
            ctx = ConversationContext(user_id=user_id)
            self._contexts[user_id] = ctx
        
        return ctx
    
    def add_message(self, user_id: str, role: str, content: str):
        """–î–æ–±–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç"""
        ctx = self.get_context(user_id)
        
        ctx.messages.append({
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat()
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
        if len(ctx.messages) > self.MAX_CONTEXT_MESSAGES:
            ctx.messages = ctx.messages[-self.MAX_CONTEXT_MESSAGES:]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–º—ã
        topics = self._extract_topics(content)
        ctx.topics.update(topics)
    
    def get_contextual_response(self, user_id: str, query: str) -> Optional[str]:
        """
        –ù–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        """
        ctx = self.get_context(user_id)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        context_keywords = list(ctx.topics)[:5]
        recent_messages = [m['content'] for m in ctx.messages[-3:]]
        
        # –ò—â–µ–º –≤ –∫—ç—à–µ —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        query_emb = get_local_embedding(query)
        if not query_emb:
            return None
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–º
        cached = self.brain.query("""
            SELECT query, response, hit_count FROM cache
            WHERE hit_count > 0
            ORDER BY created_at DESC
            LIMIT 200
        """)
        
        best_match = None
        best_score = 0.0
        
        for row in cached:
            cached_emb = get_local_embedding(row['query'])
            if not cached_emb:
                continue
            
            # –ë–∞–∑–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            base_sim = cosine_similarity(query_emb, cached_emb)
            
            # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_bonus = 0.0
            cached_topics = self._extract_topics(row['query'])
            common_topics = ctx.topics & cached_topics
            if common_topics:
                context_bonus = min(0.15, len(common_topics) * 0.05)
            
            # –ò—Ç–æ–≥–æ–≤—ã–π score
            score = base_sim + context_bonus
            
            if score > best_score and score >= 0.85:
                best_score = score
                best_match = row['response']
        
        return best_match
    
    def _extract_topics(self, text: str) -> Set[str]:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–º—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        topics = set()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        topic_patterns = {
            '–∫–æ–¥': r'\b(–∫–æ–¥|—Ñ—É–Ω–∫—Ü–∏|–∫–ª–∞—Å—Å|–º–µ—Ç–æ–¥|python|javascript|–ø—Ä–æ–≥—Ä–∞–º–º)\w*',
            '–æ—à–∏–±–∫–∞': r'\b(–æ—à–∏–±–∫|error|bug|–∏—Å–ø—Ä–∞–≤|fix)\w*',
            '–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ': r'\b(–æ–±—ä—è—Å–Ω|—Ä–∞—Å—Å–∫–∞–∂|–ø–æ—á–µ–º—É|–∑–∞—á–µ–º|–∫–∞–∫)\w*',
            '—Å–æ–∑–¥–∞–Ω–∏–µ': r'\b(—Å–æ–∑–¥–∞–π|—Å–¥–µ–ª–∞–π|–Ω–∞–ø–∏—à–∏|–≥–µ–Ω–µ—Ä–∏—Ä)\w*',
            '–∞–Ω–∞–ª–∏–∑': r'\b(–∞–Ω–∞–ª–∏–∑|—Ä–∞–∑–±–µ—Ä|–ø—Ä–æ–≤–µ—Ä|–æ—Ü–µ–Ω)\w*',
        }
        
        text_lower = text.lower()
        for topic, pattern in topic_patterns.items():
            if re.search(pattern, text_lower):
                topics.add(topic)
        
        return topics
    
    def clear_context(self, user_id: str):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        if user_id in self._contexts:
            del self._contexts[user_id]


# ============== Autonomy Decider ==============

class AutonomyDecision(Enum):
    """–†–µ—à–µ–Ω–∏–µ –æ —Å–ø–æ—Å–æ–±–µ –æ—Ç–≤–µ—Ç–∞"""
    AUTONOMOUS = "autonomous"      # –ü–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ
    HYBRID = "hybrid"              # –ê–≤—Ç–æ–Ω–æ–º–Ω–æ + –ø—Ä–æ–≤–µ—Ä–∫–∞ LLM
    LLM_REQUIRED = "llm_required"  # –¢–æ–ª—å–∫–æ LLM
    ESCALATE = "escalate"          # –¢—Ä–µ–±—É–µ—Ç—Å—è —á–µ–ª–æ–≤–µ–∫


@dataclass
class DecisionResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–µ—à–µ–Ω–∏—è"""
    decision: AutonomyDecision
    confidence: float
    reasoning: str
    suggested_source: str  # pathway/cache/llm
    fallback_enabled: bool = True


class AutonomyDecider:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
    
    –†–µ—à–∞–µ—Ç: –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–≤–µ—Ç–∏—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏–ª–∏ –Ω—É–∂–µ–Ω LLM?
    
    –£—á–∏—Ç—ã–≤–∞–µ—Ç:
    - QualityPrediction
    - –¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞
    - –†–∏—Å–∫–∏ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
    - –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    """
    
    # –¢–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –≥–¥–µ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–∞
    HIGH_RISK_PATTERNS = [
        r'\b(—É–¥–∞–ª–∏|—É–¥–∞–ª–∏—Ç—å|remove|delete)\b.*\b(—Ñ–∞–π–ª|–¥–∞–Ω–Ω—ã|–≤—Å—ë)\b',
        r'\b(–¥–µ–Ω—å–≥|–æ–ø–ª–∞—Ç|–±–∞–Ω–∫|–∫–∞—Ä—Ç)\w*',
        r'\b(–ø–∞—Ä–æ–ª|secret|key|token|api.?key)\w*',
        r'\b(sudo|admin|root|chmod)\b',
    ]
    
    # –¢–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–¥–µ–∞–ª—å–Ω—ã–µ –¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏
    SAFE_PATTERNS = [
        r'^–ø—Ä–∏–≤–µ—Ç\b|^–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π',
        r'^–∫–∞–∫ –¥–µ–ª–∞|^—á—Ç–æ –Ω–æ–≤–æ–≥–æ',
        r'^—Å–ø–∞—Å–∏–±–æ|^–±–ª–∞–≥–æ–¥–∞—Ä—é',
        r'^–ø–æ–º–æ—â—å$|^help$',
        r'^—á—Ç–æ (—Ç—ã )?(—É–º–µ–µ—à—å|–º–æ–∂–µ—à—å)',
    ]
    
    def __init__(
        self,
        quality_predictor: Optional[QualityPredictor] = None,
        brain: Optional[NeiraBrain] = None
    ):
        self.brain = brain or get_brain()
        self.quality_predictor = quality_predictor or QualityPredictor(self.brain)
        
        logger.info("ü§î AutonomyDecider –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def decide(
        self,
        query: str,
        user_id: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> DecisionResult:
        """
        –ü—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ —Å–ø–æ—Å–æ–±–µ –æ—Ç–≤–µ—Ç–∞
        """
        query_lower = query.lower().strip()
        
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º safe patterns (–±—ã—Å—Ç—Ä—ã–π autonomous)
        for pattern in self.SAFE_PATTERNS:
            if re.match(pattern, query_lower):
                # –ù–µ —Å—á–∏—Ç–∞—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è "–±–µ–∑–æ–ø–∞—Å–Ω—ã–º" –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö/—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.
                # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–µ–µ 3 —Å–ª–æ–≤ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ/–∑–Ω–∞–∫ ‚Äî —Ç—Ä–µ–±—É–µ–º LLM.
                tokens = query_lower.split()
                if len(tokens) > 3 or '?' in query_lower or query_lower.startswith('–∫–∞–∫ '):
                    # –Ω–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å –±—ã—Å—Ç—Ä—ã–π autonomous, –ø—Ä–æ–¥–æ–ª–∂–∏–º –¥–∞–ª—å–Ω–µ–π—à—É—é –ª–æ–≥–∏–∫—É
                    break
                return DecisionResult(
                    decision=AutonomyDecision.AUTONOMOUS,
                    confidence=0.95,
                    reasoning="Safe pattern match",
                    suggested_source="pathway"
                )
        
        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º high-risk patterns (—Ç—Ä–µ–±—É–µ—Ç—Å—è LLM)
        for pattern in self.HIGH_RISK_PATTERNS:
            if re.search(pattern, query_lower):
                return DecisionResult(
                    decision=AutonomyDecision.LLM_REQUIRED,
                    confidence=0.9,
                    reasoning="High-risk pattern detected",
                    suggested_source="llm",
                    fallback_enabled=False
                )
        
        # 3. –ü–æ–ª—É—á–∞–µ–º quality prediction
        prediction = self.quality_predictor.predict(query, user_id, context)
        
        # 4. –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ prediction
        if prediction.quality == ResponseQuality.EXCELLENT:
            decision = AutonomyDecision.AUTONOMOUS
            suggested = "pathway" if prediction.factors['pathway_match'] > 0.7 else "cache"
        
        elif prediction.quality == ResponseQuality.GOOD:
            decision = AutonomyDecision.AUTONOMOUS
            suggested = "cache" if prediction.factors['cache_match'] > 0.5 else "pathway"
        
        elif prediction.quality == ResponseQuality.ACCEPTABLE:
            decision = AutonomyDecision.HYBRID
            suggested = "pathway"
        
        else:
            decision = AutonomyDecision.LLM_REQUIRED
            suggested = "llm"
        
        return DecisionResult(
            decision=decision,
            confidence=prediction.confidence,
            reasoning=f"Quality: {prediction.quality.value}, factors: {prediction.factors}",
            suggested_source=suggested
        )


# ============== Self Monitor ==============

@dataclass
class AutonomyMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏"""
    total_requests: int = 0
    autonomous_responses: int = 0
    llm_responses: int = 0
    hybrid_responses: int = 0
    
    # –ö–∞—á–µ—Å—Ç–≤–æ
    positive_feedback: int = 0
    negative_feedback: int = 0
    
    # –ü–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
    pathway_hits: int = 0
    cache_hits: int = 0
    
    # –í—Ä–µ–º—è
    avg_autonomous_latency_ms: float = 0.0
    avg_llm_latency_ms: float = 0.0
    
    @property
    def autonomy_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return (self.autonomous_responses + self.hybrid_responses * 0.5) / self.total_requests
    
    @property
    def quality_score(self) -> float:
        total_feedback = self.positive_feedback + self.negative_feedback
        if total_feedback == 0:
            return 0.5
        return self.positive_feedback / total_feedback


class SelfMonitor:
    """
    –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏
    
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç:
    - % –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    - –ö–∞—á–µ—Å—Ç–≤–æ (—á–µ—Ä–µ–∑ feedback)
    - Latency
    - –¢—Ä–µ–Ω–¥—ã
    """
    
    TARGET_AUTONOMY_RATE = 0.70  # –¶–µ–ª—å: 70%
    MIN_QUALITY_SCORE = 0.80     # –ú–∏–Ω–∏–º—É–º: 80% –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö
    
    def __init__(self, brain: Optional[NeiraBrain] = None):
        self.brain = brain or get_brain()
        self._session_metrics = AutonomyMetrics()
        self._latencies: List[Tuple[str, float]] = []  # (source, ms)
        
        logger.info("üìà SelfMonitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def record_response(
        self,
        source: str,
        latency_ms: float,
        was_autonomous: bool
    ):
        """–ó–∞–ø–∏—Å–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç–≤–µ—Ç–µ"""
        self._session_metrics.total_requests += 1
        
        if was_autonomous:
            self._session_metrics.autonomous_responses += 1
            if 'pathway' in source:
                self._session_metrics.pathway_hits += 1
            elif 'cache' in source:
                self._session_metrics.cache_hits += 1
        else:
            self._session_metrics.llm_responses += 1
        
        self._latencies.append((source, latency_ms))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ
        autonomous_latencies = [l for s, l in self._latencies if 'llm' not in s]
        llm_latencies = [l for s, l in self._latencies if 'llm' in s]
        
        if autonomous_latencies:
            self._session_metrics.avg_autonomous_latency_ms = sum(autonomous_latencies) / len(autonomous_latencies)
        if llm_latencies:
            self._session_metrics.avg_llm_latency_ms = sum(llm_latencies) / len(llm_latencies)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –ë–î
        self.brain.record_metric('response', source, {
            'latency_ms': latency_ms,
            'autonomous': was_autonomous
        })
    
    def record_feedback(self, positive: bool):
        """–ó–∞–ø–∏—Å–∞—Ç—å feedback"""
        if positive:
            self._session_metrics.positive_feedback += 1
        else:
            self._session_metrics.negative_feedback += 1
    
    def get_metrics(self) -> AutonomyMetrics:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏"""
        return self._session_metrics
    
    def get_recommendations(self) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é"""
        recommendations = []
        metrics = self._session_metrics
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º autonomy rate
        if metrics.autonomy_rate < self.TARGET_AUTONOMY_RATE:
            gap = self.TARGET_AUTONOMY_RATE - metrics.autonomy_rate
            recommendations.append(
                f"‚ö†Ô∏è Autonomy rate ({metrics.autonomy_rate:.1%}) –Ω–∏–∂–µ —Ü–µ–ª–∏ ({self.TARGET_AUTONOMY_RATE:.0%}). "
                f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ pathways –∏–ª–∏ —É–ª—É—á—à–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ."
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if metrics.quality_score < self.MIN_QUALITY_SCORE and metrics.total_requests > 10:
            recommendations.append(
                f"‚ö†Ô∏è Quality score ({metrics.quality_score:.1%}) –Ω–∏–∂–µ –º–∏–Ω–∏–º—É–º–∞ ({self.MIN_QUALITY_SCORE:.0%}). "
                f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å pathways —Å –Ω–∏–∑–∫–∏–º success_count."
            )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º latency
        if metrics.avg_autonomous_latency_ms > 500:
            recommendations.append(
                f"‚ö†Ô∏è –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è latency ({metrics.avg_autonomous_latency_ms:.0f}ms) –≤—ã—Å–æ–∫–∞—è. "
                f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∏—Å–∫ pathways."
            )
        
        if not recommendations:
            recommendations.append(
                f"‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ: autonomy={metrics.autonomy_rate:.1%}, "
                f"quality={metrics.quality_score:.1%}"
            )
        
        return recommendations
    
    def get_dashboard(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—à–±–æ—Ä–¥–∞"""
        metrics = self._session_metrics
        
        return {
            "autonomy": {
                "rate": f"{metrics.autonomy_rate:.1%}",
                "target": f"{self.TARGET_AUTONOMY_RATE:.0%}",
                "status": "‚úÖ" if metrics.autonomy_rate >= self.TARGET_AUTONOMY_RATE else "‚ö†Ô∏è"
            },
            "quality": {
                "score": f"{metrics.quality_score:.1%}",
                "positive": metrics.positive_feedback,
                "negative": metrics.negative_feedback
            },
            "performance": {
                "total_requests": metrics.total_requests,
                "autonomous": metrics.autonomous_responses,
                "llm": metrics.llm_responses,
                "pathway_hits": metrics.pathway_hits,
                "cache_hits": metrics.cache_hits
            },
            "latency": {
                "autonomous_avg_ms": round(metrics.avg_autonomous_latency_ms, 1),
                "llm_avg_ms": round(metrics.avg_llm_latency_ms, 1)
            },
            "recommendations": self.get_recommendations()
        }


# ============== Main Autonomy Engine ==============

class AutonomyEngine:
    """
    –ì–ª–∞–≤–Ω—ã–π –¥–≤–∏–∂–æ–∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Phase 3
    """
    
    def __init__(self, brain: Optional[NeiraBrain] = None):
        self.brain = brain or get_brain()
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.clusterer = SemanticClusterer(self.brain)
        self.quality_predictor = QualityPredictor(self.brain)
        self.context_cache = ContextAwareCache(self.brain)
        self.decider = AutonomyDecider(self.quality_predictor, self.brain)
        self.monitor = SelfMonitor(self.brain)
        
        logger.info("üöÄ AutonomyEngine v1.0 –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def should_respond_autonomous(
        self,
        query: str,
        user_id: Optional[str] = None,
        context: Optional[Dict] = None
    ) -> DecisionResult:
        """
        –†–µ—à–∏—Ç—å: –æ—Ç–≤–µ—á–∞—Ç—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LLM?
        """
        return self.decider.decide(query, user_id, context)
    
    def get_contextual_response(
        self,
        query: str,
        user_id: str
    ) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        """
        return self.context_cache.get_contextual_response(user_id, query)
    
    def update_context(self, user_id: str, role: str, content: str):
        """–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        self.context_cache.add_message(user_id, role, content)
    
    def record_response(
        self,
        source: str,
        latency_ms: float,
        was_autonomous: bool
    ):
        """–ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É –æ—Ç–≤–µ—Ç–∞"""
        self.monitor.record_response(source, latency_ms, was_autonomous)
    
    def record_feedback(self, positive: bool):
        """–ó–∞–ø–∏—Å–∞—Ç—å feedback"""
        self.monitor.record_feedback(positive)
    
    def optimize(self) -> Dict[str, Any]:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏
        
        - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è pathways
        - –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        """
        results = {}
        
        # 1. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        results['clustering'] = self.clusterer.cluster_pathways()
        
        # 2. –ü–∞—Ç—Ç–µ—Ä–Ω—ã
        results['patterns'] = self.clusterer.find_query_patterns()[:10]
        
        # 3. Dashboard
        results['dashboard'] = self.monitor.get_dashboard()
        
        logger.info(f"üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {results}")
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return self.monitor.get_dashboard()


# ============== Global Instance ==============

_autonomy_engine: Optional[AutonomyEngine] = None


def get_autonomy_engine() -> AutonomyEngine:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä AutonomyEngine"""
    global _autonomy_engine
    if _autonomy_engine is None:
        _autonomy_engine = AutonomyEngine()
    return _autonomy_engine


# ============== Test ==============

if __name__ == "__main__":
    import os
    os.environ["NEIRA_LOCAL_EMBEDDINGS"] = "true"
    
    print("üß™ –¢–µ—Å—Ç AutonomyEngine v1.0")
    print("=" * 60)
    
    engine = get_autonomy_engine()
    
    # –¢–µ—Å—Ç 1: Safe pattern
    decision = engine.should_respond_autonomous("–ü—Ä–∏–≤–µ—Ç!")
    print(f"\n1. '–ü—Ä–∏–≤–µ—Ç!' ‚Üí {decision.decision.value} (conf: {decision.confidence:.2f})")
    print(f"   Reasoning: {decision.reasoning}")
    
    # –¢–µ—Å—Ç 2: High-risk pattern  
    decision = engine.should_respond_autonomous("–£–¥–∞–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ")
    print(f"\n2. '–£–¥–∞–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã' ‚Üí {decision.decision.value} (conf: {decision.confidence:.2f})")
    print(f"   Reasoning: {decision.reasoning}")
    
    # –¢–µ—Å—Ç 3: –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    decision = engine.should_respond_autonomous("–ö–∞–∫ –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ Python?")
    print(f"\n3. '–ö–∞–∫ –Ω–∞–ø–∏—Å–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é' ‚Üí {decision.decision.value} (conf: {decision.confidence:.2f})")
    print(f"   Reasoning: {decision.reasoning}")
    
    # –¢–µ—Å—Ç 4: –ö–æ–Ω—Ç–µ–∫—Å—Ç
    engine.update_context("user123", "user", "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python")
    engine.update_context("user123", "assistant", "Python ‚Äî —ç—Ç–æ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è...")
    ctx = engine.context_cache.get_context("user123")
    print(f"\n4. –ö–æ–Ω—Ç–µ–∫—Å—Ç user123: {len(ctx.messages)} —Å–æ–æ–±—â–µ–Ω–∏–π, —Ç–µ–º—ã: {ctx.topics}")
    
    # –¢–µ—Å—Ç 5: Dashboard
    engine.record_response("pathway:test", 50.0, True)
    engine.record_response("llm", 2000.0, False)
    engine.record_feedback(True)
    
    print("\n5. Dashboard:")
    dashboard = engine.get_stats()
    for key, value in dashboard.items():
        print(f"   {key}: {value}")
    
    print("\nüéâ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
