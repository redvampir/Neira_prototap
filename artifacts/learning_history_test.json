[
  {
    "source": "F:\\–ù–µ–π—Ä–æ–Ω–∫–∏\\prototype\\README.md",
    "source_type": "file",
    "title": "README.md",
    "word_count": 2380,
    "category": "documentation",
    "learned_at": "2025-12-28T15:17:56.338263",
    "summary": "# NEIRA v0.8.2 ‚Äî –ê–≤—Ç–æ–Ω–æ–º–Ω–∞—è –≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∞—è –ü—Ä–æ–≥—Ä–∞–º–º–∞ ## üåø Neira Web (Next.js, v2 MVP) - `cd neira-app` - `npm install` - `npm run dev` - –æ—Ç–∫—Ä—ã—Ç—å `http://localhost:3000` - `npm run check` - `npm run build` –ö–æ–¥: `neira-app/organism` ‚Üí `neira-app/manifestation` ‚Üí `neira-app/ritual`. üìñ **(.md)** | **(USER_MANAGEMENT_GUIDE.md)** **–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç:** ```bash pip install -r requirements-.txt echo \"_BOT_TOKEN=–≤–∞—à_—Ç–æ–∫–µ–Ω_–æ—Ç_BotFather\" >> .env python _bot.py ``` **–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:** ``` /help # –°–ø—Ä–∞–≤–∫–∞ –ø–æ –∫–æ–º–∞–Ω–¥–∞–º /myname –ò–º—è # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–≤–æ—ë –∏–º—è /feedback # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ emoji-–æ–±—É—á–µ–Ω–∏—è (–∞–¥–º–∏–Ω) /imagine <–æ–ø–∏—Å–∞–Ω–∏–µ> # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç–∏–Ω–æ–∫ /vision # –°—Ç–∞—Ç—É—Å vision —Å–∏—Å—Ç–µ–º # + –æ—Ç–ø—Ä–∞–≤—å —Ñ–æ—Ç–æ ‚Äî –ù–µ–π—Ä–∞ –æ–ø–∏—à–µ—Ç —á—Ç–æ –≤–∏–¥–∏—Ç ``` **–ê–¥–º–∏–Ω-–∫–æ–º–∞–Ω–¥—ã:** ``` /admin # –ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è /admin users # –°–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π /admin add <user_id> # –î–æ–±–∞–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è /admin mode whitelist # –†–µ–∂–∏–º –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ``` ``` --- ## üñ•Ô∏è VS Code Extension v1.0 **Neira AI Assistant** ‚Äî –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π AI-–∞–≥–µ–Ω—Ç –¥–ª—è VS Code –∏ Cursor! ### –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: - üí¨ **–ß–∞—Ç —Å –ù–µ–π—Ä–æ–π** ‚Äî –±–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å streaming –æ—Ç–≤–µ—Ç–∞–º–∏ - üìÇ **File System Agent** ‚Äî —á—Ç–µ–Ω–∏–µ/–∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–æ–≤, grep-–ø–æ–∏—Å–∫ - üîß **Tool Use** ‚Äî –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥, –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ - ‚úèÔ∏è **Multi-file Edit** ‚Äî —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ —Å diff preview - üìä **Workspace Indexer** ‚Äî –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤ (Ctrl+T) - üß† **Context Manager** ‚Äî —É–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º ### –°–∞–º–æ–æ—Å–æ–∑–Ω–∞–Ω–∏–µ –ù–µ–π—Ä—ã: - **–ò–Ω—Ç—Ä–æ—Å–ø–µ–∫—Ü–∏—è** ‚Äî –ø—Ä–æ—Å–º–æ—Ç—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ—Ä–≥–∞–Ω–æ–≤ - **–°–∏—Å—Ç–µ–º–∞ –æ–ø—ã—Ç–∞** ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ —É—Å–ø–µ—Ö–æ–≤/–Ω–µ—É–¥–∞—á - **–õ—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ** ‚Äî –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –æ –∫–æ–¥–µ - **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å** ‚Äî –ø–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ **–£—Å—Ç–∞–Ω–æ–≤–∫–∞:** ```bash cd neira-vscode npm install && npm run compile # F5 –≤ VS Code –¥–ª—è –∑–∞–ø—É—Å–∫–∞ ``` üìñ **–ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** (neira-vscode/README.md) --- ## üß¨ –ß—Ç–æ –±—ã–ª–æ –≤ v0.6 ### –ü–æ–ª–Ω–∞—è –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è —ç–≤–æ–ª—é—Ü–∏—è! **Fine-Tuning Pipeline** ‚Äî –¥–æ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –ª–∏—á–Ω–æ—Å—Ç–∏ –Ω–∞ –æ–ø—ã—Ç–µ üìñ **–ü–æ–¥—Ä–æ–±–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è:** (EVOLUTION.md) ### –ù–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã ```bash /evolution stats # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º —ç–≤–æ–ª—é—Ü–∏–∏ /evolution log # –õ–æ–≥ —ç–≤–æ–ª—é—Ü–∏–∏ /evolution cycle # –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ—ç–≤–æ–ª—é—Ü–∏—é /evolution help # –ü–æ–ª–Ω–∞—è —Å–ø—Ä–∞–≤–∫–∞ ``` --- ## –ß—Ç–æ –±—ã–ª–æ –≤ v0.5 ### –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å + —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–º–æ—â–Ω–∏–∫–∏ **–û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å** (RTX 3060 Ti): - **nemotron-mini** (NVIDIA Nemotron Nano 9B v2, ~6 –ì–ë VRAM) ‚Äî —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –¥–ª—è –∫–æ–¥–∞, —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –∏ –¥–∏–∞–ª–æ–≥–æ–≤ - 9B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, 128K –∫–æ–Ω—Ç–µ–∫—Å—Ç - –ó–∞–º–µ–Ω—è–µ—Ç ministral-3:3b, qwen2.5-coder:7b, neira-personality **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏**: - **llava:7b** (~7 –ì–ë VRAM) ‚Äî –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π - **nomic-embed-text** (~0.5 –ì–ë VRAM) ‚Äî embeddings –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ **–û–±–ª–∞—á–Ω—ã–µ fallback** (0 VRAM): - **Groq** (llama-3.1-70b, –±–µ—Å–ø–ª–∞—Ç–Ω–æ) ‚Äî –±—ã—Å—Ç—Ä—ã–π fallback - **OpenAI GPT-4** ‚Äî —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ - **Claude Sonnet** ‚Äî –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ ### LoRA-–∞–¥–∞–ø—Ç–µ—Ä—ã ModelManager —É–º–µ–µ—Ç –ø–æ–¥–≥—Ä—É–∂–∞—Ç—å –∞–¥–∞–ø—Ç–µ—Ä—ã –ø–æ–≤–µ—Ä—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ `options.adapter` (Ollama API): - **executor-dialogue-lora** (–±–∞–∑–∞: **mistral:7b-instruct**, +~0.8 –ì–ë VRAM) - –¥–∏–∞–ª–æ–≥–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è - **code-assistant-lora** (–±–∞–∑–∞: **qwen2.5-coder:7b**, +~0.6 –ì–ë VRAM) - —É—Å–∏–ª–µ–Ω–∏–µ –¥–ª—è –∑–∞–¥–∞—á —Å –∫–æ–¥–æ–º –ü—Ä–∏–º–µ—Ä –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ—Ä–µ–∑ Ollama (—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–¥ –≤—ã–∑—ã–≤–∞–µ—Ç `generate` —Å `options.adapter`): ```bash ollama pull executor-dialogue-lora ollama pull code-assistant-lora # —Ä—É—á–Ω–æ–π –ø—Ä–æ–≥—Ä–µ–≤ –∞–¥–∞–ø—Ç–µ—Ä–∞ ollama run mistral:7b-instruct --adapter executor-dialogue-lora --prompt \"init\" ollama run qwen2.5-coder:7b --adapter code-assistant-lora --prompt \"init\" ``` –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ llama.cpp –Ω–∞–ø—Ä—è–º—É—é, –ø–æ–¥–∫–ª—é—á–∞–π—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π GGUF-—Ñ–∞–π–ª —á–µ—Ä–µ–∑ `--lora <–ø—É—Ç—å_–∫_adapter.gguf>` –Ω–∞–¥ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é (VRAM = —Ä–∞–∑–º–µ—Ä –±–∞–∑—ã + —Ä–∞–∑–º–µ—Ä –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤—ã—à–µ). –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ Python ```bash # –ë—ã—Å—Ç—Ä–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π pip install -r requirements.txt # –†–µ–ø—Ä–æ–¥—É—Ü–∏—Ä—É–µ–º–∞—è —Å–±–æ—Ä–∫–∞ —Å –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—ã–º–∏ –ø–∞–∫–µ—Ç–∞–º–∏ pip install -r requirements.lock ``` –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞–∫—Ä–µ–ø–ª–µ–Ω—ã –≤ `requirements.txt`, –∞ –ø–æ–ª–Ω—ã–π —Å–Ω–∏–º–æ–∫ (—Å —É—á—ë—Ç–æ–º —Ç—Ä–∞–Ω–∑–∏—Ç–∏–≤–Ω—ã—Ö –ø–∞–∫–µ—Ç–æ–≤) ‚Äî –≤ `requirements.lock`, —á—Ç–æ–±—ã Renovate –º–æ–≥ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—å –æ–∫—Ä—É–∂–µ–Ω–∏–µ –±–µ–∑ —Å—é—Ä–ø—Ä–∏–∑–æ–≤."
  },
  {
    "source": "F:\\–ù–µ–π—Ä–æ–Ω–∫–∏\\prototype\\content_extractor.py",
    "source_type": "file",
    "title": "content_extractor.py",
    "word_count": 1632,
    "category": "code",
    "learned_at": "2025-12-28T15:26:31.274075",
    "summary": "\"\"\" Content Extractor v1.0 ‚Äî –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ù–µ–π—Ä—ã –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: - –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã (.txt, .md, .py, .js, etc.) - –í–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã (—Å—Ç–∞—Ç—å–∏, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è) - YouTube –≤–∏–¥–µ–æ (—Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã) - PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã –ó–∞—â–∏—Ç–∞ –æ—Ç —à—É–º–∞: - –£–¥–∞–ª–µ–Ω–∏–µ , –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, —Å–∞–π–¥–±–∞—Ä–æ–≤ - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –û—á–∏—Å—Ç–∫–∞ –æ—Ç HTML/JS –º—É—Å–æ—Ä–∞ \"\"\" import re import json import asyncio from pathlib import Path from typing import Optional, List, Dict, Any, Tuple from dataclasses import dataclass, field from datetime import datetime from urllib.parse import urlparse, parse_qs import logging logger = logging.getLogger(__name__) @dataclass class ExtractedContent: \"\"\"–ò–∑–≤–ª–µ—á—ë–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç\"\"\" source: str # URL –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É source_type: str # file, web, youtube, pdf title: str content: str summary: Optional = None word_count: int = 0 language: str = \"ru\" extracted_at: str = field(default_factory=lambda: datetime.now().isoformat()) metadata: Dict = field(default_factory=dict) def to_dict(self) -> Dict: return { \"source\": self.source, \"source_type\": self.source_type, \"title\": self.title, \"content\": self.content, \"summary\": self.summary, \"word_count\": self.word_count, \"language\": self.language, \"extracted_at\": self.extracted_at, \"metadata\": self.metadata } class NoiseFilter: \"\"\"–§–∏–ª—å—Ç—Ä —à—É–º–∞ ‚Äî —É–¥–∞–ª—è–µ—Ç —Ä–µ–∫–ª–∞–º—É, –Ω–∞–≤–∏–≥–∞—Ü–∏—é –∏ –º—É—Å–æ—Ä\"\"\" # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è NOISE_PATTERNS = ||||)', r'(?i)(|||)', # –ù–∞–≤–∏–≥–∞—Ü–∏—è r'(?i)(|||||)', r'(?i)(||||||)', # –∏ –ø–æ–ø–∞–ø—ã r'(?i)(|||)', r'(?i)(|||)', # –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ r'(?i)(|||vk\\.com|)', r'(?i)(|||)', # –ú—É—Å–æ—Ä r'(?i)(||)', r'\\', # –ö–≤–∞–¥—Ä–∞—Ç–Ω—ã–µ —Å–∫–æ–±–∫–∏ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º r'', r'', r'', ] # –¢–µ–≥–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è NOISE_TAGS = # –ö–ª–∞—Å—Å—ã/ID –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è NOISE_CLASSES = @classmethod def clean_text(cls, text: str) -> str: \"\"\"–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç —à—É–º–∞\"\"\" if not text: return \"\" # –£–¥–∞–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —à—É–º–∞ for pattern in cls.NOISE_PATTERNS: text = re.sub(pattern, '', text, flags=re.DOTALL) # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã text = re.sub(r'\\s+', ' ', text) text = re.sub(r'\\n{3,}', '\\n\\n', text) # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ä–æ—á–µ 10 —Å–∏–º–≤–æ–ª–æ–≤ (–æ–±—ã—á–Ω–æ –º—É—Å–æ—Ä) lines = text.split('\\n') lines = text = '\\n'.join(lines) return text.strip() @classmethod def clean_html(cls, soup) -> None: \"\"\"–£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∏–∑ BeautifulSoup\"\"\" # –£–¥–∞–ª—è–µ–º —à—É–º–Ω—ã–µ —Ç–µ–≥–∏ for tag in cls.NOISE_TAGS: for element in soup.find_all(tag): element.decompose() # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å —à—É–º–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏/ID for class_name in cls.NOISE_CLASSES: for element in soup.find_all(class_=re.compile(class_name, re.I)): element.decompose() for element in soup.find_all(id=re.compile(class_name, re.I)): element.decompose() class ContentExtractor: \"\"\"–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\"\"\" # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ TEXT_EXTENSIONS = { '.txt', '.md', '.markdown', '.rst', '.text', '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.html', '.htm', '.xml', '.json', '.yaml', '.yml', '.css', '.sql', '.sh', '.bat', '.ps1', '.ini', '.cfg', '.conf', '.env', '.log' } def __init__(self): self.extracted_count = 0 self.total_words = 0 async def extract(self, source: str) -> ExtractedContent: \"\"\" –ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ Args: source: URL –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É Returns: ExtractedContent —Å –æ—á–∏—â–µ–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º \"\"\" source = source.strip() # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏—Å—Ç–æ—á–Ω–∏–∫–∞ if self._is_youtube(source): return await self._extract_youtube(source) elif self._is_url(source): return await self._extract_web(source) elif Path(source).exists(): return await self._extract_file(source) else: raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {source}\") def _is_url(self, source: str) -> bool: \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ URL\"\"\" return source.startswith(('http://', 'https://', 'www.')) def _is_youtube(self, source: str) -> bool: \"\"\"–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ YouTube\"\"\" youtube_patterns = return any(re.search(p, source) for p in youtube_patterns) def _get_youtube_id(self, url: str) -> Optional: \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –≤–∏–¥–µ–æ YouTube\"\"\" patterns = +)', r'youtube\\.com/v/(+)' ] for pattern in patterns: match = re.search(pattern, url) if match: return match.group(1) return None async def _extract_file(self, path: str) -> ExtractedContent: \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞\"\"\" file_path = Path(path) if not file_path.exists(): raise FileNotFoundError(f\"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}\") ext = file_path.suffix.lower() # PDF —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ if ext == '.pdf': return await self._extract_pdf(path) # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã if ext in self.TEXT_EXTENSIONS or ext == '': try: content = file_path.read_text(encoding='utf-8') except UnicodeDecodeError: content = file_path.read_text(encoding='cp1251') # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç content = NoiseFilter.clean_text(content) word_count = len(content.split()) return ExtractedContent( source=str(file_path.absolute()), source_type=\"file\", title=file_path.name, content=content, word_count=word_count, metadata={ \"extension\": ext, \"size_bytes\": file_path.stat().st_size } ) raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {ext}\") async def _extract_pdf(self, path: str) -> ExtractedContent: \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF\"\"\" try: import pypdf reader = pypdf.PdfReader(path) text_parts = for page in reader.pages: text = page.extract_text() if text: text_parts.append(text) content = '\\n\\n'.join(text_parts) content = NoiseFilter.clean_text(content) return ExtractedContent( source=path, source_type=\"pdf\", title=Path(path).name, content=content, word_count=len(content.split()), metadata={ \"pages\": len(reader.pages) } ) except ImportError: raise ImportError(\"–î–ª—è PDF —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pypdf\") async def _extract_web(self, url: str) -> ExtractedContent: \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã\"\"\" try: import aiohttp from bs4 import BeautifulSoup except ImportError: raise ImportError(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install aiohttp beautifulsoup4\") s = { 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36' } timeout = aiohttp.ClientTimeout(total=30) async with aiohttp.ClientSession(timeout=timeout) as session: async with session.get(url, s=s) as response: if response.status != 200: raise Exception(f\"HTTP –æ—à–∏–±–∫–∞: {response.status}\") html = await response.text() soup = BeautifulSoup(html, 'html.parser') # –£–¥–∞–ª—è–µ–º —à—É–º NoiseFilter.clean_html(soup) # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ title = \"\" if soup.title: title = soup.title.string or \"\" if not title: h1 = soup.find('h1') title = h1.get_text() if h1 else urlparse(url).netloc # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç content = \"\" # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏ main_selectors = ', '.article', '.post', '.content', '.entry', '#article', '#content', '#main' ] for selector in main_selectors: main = soup.select_one(selector) if main: content = main.get_text(separator='\\n', strip=True) break # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä—ë–º body if not content: body = soup.find('body') if body: content = body.get_text(separator='\\n', strip=True) # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç content = NoiseFilter.clean_text(content) # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫ (—á–∞—Å—Ç–∞—è –ø—Ä–æ–±–ª–µ–º–∞) lines = content.split('\\n') seen = set() unique_lines = for line in lines: line_key = line.strip().lower() if line_key not in seen or line.strip() == '': seen.add(line_key) unique_lines.append(line) content = '\\n'.join(unique_lines) return ExtractedContent( source=url, source_type=\"web\", title=title.strip(), content=content, word_count=len(content.split()), metadata={ \"domain\": urlparse(url).netloc } ) async def _extract_youtube(self, url: str) -> ExtractedContent: \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞ YouTube –≤–∏–¥–µ–æ\"\"\" video_id = self._get_youtube_id(url) if not video_id: raise ValueError(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å ID –≤–∏–¥–µ–æ: {url}\") try: from youtube_transcript_api import YouTubeTranscriptApi except ImportError: raise ImportError(\"–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install youtube-transcript-api\") # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —è–∑—ã–∫–∏ languages = transcript = None for lang in languages: try: if lang == 'auto': transcript_list = YouTubeTranscriptApi.list_transcripts(video_id) transcript = transcript_list.find_generated_transcript().fetch() else: transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=) break except Exception: continue if not transcript: raise Exception(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç –¥–ª—è: {url}\") # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç text_parts = for entry in transcript] content = ' '.join(text_parts) # –û—á–∏—â–∞–µ–º content = NoiseFilter.clean_text(content) # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ –∞–±–∑–∞—Ü—ã (–ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) sentences = re.split(r'(?<=)\\s+', content) paragraphs = for i in range(0, len(sentences), 5): paragraph = ' '.join(sentences) if paragraph.strip(): paragraphs.append(paragraph) content = '\\n\\n'.join(paragraphs) # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ title = f\"YouTube: {video_id}\" try: import aiohttp async with aiohttp.ClientSession() as session: oembed_url = f\"https://www.youtube.com/oembed?url={url}&format=json\" async with session.get(oembed_url) as response: if response.status == 200: data = await response.json() title = data.get('title', title) except Exception: pass return ExtractedContent( source=url, source_type=\"youtube\", title=title, content=content, word_count=len(content.split()), metadata={ \"video_id\": video_id, \"duration_segments\": len(transcript) } ) async def extract_batch(self, sources: List) -> List: \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\"\"\" results = for source in sources: try: content = await self.extract(source) results.append(content) self.extracted_count += 1 self.total_words += content.word_count logger.info(f\"‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ: {content.title} ({content.word_count} —Å–ª–æ–≤)\") except Exception as e: logger.error(f\"‚úó –û—à–∏–±–∫–∞ –¥–ª—è {source}: {e}\") results.append(ExtractedContent( source=source, source_type=\"error\", title=\"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è\", content=str(e), metadata={\"error\": str(e)} )) return results class LearningManager: \"\"\"–ú–µ–Ω–µ–¥–∂–µ—Ä –æ–±—É—á–µ–Ω–∏—è ‚Äî —É–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ–º –æ–±—É—á–µ–Ω–∏—è –ù–µ–π—Ä—ã\"\"\" def __init__(self, memory_system=None): self.extractor = ContentExtractor() self.memory = memory_system self.learned_sources: List = self.learning_history_file = Path(\"data/learning_history.json\") self._load_history() def _load_history(self): \"\"\" –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è\"\"\" if self.learning_history_file.exists(): try: self.learned_sources = json.loads( self.learning_history_file.read_text(encoding='utf-8') ) except Exception: self.learned_sources = def _save_history(self): \"\"\"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è\"\"\" self.learning_history_file.parent.mkdir(parents=True, exist_ok=True) self.learning_history_file.write_text( json.dumps(self.learned_sources, ensure_ascii=False, indent=2), encoding='utf-8' ) async def learn_from_source(self, source: str, category: str = \"knowledge\", summarize: bool = True) -> Dict: \"\"\" –û–±—É—á–µ–Ω–∏–µ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ Args: source: URL –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–Ω–∞–Ω–∏–π summarize: –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ Returns: –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—É—á–µ–Ω–∏—è \"\"\" # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç content = await self.extractor.extract(source) # –°–æ–∑–¥–∞—ë–º summary –µ—Å–ª–∏ –Ω—É–∂–Ω–æ if summarize and len(content.content) > 500: content.summary = self._create_summary(content.content) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å if self.memory: # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ chunks = self._chunk_content(content.content) for i, chunk in enumerate(chunks): self.memory.remember( text=chunk, source=f\"{content.source}#chunk{i}\", context= ) # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é history_entry = { \"source\": content.source, \"source_type\": content.source_type, \"title\": content.title, \"word_count\": content.word_count, \"category\": category, \"learned_at\": datetime.now().isoformat(), \"summary\": content.summary } self.learned_sources.append(history_entry) self._save_history() return { \"success\": True, \"title\": content.title, \"source_type\": content.source_type, \"word_count\": content.word_count, \"chunks\": len(self._chunk_content(content.content)), \"summary\": content.summary, \"message\": f\"–ò–∑—É—á–∏–ª–∞: {content.title} ({content.word_count} —Å–ª–æ–≤)\" } async def learn_batch(self, sources: List, category: str = \"knowledge\") -> Dict: \"\"\"–û–±—É—á–µ–Ω–∏–µ –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\"\"\" results = success_count = 0 total_words = 0 for source in sources: try: result = await self.learn_from_source(source, category) results.append(result) if result.get(\"success\"): success_count += 1 total_words += result.get(\"word_count\", 0) except Exception as e: results.append({ \"success\": False, \"source\": source, \"error\": str(e) }) return { \"total\": len(sources), \"success\": success_count, \"failed\": len(sources) - success_count, \"total_words\": total_words, \"results\": results } def _create_summary(self, text: str, max_sentences: int = 5) -> str: \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (—ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥)\"\"\" # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è sentences = re.split(r'(?<=)\\s+', text) if len(sentences) <= max_sentences: return text # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–æ–±—ã—á–Ω–æ –≤–∞–∂–Ω–æ–µ) summary_sentences = ] # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ keywords = for sentence in sentences: if any(kw in sentence.lower() for kw in keywords): summary_sentences.append(sentence) if len(summary_sentences) >= max_sentences: break # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (—á–∞—Å—Ç–æ –≤—ã–≤–æ–¥) if len(summary_sentences) < max_sentences and sentences not in summary_sentences: summary_sentences.append(sentences) return ' '.join(summary_sentences) def _chunk_content(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List: \"\"\"–†–∞–∑–±–∏–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –ø–∞–º—è—Ç–∏\"\"\" if len(text) <= chunk_size: return chunks = start = 0 while start < len(text): end = start + chunk_size # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è if end < len(text): # –ò—â–µ–º —Ç–æ—á–∫—É, –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫ for punct in '.!?\\n': punct_pos = text.rfind(punct, start, end) if punct_pos > start: end = punct_pos + 1 break chunk = text.strip() if chunk: chunks.append(chunk) start = end - overlap return chunks def get_learning_stats(self) -> Dict: \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è\"\"\" total_words = sum(s.get('word_count', 0) for s in self.learned_sources) by_type = {} by_category = {} for source in self.learned_sources: source_type = source.get('source_type', 'unknown') category = source.get('category', 'unknown') by_type = by_type.get(source_type, 0) + 1 by_category = by_category.get(category, 0) + 1 return { \"total_sources\": len(self.learned_sources), \"total_words\": total_words, \"by_type\": by_type, \"by_category\": by_category, \"recent\": self.learned_sources if self.learned_sources else } def get_learned_topics(self) -> List: \"\"\"–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∏–∑—É—á–µ–Ω–Ω—ã—Ö —Ç–µ–º\"\"\" return # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ async def test_extractor(): \"\"\"–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞\"\"\" extractor = ContentExtractor() # –¢–µ—Å—Ç —Ñ–∞–π–ª–∞ print(\"=== –¢–µ—Å—Ç —Ñ–∞–π–ª–∞ ===\") try: content = await extractor.extract(\"README.md\") print(f\"‚úì {content.title}: {content.word_count} —Å–ª–æ–≤\") except Exception as e: print(f\"‚úó –§–∞–π–ª: {e}\") # –¢–µ—Å—Ç URL print(\"\\n=== –¢–µ—Å—Ç URL ===\") try: content = await extractor.extract(\"https://docs.python.org/3/library/asyncio.html\") print(f\"‚úì {content.title}: {content.word_count} —Å–ª–æ–≤\") print(f\" –ü—Ä–µ–≤—å—é: {content.content}...\") except Exception as e: print(f\"‚úó URL: {e}\") if __name__ == \"__main__\": asyncio.run(test_extractor())"
  },
  {
    "source": "F:\\–ù–µ–π—Ä–æ–Ω–∫–∏\\prototype\\artifacts\\learn_test.txt",
    "source_type": "file",
    "title": "learn_test.txt",
    "word_count": 4,
    "category": "knowledge",
    "learned_at": "2025-12-29T14:27:49.964959",
    "summary": null
  }
]