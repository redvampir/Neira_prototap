# ‚úÖ –ß–µ–∫–ª–∏—Å—Ç: –ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç Ollama

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ –∫–æ–º–º–∏—Ç–æ–º

### 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ llm_providers.py

- [ ] `python -c "from llm_providers import create_default_manager; m = create_default_manager(); print(m.get_stats())"`
  - –û–∂–∏–¥–∞–µ—Ç—Å—è: —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
  
- [ ] –¢–µ—Å—Ç embeddings (Ollama):
  ```python
  from llm_providers import OllamaProvider
  p = OllamaProvider()
  e = p.get_embedding("test")
  print(len(e) if e else "Failed")
  # –û–∂–∏–¥–∞–µ—Ç—Å—è: 768 (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å nomic-embed-text)
  ```

- [ ] –¢–µ—Å—Ç embeddings (OpenAI, –µ—Å–ª–∏ –∫–ª—é—á –µ—Å—Ç—å):
  ```python
  from llm_providers import OpenAIProvider
  p = OpenAIProvider()
  if p.available:
      e = p.get_embedding("test")
      print(len(e) if e else "Failed")
  # –û–∂–∏–¥–∞–µ—Ç—Å—è: 1536 (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å text-embedding-3-small)
  ```

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ memory_system.py

- [ ] Import –±–µ–∑ –æ—à–∏–±–æ–∫:
  ```python
  from memory_system import SemanticSearch
  print("OK")
  ```

- [ ] Embeddings —Å fallback:
  ```python
  from memory_system import SemanticSearch
  e = SemanticSearch.get_embedding("—Ç–µ—Å—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º")
  print("‚úì OK" if e else "‚úó Failed")
  ```

### 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ cells.py

- [ ] LLMManager –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è:
  ```bash
  grep -n "LLM_MANAGER_AVAILABLE" cells.py
  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å: —Å—Ç—Ä–æ–∫–∏ 72, 74, 359, 383
  ```

- [ ] Legacy —Ä–µ–∂–∏–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç:
  ```bash
  grep -n "_call_ollama_legacy" cells.py
  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å: –º–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª—ë–Ω
  ```

### 4. –°–∫—Ä–∏–ø—Ç—ã –∑–∞–ø—É—Å–∫–∞

- [ ] `start_cloud_only.bat` —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
- [ ] `start_hybrid.bat` —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
- [ ] –û–±–∞ —Å–∫—Ä–∏–ø—Ç–∞ –∏–º–µ—é—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8 BOM (–¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤)

### 5. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- [ ] `OLLAMA_INDEPENDENCE.md` —Å–æ–∑–¥–∞–Ω (300+ —Å—Ç—Ä–æ–∫)
- [ ] `OLLAMA_INDEPENDENCE_REPORT.md` —Å–æ–∑–¥–∞–Ω
- [ ] `QUICKSTART.md` –æ–±–Ω–æ–≤–ª—ë–Ω (—Ä–∞–∑–¥–µ–ª "–ü—Ä–æ–≤–∞–π–¥–µ—Ä—ã")
- [ ] `README.md` –æ–±–Ω–æ–≤–ª—ë–Ω (v0.8.1 changelog)

---

## –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –°—Ü–µ–Ω–∞—Ä–∏–π A: Cloud-only (–±–µ–∑ Ollama)

**–®–∞–≥–∏:**
1. –£–±–µ–¥–∏—Å—å —á—Ç–æ Ollama –≤—ã–∫–ª—é—á–µ–Ω: `tasklist | find "ollama"`
2. –°–æ–∑–¥–∞–π .env —Å `GROQ_API_KEY=gsk_...`
3. –ó–∞–ø—É—Å—Ç–∏ `start_cloud_only.bat`
4. –û—Ç–ø—Ä–∞–≤—å —Å–æ–æ–±—â–µ–Ω–∏–µ –±–æ—Ç—É

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ë–æ—Ç –æ—Ç–≤–µ—á–∞–µ—Ç —á–µ—Ä–µ–∑ Groq
- ‚úÖ –í –ª–æ–≥–∞—Ö: "Trying groq"
- ‚úÖ –ù–µ—Ç –æ—à–∏–±–æ–∫ "Ollama offline"

### –°—Ü–µ–Ω–∞—Ä–∏–π B: Hybrid (Ollama + Cloud)

**–®–∞–≥–∏:**
1. –ó–∞–ø—É—Å—Ç–∏ Ollama: `ollama serve`
2. –ó–∞–ø—É—Å—Ç–∏ `start_hybrid.bat`
3. –û—Ç–ø—Ä–∞–≤—å —Å–æ–æ–±—â–µ–Ω–∏–µ
4. –û—Å—Ç–∞–Ω–æ–≤–∏ Ollama: `taskkill /f /im ollama.exe`
5. –û—Ç–ø—Ä–∞–≤—å –µ—â—ë —Å–æ–æ–±—â–µ–Ω–∏–µ

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ –ü–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Ollama
- ‚úÖ –í—Ç–æ—Ä–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Groq (fallback)
- ‚úÖ –ù–µ—Ç –∫—Ä–∞—à–∞, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ

### –°—Ü–µ–Ω–∞—Ä–∏–π C: Embeddings fallback

**–®–∞–≥–∏:**
1. –í—ã–∫–ª—é—á–∏ Ollama
2. –ù–∞—Å—Ç—Ä–æ–π OPENAI_API_KEY –≤ .env
3. –ó–∞–ø—É—Å—Ç–∏ Python:
   ```python
   from memory_system import SemanticSearch
   e = SemanticSearch.get_embedding("test")
   print("OpenAI embeddings —Ä–∞–±–æ—Ç–∞—é—Ç!" if e else "Fail")
   ```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- ‚úÖ Embeddings —á–µ—Ä–µ–∑ OpenAI
- ‚úÖ –í –ª–æ–≥–∞—Ö: "‚úì Embedding from openai"

---

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã

### Telegram Bot

- [ ] `/providers` ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
- [ ] –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ Ollama
- [ ] –ë–æ—Ç –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ –ø—Ä–∏ —Å–±–æ–µ
- [ ] Vision —Ñ—É–Ω–∫—Ü–∏–∏ (llava) –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –µ—Å–ª–∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

### Web UI

- [ ] –†–∞–±–æ—Ç–∞–µ—Ç —Å –ª—é–±—ã–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º
- [ ] –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
- [ ] –ù–µ—Ç hardcoded —Å—Å—ã–ª–æ–∫ –Ω–∞ localhost:11434

---

## –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

### Latency —Ç–µ—Å—Ç

```python
import time
from llm_providers import create_default_manager

manager = create_default_manager()

start = time.time()
response = manager.generate("–ü—Ä–∏–≤–µ—Ç!")
end = time.time()

print(f"Provider: {response.provider.value}")
print(f"Latency: {end - start:.2f}s")
print(f"Success: {response.success}")
```

**–û–∂–∏–¥–∞–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:**
- Ollama: 0.5-2s
- Groq: 1-3s
- OpenAI: 2-5s
- Claude: 3-6s

### Memory —Ç–µ—Å—Ç

```python
from memory_system import MemorySystem
import os

ms = MemorySystem(".")
ms.add_memory("–¢–µ—Å—Ç –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç Ollama", category="fact")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ embeddings
entries = ms.search_memories("Ollama", top_k=1)
print(f"‚úì Search —Ä–∞–±–æ—Ç–∞–µ—Ç: {len(entries)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
```

---

## –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã

### Legacy —Ñ—É–Ω–∫—Ü–∏–∏ (–¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å)

- [ ] `cells.py` ‚Äî Cell.call_llm() —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] `memory_system.py` ‚Äî MemorySystem.add_memory() —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] `telegram_bot.py` ‚Äî –±–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è
- [ ] `main.py` ‚Äî –∫–æ–Ω—Å–æ–ª—å–Ω—ã–π —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç–∞–µ—Ç

### –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å)

- [ ] LLMManager.generate() —Å preferred_provider
- [ ] LLMManager.get_embedding() —Å fallback
- [ ] SemanticSearch.get_embedding() —á–µ—Ä–µ–∑ LLMManager
- [ ] start_cloud_only.bat
- [ ] start_hybrid.bat

---

## –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å

### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç)

–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ù–ï –æ–±–Ω–æ–≤–∏–ª –∫–æ–Ω—Ñ–∏–≥:
- [ ] –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ legacy —Ä–µ–∂–∏–º–µ (—Ç–æ–ª—å–∫–æ Ollama)
- [ ] –ù–µ—Ç –æ—à–∏–±–æ–∫ –∏–º–ø–æ—Ä—Ç–∞
- [ ] –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ LLMManager

### –ú–∏–≥—Ä–∞—Ü–∏—è

–°—Ç–∞—Ä—ã–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–æ–ª–∂–Ω—ã:
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∏—Ç—å LLMManager –ø—Ä–∏ `pip install -r requirements.txt`
- [ ] –†–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è .env (Ollama –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
- [ ] –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –Ω–æ–≤—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ

---

## Checklist –¥–ª—è –∫–æ–º–º–∏—Ç–∞

- [ ] –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Å UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
- [ ] –ù–µ—Ç debug print() –≤ production –∫–æ–¥–µ
- [ ] –õ–æ–≥–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç logger.info/warning/error
- [ ] –í—Å–µ TODO –≤ –∫–æ–¥–µ –∑–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ OLLAMA_INDEPENDENCE.md
- [ ] –í–µ—Ä—Å–∏—è –≤ README.md –æ–±–Ω–æ–≤–ª–µ–Ω–∞ (v0.8.1)
- [ ] Changelog –æ–ø–∏—Å—ã–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è

---

## Git commit message

```
üåê Add multi-provider LLM support (v0.8.1)

- Add embeddings abstraction to LLMProvider
- Implement Ollama & OpenAI embeddings
- Update MemorySystem to use LLMManager
- Add start_cloud_only.bat & start_hybrid.bat
- Create OLLAMA_INDEPENDENCE.md documentation
- Update QUICKSTART.md with provider guide

Breaking changes: None (backward compatible)
New dependencies: None (optional: openai, anthropic, groq)

Closes #OLLAMA-INDEPENDENCE
```

---

## –ü–æ—Å–ª–µ –∫–æ–º–º–∏—Ç–∞

- [ ] –°–æ–∑–¥–∞—Ç—å GitHub Release v0.8.1
- [ ] –û–±–Ω–æ–≤–∏—Ç—å Wiki —Å –Ω–æ–≤—ã–º —Ä–∞–∑–¥–µ–ª–æ–º "Multi-Provider Setup"
- [ ] –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –≤ Issues/Discussions
- [ ] –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ —á–∏—Å—Ç–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–µ (–±–µ–∑ Ollama)
