# NEIRA v0.4 — Живая программа

## Что нового

### Две модели
- **qwen2.5:3b** (~2 ГБ VRAM) — для диалогов, быстрая
- **qwen2.5-coder:7b** (~5 ГБ VRAM) — для кода, качественная

### Retry-логика
Если верификатор даёт оценку < 7/10, система автоматически пробует исправить ответ (до 2 раз).

### Анализ субъекта действия
Теперь анализатор определяет КТО должен действовать:
- СУБЪЕКТ: Нейра → она выполняет
- СУБЪЕКТ: пользователь → он выполняет

### Принудительные инструменты
Если нужен код — он автоматически читается, а не просто планируется.

## Установка

```bash
# 1. Запусти Ollama
ollama serve

# 2. Установи модели
ollama pull qwen2.5:3b
ollama pull qwen2.5-coder:7b
ollama pull nomic-embed-text

# 3. Установи зависимости Python
pip install requests numpy duckduckgo-search

# 4. Запусти
python main.py
```

## Команды

```
/help        — справка
/memory      — показать память
/experience  — показать опыт
/personality — показать личность
/stats       — статистика и статус моделей
/code list   — список файлов
/code read   — прочитать файл
/exit        — выход

 КОМАНДЫ НЕЙРЫ v0.4

Память и опыт:
  /memory              — показать память
  /experience          — показать накопленный опыт
  /personality         — показать личность
  /clear               — очистить память

Обучение:
  /learn <тема>        — изучить тему из интернета

Работа с кодом:
  /code list           — список файлов
  /code read <файл>    — прочитать файл
  /code analyze <файл> — анализ кода
  /code self           — самоанализ

Прочее:
  /stats               — статистика
  /models              — проверить модели
  /help                — эта справка
  /exit                — выход

Или просто пиши — Нейра ответит!

```

## Структура файлов

```
neira_v04/
├── main.py         — Оркестратор с retry-логикой
├── cells.py        — Базовые клетки (2 модели)
├── code_cell.py    — Работа с кодом
├── web_cell.py     — Поиск в интернете
├── experience.py   — Система опыта и личности
└── README.md       — Этот файл
```

## Требования к железу

- **Минимум**: RTX 3060 Ti (8 ГБ VRAM) или аналог
- **RAM**: 16+ ГБ
- Две модели НЕ загружаются одновременно — Ollama переключает автоматически

## Изменения из v0.3

| Было | Стало |
|------|-------|
| Gemma 12B (медленная) | Qwen2.5:3b + Coder:7b |
| Timeout 600 сек | Timeout 180 сек |
| Путаница ролей | Анализ субъекта |
| Нет retry | До 2 попыток исправить |
| Инструменты игнорируются | Принудительное использование |

## Решение проблем

### "Ollama не запущена"
```bash
ollama serve
```

### "Модель не найдена"
```bash
ollama pull qwen2.5:3b
ollama pull qwen2.5-coder:7b
```

### Timeout
Qwen2.5:3b должна отвечать за 10-30 секунд. Если дольше — проверь что Ollama использует GPU:
```bash
nvidia-smi
```

## Roadmap

- [ ] Иерархическая память (важность, забывание)
- [ ] Интеграция генерации изображений
- [ ] Голосовой ввод/вывод
- [ ] WebUI вместо консоли
