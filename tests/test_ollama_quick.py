"""
–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å LM Studio / Ollama
"""
import os
import requests
import json

# === LM Studio (–æ—Å–Ω–æ–≤–Ω–æ–π) ===
LMSTUDIO_URL = os.getenv("NEIRA_LMSTUDIO_URL", "http://localhost:1234/v1/chat/completions")
LMSTUDIO_MODEL = os.getenv("NEIRA_LMSTUDIO_MODEL", "qwen/qwen2.5-coder-14b")

# === Ollama (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π) ===
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = os.getenv("NEIRA_OLLAMA_MODEL", "nemotron-mini")


def test_lmstudio():
    """–¢–µ—Å—Ç LM Studio"""
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä—è—é LM Studio ({LMSTUDIO_MODEL})...")
    try:
        response = requests.post(
            LMSTUDIO_URL,
            json={
                "model": LMSTUDIO_MODEL,
                "messages": [
                    {"role": "system", "content": "–¢—ã - –ù–µ–π—Ä–∞, –∂–∏–≤–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞."},
                    {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Å—è –∫—Ä–∞—Ç–∫–æ."}
                ],
                "max_tokens": 100
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            answer = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            print(f"‚úÖ LM Studio —Ä–∞–±–æ—Ç–∞–µ—Ç!")
            print(f"üìù –û—Ç–≤–µ—Ç: {answer[:200]}")
            return True
        else:
            print(f"‚ùå HTTP {response.status_code}: {response.text}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("‚ùå LM Studio –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞! –ó–∞–ø—É—Å—Ç–∏ Local Server –≤ LM Studio")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


def test_ollama():
    """–¢–µ—Å—Ç Ollama (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π)"""
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä—è—é Ollama ({OLLAMA_MODEL})...")
    try:
        response = requests.post(
            OLLAMA_URL,
            json={
                "model": OLLAMA_MODEL,
                "prompt": "–ü—Ä–∏–≤–µ—Ç! –ü—Ä–µ–¥—Å—Ç–∞–≤—å—Å—è –∫—Ä–∞—Ç–∫–æ.",
                "system": "–¢—ã - –ù–µ–π—Ä–∞, –∂–∏–≤–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞.",
                "stream": False,
                "options": {"temperature": 0.7, "num_predict": 100}
            },
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            answer = result.get("response", "")
            if answer.strip():
                print(f"‚úÖ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç!")
                print(f"üìù –û—Ç–≤–µ—Ç: {answer[:200]}")
                return True
            else:
                print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama")
                return False
        else:
            print(f"‚ùå HTTP {response.status_code}: {response.text}")
            return False
            
    except requests.exceptions.ConnectionError:
        print("‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞")
        return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False


if __name__ == "__main__":
    print("=== –¢–µ—Å—Ç LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ù–µ–π—Ä—ã ===")
    
    lm_ok = test_lmstudio()
    ollama_ok = test_ollama()
    
    print("\n=== –ò—Ç–æ–≥ ===")
    print(f"LM Studio: {'‚úÖ OK' if lm_ok else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}")
    print(f"Ollama:    {'‚úÖ OK' if ollama_ok else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}")
    
    if lm_ok:
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: LM Studio (–æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä)")
    elif ollama_ok:
        print("\nüí° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: Ollama (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π)")
    else:
        print("\n‚ö†Ô∏è –ù–∏ –æ–¥–∏–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π LLM –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω!")
