"""
Neira A/B Testing Framework v0.6
–§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–ª–µ—Ç–æ–∫, –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π.

–í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
1. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–≤—É—Ö+ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
2. –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ (score, confidence, latency, success rate)
3. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π
5. Multi-armed bandit –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
"""

import os
import json
import time
import random
from typing import List, Dict, Optional, Tuple, Callable, Any
from dataclasses import dataclass, asdict, field
from datetime import datetime
import statistics


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
AB_TESTS_FILE = "neira_ab_tests.json"
MIN_SAMPLES_PER_VARIANT = 10  # –ú–∏–Ω–∏–º—É–º —Ç–µ—Å—Ç–æ–≤ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
CONFIDENCE_THRESHOLD = 0.95   # –£—Ä–æ–≤–µ–Ω—å –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
EPSILON_GREEDY = 0.2          # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤ epsilon-greedy


@dataclass
class TestSample:
    """–û–¥–∏–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫"""
    variant_id: str
    score: float
    confidence: float
    latency_ms: float
    success: bool
    timestamp: str
    metadata: Dict = field(default_factory=dict)


@dataclass
class Variant:
    """–í–∞—Ä–∏–∞–Ω—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    variant_id: str
    name: str
    description: str
    samples: List[TestSample] = field(default_factory=list)

    # –ú–µ—Ç—Ä–∏–∫–∏
    avg_score: float = 0.0
    avg_confidence: float = 0.0
    avg_latency_ms: float = 0.0
    success_rate: float = 0.0
    total_samples: int = 0

    def update_metrics(self):
        """–ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ samples"""
        if not self.samples:
            return

        self.total_samples = len(self.samples)
        self.avg_score = statistics.mean(s.score for s in self.samples)
        self.avg_confidence = statistics.mean(s.confidence for s in self.samples)
        self.avg_latency_ms = statistics.mean(s.latency_ms for s in self.samples)
        self.success_rate = sum(1 for s in self.samples if s.success) / self.total_samples

    def to_dict(self) -> dict:
        return {
            **asdict(self),
            "samples": [asdict(s) for s in self.samples]
        }

    @staticmethod
    def from_dict(d: dict) -> "Variant":
        samples = [TestSample(**s) for s in d.pop("samples", [])]
        variant = Variant(**d)
        variant.samples = samples
        return variant


@dataclass
class ABTest:
    """A/B —Ç–µ—Å—Ç"""
    test_id: str
    test_name: str
    created_at: str
    status: str  # running, completed, cancelled
    variants: List[Variant] = field(default_factory=list)
    winner_variant_id: Optional[str] = None
    decision_reason: Optional[str] = None
    algorithm: str = "epsilon_greedy"  # epsilon_greedy, round_robin, ucb

    def to_dict(self) -> dict:
        data = asdict(self)
        data["variants"] = [v.to_dict() for v in self.variants]
        return data

    @staticmethod
    def from_dict(d: dict) -> "ABTest":
        variants = [Variant.from_dict(v) for v in d.pop("variants", [])]
        test = ABTest(**d)
        test.variants = variants
        return test


class ABTestingFramework:
    """–§—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""

    def __init__(self):
        self.tests: Dict[str, ABTest] = {}
        self.load_tests()

    def load_tests(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ—Å—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞"""
        if os.path.exists(AB_TESTS_FILE):
            try:
                with open(AB_TESTS_FILE, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    self.tests = {
                        test_id: ABTest.from_dict(test_data)
                        for test_id, test_data in data.items()
                    }
                print(f"üß™ –ó–∞–≥—Ä—É–∂–µ–Ω–æ A/B —Ç–µ—Å—Ç–æ–≤: {len(self.tests)}")
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ—Å—Ç–æ–≤: {e}")

    def save_tests(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ç–µ—Å—Ç—ã"""
        try:
            data = {
                test_id: test.to_dict()
                for test_id, test in self.tests.items()
            }

            with open(AB_TESTS_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤: {e}")

    def create_test(self, test_name: str, variants: List[Tuple[str, str]],
                   algorithm: str = "epsilon_greedy") -> ABTest:
        """
        –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π A/B —Ç–µ—Å—Ç

        Args:
            test_name: –ò–º—è —Ç–µ—Å—Ç–∞
            variants: [(variant_id, description), ...]
            algorithm: –ê–ª–≥–æ—Ä–∏—Ç–º –≤—ã–±–æ—Ä–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        """
        test_id = f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        test = ABTest(
            test_id=test_id,
            test_name=test_name,
            created_at=datetime.now().isoformat(),
            status="running",
            algorithm=algorithm
        )

        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        for variant_id, description in variants:
            variant = Variant(
                variant_id=variant_id,
                name=variant_id,
                description=description
            )
            test.variants.append(variant)

        self.tests[test_id] = test
        self.save_tests()

        print(f"üß™ –°–æ–∑–¥–∞–Ω A/B —Ç–µ—Å—Ç: {test_id}")
        print(f"   –ò–º—è: {test_name}")
        print(f"   –í–∞—Ä–∏–∞–Ω—Ç—ã: {', '.join(v.variant_id for v in test.variants)}")
        print(f"   –ê–ª–≥–æ—Ä–∏—Ç–º: {algorithm}")

        return test

    def select_variant(self, test_id: str) -> Optional[Variant]:
        """–í—ã–±—Ä–∞—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–µ—Å—Ç–∞"""
        if test_id not in self.tests:
            return None

        test = self.tests[test_id]

        if test.status != "running":
            return None

        algorithm = test.algorithm

        if algorithm == "round_robin":
            return self._round_robin_select(test)
        elif algorithm == "epsilon_greedy":
            return self._epsilon_greedy_select(test)
        elif algorithm == "ucb":
            return self._ucb_select(test)
        else:
            return random.choice(test.variants)

    def _round_robin_select(self, test: ABTest) -> Variant:
        """Round-robin –≤—ã–±–æ—Ä"""
        # –í—ã–±–∏—Ä–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç —Å –º–∏–Ω–∏–º—É–º–æ–º —Ç–µ—Å—Ç–æ–≤
        return min(test.variants, key=lambda v: v.total_samples)

    def _epsilon_greedy_select(self, test: ABTest) -> Variant:
        """Epsilon-greedy: –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ vs —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è"""
        if random.random() < EPSILON_GREEDY:
            # –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: —Å–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä
            return random.choice(test.variants)
        else:
            # –≠–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è: –ª—É—á—à–∏–π –ø–æ –º–µ—Ç—Ä–∏–∫–µ
            return max(test.variants, key=lambda v: v.avg_score if v.total_samples > 0 else 0)

    def _ucb_select(self, test: ABTest) -> Variant:
        """Upper Confidence Bound"""
        import math

        total_trials = sum(v.total_samples for v in test.variants)

        if total_trials == 0:
            return random.choice(test.variants)

        def ucb_score(variant: Variant) -> float:
            if variant.total_samples == 0:
                return float('inf')

            mean_score = variant.avg_score / 10  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
            exploration = math.sqrt(2 * math.log(total_trials) / variant.total_samples)

            return mean_score + exploration

        return max(test.variants, key=ucb_score)

    def record_result(self, test_id: str, variant_id: str,
                     score: float, confidence: float,
                     latency_ms: float, success: bool,
                     metadata: Optional[Dict] = None):
        """–ó–∞–ø–∏—Å–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–∞"""
        if test_id not in self.tests:
            return

        test = self.tests[test_id]
        variant = next((v for v in test.variants if v.variant_id == variant_id), None)

        if not variant:
            return

        sample = TestSample(
            variant_id=variant_id,
            score=score,
            confidence=confidence,
            latency_ms=latency_ms,
            success=success,
            timestamp=datetime.now().isoformat(),
            metadata=metadata or {}
        )

        variant.samples.append(sample)
        variant.update_metrics()

        self.save_tests()

    def analyze_test(self, test_id: str) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ—Å—Ç–∞"""
        if test_id not in self.tests:
            return {}

        test = self.tests[test_id]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –¥–∞–Ω–Ω—ã—Ö
        min_samples_ok = all(v.total_samples >= MIN_SAMPLES_PER_VARIANT for v in test.variants)

        if not min_samples_ok:
            return {
                "ready": False,
                "reason": f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ—Å—Ç–æ–≤ (–º–∏–Ω–∏–º—É–º {MIN_SAMPLES_PER_VARIANT} –Ω–∞ –≤–∞—Ä–∏–∞–Ω—Ç)"
            }

        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ score
        best_variant = max(test.variants, key=lambda v: v.avg_score)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É —Å –¥—Ä—É–≥–∏–º–∏
        comparisons = []

        for variant in test.variants:
            if variant.variant_id == best_variant.variant_id:
                continue

            score_diff = best_variant.avg_score - variant.avg_score
            latency_diff = variant.avg_latency_ms - best_variant.avg_latency_ms
            success_rate_diff = best_variant.success_rate - variant.success_rate

            # –ü—Ä–æ—Å—Ç–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π —Ç–µ—Å—Ç (t-test —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π)
            significance = self._simple_significance_test(
                [s.score for s in best_variant.samples],
                [s.score for s in variant.samples]
            )

            comparisons.append({
                "variant": variant.variant_id,
                "score_diff": score_diff,
                "latency_diff_ms": latency_diff,
                "success_rate_diff": success_rate_diff,
                "significant": significance
            })

        return {
            "ready": True,
            "best_variant": best_variant.variant_id,
            "best_metrics": {
                "score": best_variant.avg_score,
                "confidence": best_variant.avg_confidence,
                "latency_ms": best_variant.avg_latency_ms,
                "success_rate": best_variant.success_rate
            },
            "comparisons": comparisons
        }

    def _simple_significance_test(self, samples_a: List[float],
                                  samples_b: List[float]) -> bool:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∏–π"""
        if len(samples_a) < 3 or len(samples_b) < 3:
            return False

        mean_a = statistics.mean(samples_a)
        mean_b = statistics.mean(samples_b)
        stdev_a = statistics.stdev(samples_a)
        stdev_b = statistics.stdev(samples_b)

        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Ä–∞–∑–Ω–∏—Ü–∞ > 0.5 * —Å—Ä–µ–¥–Ω–µ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ
        pooled_stdev = (stdev_a + stdev_b) / 2
        diff = abs(mean_a - mean_b)

        return diff > 0.5 * pooled_stdev

    def make_decision(self, test_id: str, auto_activate: bool = False) -> Optional[str]:
        """–ü—Ä–∏–Ω—è—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ"""
        analysis = self.analyze_test(test_id)

        if not analysis.get("ready"):
            print(f"‚è∏Ô∏è –¢–µ—Å—Ç –Ω–µ –≥–æ—Ç–æ–≤: {analysis.get('reason')}")
            return None

        test = self.tests[test_id]
        winner_id = analysis["best_variant"]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏–π
        significant_improvements = sum(
            1 for c in analysis["comparisons"] if c["significant"] and c["score_diff"] > 0
        )

        if significant_improvements < len(test.variants) - 1:
            print(f"‚ö†Ô∏è –£–ª—É—á—à–µ–Ω–∏—è –Ω–µ –∑–Ω–∞—á–∏–º—ã —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏")
            return None

        # –§–∏–∫—Å–∏—Ä—É–µ–º —Ä–µ—à–µ–Ω–∏–µ
        test.winner_variant_id = winner_id
        test.status = "completed"
        test.decision_reason = f"–õ—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ score: {analysis['best_metrics']['score']:.2f}"

        self.save_tests()

        print(f"\nüèÜ –ü–û–ë–ï–î–ò–¢–ï–õ–¨ –¢–ï–°–¢–ê {test_id}:")
        print(f"   –í–∞—Ä–∏–∞–Ω—Ç: {winner_id}")
        print(f"   Score: {analysis['best_metrics']['score']:.2f}/10")
        print(f"   Success rate: {analysis['best_metrics']['success_rate']*100:.0f}%")
        print(f"   Latency: {analysis['best_metrics']['latency_ms']:.0f}ms")

        if auto_activate:
            print(f"\n   ‚úÖ –ê–≤—Ç–æ–∞–∫—Ç–∏–≤–∞—Ü–∏—è –ø–æ–±–µ–¥–∏—Ç–µ–ª—è")

        return winner_id

    def show_test_results(self, test_id: str) -> str:
        """–ü–æ–∫–∞–∑–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞"""
        if test_id not in self.tests:
            return f"‚ö†Ô∏è –¢–µ—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_id}"

        test = self.tests[test_id]

        output = f"üß™ A/B –¢–ï–°–¢: {test.test_name}\n\n"
        output += f"ID: {test_id}\n"
        output += f"–°—Ç–∞—Ç—É—Å: {test.status}\n"
        output += f"–°–æ–∑–¥–∞–Ω: {test.created_at[:19]}\n"
        output += f"–ê–ª–≥–æ—Ä–∏—Ç–º: {test.algorithm}\n\n"

        output += f"–í–ê–†–ò–ê–ù–¢–´:\n\n"

        for i, variant in enumerate(test.variants, 1):
            winner_mark = " üèÜ –ü–û–ë–ï–î–ò–¢–ï–õ–¨" if variant.variant_id == test.winner_variant_id else ""

            output += f"{i}. {variant.variant_id}{winner_mark}\n"
            output += f"   –û–ø–∏—Å–∞–Ω–∏–µ: {variant.description}\n"
            output += f"   –¢–µ—Å—Ç–æ–≤: {variant.total_samples}\n"

            if variant.total_samples > 0:
                output += f"   Score: {variant.avg_score:.2f}/10\n"
                output += f"   Confidence: {variant.avg_confidence:.2f}\n"
                output += f"   Latency: {variant.avg_latency_ms:.0f}ms\n"
                output += f"   Success rate: {variant.success_rate*100:.0f}%\n"

            output += "\n"

        if test.status == "completed" and test.decision_reason:
            output += f"–†–ï–®–ï–ù–ò–ï: {test.decision_reason}\n"

        return output

    def get_active_tests(self) -> List[ABTest]:
        """–ü–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç–µ—Å—Ç—ã"""
        return [t for t in self.tests.values() if t.status == "running"]

    def cancel_test(self, test_id: str):
        """–û—Ç–º–µ–Ω–∏—Ç—å —Ç–µ—Å—Ç"""
        if test_id in self.tests:
            self.tests[test_id].status = "cancelled"
            self.save_tests()
            print(f"‚ùå –¢–µ—Å—Ç –æ—Ç–º–µ–Ω–µ–Ω: {test_id}")

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        return {
            "total_tests": len(self.tests),
            "running": len([t for t in self.tests.values() if t.status == "running"]),
            "completed": len([t for t in self.tests.values() if t.status == "completed"]),
            "cancelled": len([t for t in self.tests.values() if t.status == "cancelled"])
        }


# === –¢–ï–°–¢ ===
if __name__ == "__main__":
    print("=" * 60)
    print("–¢–µ—Å—Ç ABTestingFramework")
    print("=" * 60)

    framework = ABTestingFramework()

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç
    test = framework.create_test(
        "Prompts comparison",
        variants=[
            ("variant_a", "–ü—Ä–æ–º–ø—Ç –≤–µ—Ä—Å–∏—è A"),
            ("variant_b", "–ü—Ä–æ–º–ø—Ç –≤–µ—Ä—Å–∏—è B")
        ]
    )

    # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
    for i in range(15):
        variant = framework.select_variant(test.test_id)

        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (B –ª—É—á—à–µ A)
        if variant.variant_id == "variant_b":
            score = random.uniform(7, 9)
            success = random.random() > 0.1
        else:
            score = random.uniform(5, 7)
            success = random.random() > 0.3

        framework.record_result(
            test.test_id,
            variant.variant_id,
            score=score,
            confidence=random.uniform(0.6, 0.9),
            latency_ms=random.uniform(100, 500),
            success=success
        )

    print(f"\n{framework.show_test_results(test.test_id)}")

    # –ê–Ω–∞–ª–∏–∑
    winner = framework.make_decision(test.test_id)

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(json.dumps(framework.get_stats(), indent=2))
