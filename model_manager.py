"""
Model Manager v0.5 â€” VRAM management and model switching
Manages 3 local models + 1 Ollama cloud model for complex tasks
"""

import requests
import time
import os
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field

OLLAMA_API = "http://localhost:11434/api"


@dataclass
class ModelInfo:
    """Model metadata"""
    name: str
    size_gb: float
    type: str  # local / cloud
    use_case: str


@dataclass
class LoraInfo:
    """Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ LoRA-Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ðµ"""

    key: str
    adapter_name: str
    size_gb: float
    base_model_key: str
    description: str = ""


@dataclass
class LoadedLoraState:
    """Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ð¾Ð³Ð¾ LoRA Ð² VRAM"""

    info: LoraInfo
    last_used: float = field(default_factory=time.time)


# Model registry
MODELS = {
    # Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸
    "code": ModelInfo("qwen2.5-coder:7b", 5.0, "local", "Code generation and analysis"),
    "reason": ModelInfo("mistral:7b-instruct", 4.5, "local", "Planning, reasoning, verification"),
    "personality": ModelInfo("neira-personality", 1.5, "local", "Dialogue with personality"),

    # ÐžÐ±Ð»Ð°Ñ‡Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸ (0 VRAM, ÑƒÐ´Ð°Ð»Ñ‘Ð½Ð½Ñ‹Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ)
    "cloud_code": ModelInfo("qwen3-coder:480b-cloud", 0, "cloud", "Complex code tasks (480B params)"),
    "cloud_universal": ModelInfo("deepseek-v3.1:671b-cloud", 0, "cloud", "Complex universal tasks (671B params)"),
    "cloud_vision": ModelInfo("qwen3-vl:235b-cloud", 0, "cloud", "Multimodal tasks (235B params)")
}

# Ð ÐµÐµÑÑ‚Ñ€ LoRA-Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€Ð¾Ð² (ÐºÐ»ÑŽÑ‡ â†’ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹)
LORA_ADAPTERS: Dict[str, LoraInfo] = {
    "executor_dialog": LoraInfo(
        key="executor_dialog",
        adapter_name="executor-dialogue-lora",
        size_gb=0.8,
        base_model_key="reason",
        description="Ð”Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²Ð°Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð¸ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»Ñ"
    ),
    "code_assistant": LoraInfo(
        key="code_assistant",
        adapter_name="code-assistant-lora",
        size_gb=0.6,
        base_model_key="code",
        description="Ð£ÑÐ¸Ð»ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡ Ñ ÐºÐ¾Ð´Ð¾Ð¼"
    )
}


class ModelManager:
    """Manages model loading/unloading to stay within VRAM limits"""

    def __init__(self, max_vram_gb: float = 8.0, verbose: bool = True):
        self.max_vram = max_vram_gb
        self.current_model: Optional[str] = None
        self.switch_count = 0
        self.verbose = verbose
        self.cloud_models_available = self._check_cloud_models()
        self.loaded_loras: Dict[str, LoadedLoraState] = {}
        self.current_vram: float = 0.0

    def log(self, message: str):
        if self.verbose:
            print(message)

    def _check_cloud_models(self) -> Dict[str, bool]:
        """Check which cloud models are available in Ollama"""
        available = {
            "cloud_code": False,
            "cloud_universal": False,
            "cloud_vision": False
        }
        try:
            resp = requests.get(f"{OLLAMA_API}/tags", timeout=5)
            models = resp.json().get("models", [])
            model_names = [m.get("name", "") for m in models]

            for key in ["cloud_code", "cloud_universal", "cloud_vision"]:
                cloud_model = MODELS[key].name
                available[key] = cloud_model in model_names or any(cloud_model in m for m in model_names)

        except Exception as e:
            self.log(f"âš ï¸ Failed to check cloud models: {e}")

        return available

    def _current_base_vram(self) -> float:
        """ÐžÑ†ÐµÐ½ÐºÐ° VRAM, Ð·Ð°Ð½Ð¸Ð¼Ð°ÐµÐ¼Ð¾Ð¹ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ."""
        if self.current_model and MODELS.get(self.current_model):
            model_info = MODELS[self.current_model]
            if model_info.type == "local":
                return model_info.size_gb
        return 0.0

    def _update_vram_usage(self):
        lora_sum = sum(state.info.size_gb for state in self.loaded_loras.values())
        self.current_vram = self._current_base_vram() + lora_sum

    def _evict_incompatible_loras(self):
        """Ð’Ñ‹Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LoRA, Ð½ÐµÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ñ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ."""
        incompatible = [
            key for key, state in self.loaded_loras.items()
            if state.info.base_model_key != self.current_model
        ]
        for key in incompatible:
            self.unload_lora(key)

    def get_loaded_models(self) -> list:
        """Check which models are currently in VRAM"""
        try:
            resp = requests.get(f"{OLLAMA_API}/ps", timeout=5)
            return [m["name"] for m in resp.json().get("models", [])]
        except Exception as e:
            self.log(f"âš ï¸ Failed to check loaded models: {e}")
            return []

    def unload_model(self, model_name: str):
        """Unload model from VRAM"""
        try:
            # Ollama automatically unloads when keep_alive=0
            requests.post(
                f"{OLLAMA_API}/generate",
                json={"model": model_name, "keep_alive": 0},
                timeout=10
            )
            self.log(f"ðŸ—‘ï¸ Unloaded: {model_name}")
        except Exception as e:
            self.log(f"âš ï¸ Failed to unload {model_name}: {e}")

    def _load_lora_via_api(self, adapter_name: str, keep_alive: str = "10m") -> bool:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LoRA Ñ‡ÐµÑ€ÐµÐ· API Ollama/llama.cpp"""
        base_model = self.get_model_name(self.current_model or "")
        if not base_model:
            self.log("âš ï¸ ÐÐµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð° Ð±Ð°Ð·Ð¾Ð²Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ LoRA")
            return False
        try:
            resp = requests.post(
                f"{OLLAMA_API}/generate",
                json={
                    "model": base_model,
                    "prompt": "init",
                    "stream": False,
                    "keep_alive": keep_alive,
                    "options": {"adapter": adapter_name},
                },
                timeout=60,
            )
            if resp.status_code == 200:
                return True
            self.log(f"âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LoRA {adapter_name}: {resp.status_code}")
        except Exception as exc:
            self.log(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐµ LoRA {adapter_name}: {exc}")
        return False

    def _unload_lora_via_api(self, adapter_name: str):
        base_model = self.get_model_name(self.current_model or "")
        if not base_model:
            return
        try:
            requests.post(
                f"{OLLAMA_API}/generate",
                json={
                    "model": base_model,
                    "prompt": "cleanup",
                    "stream": False,
                    "keep_alive": 0,
                    "options": {"adapter": adapter_name},
                },
                timeout=30,
            )
        except Exception as exc:
            self.log(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ñ‹Ð³Ñ€ÑƒÐ·ÐºÐ¸ LoRA {adapter_name}: {exc}")

    def preload_model(self, model_name: str) -> bool:
        """Preload model into VRAM (warmup)"""
        try:
            self.log(f"ðŸ”„ Loading: {model_name}...")
            start = time.time()

            resp = requests.post(
                f"{OLLAMA_API}/generate",
                json={
                    "model": model_name,
                    "prompt": "init",
                    "stream": False,
                    "keep_alive": "10m"
                },
                timeout=60
            )

            if resp.status_code == 200:
                elapsed = time.time() - start
                self.log(f"âœ… Loaded in {elapsed:.1f}s")
                return True
            else:
                self.log(f"âŒ Load failed: {resp.status_code}")
                return False

        except Exception as e:
            self.log(f"âŒ Load error: {e}")
            return False

    def can_load_lora(self, adapter_key: str) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ, Ð¼Ð¾Ð¶Ð½Ð¾ Ð»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LoRA Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ VRAM Ð¸ LRU-Ð²Ñ‹Ð³Ñ€ÑƒÐ·ÐºÐ¸."""
        if adapter_key not in LORA_ADAPTERS:
            self.log(f"âŒ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ LoRA-ÐºÐ»ÑŽÑ‡: {adapter_key}")
            return False

        if self.current_model is None:
            self.log("âš ï¸ Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð²Ñ‹Ð±ÐµÑ€Ð¸ Ð±Ð°Ð·Ð¾Ð²ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¾Ð¹ LoRA")
            return False

        if MODELS.get(self.current_model, ModelInfo("", 0, "cloud", "")).type != "local":
            self.log("âš ï¸ LoRA Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹")
            return False

        info = LORA_ADAPTERS[adapter_key]
        if info.base_model_key != self.current_model:
            self.log(f"âš ï¸ LoRA {adapter_key} Ñ€Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ð½Ð° Ð½Ð° {info.base_model_key}, Ð° Ð°ÐºÑ‚Ð¸Ð²Ð½Ð° {self.current_model}")
            return False

        if adapter_key in self.loaded_loras:
            self.loaded_loras[adapter_key].last_used = time.time()
            return True

        projected = self._current_base_vram() + sum(s.info.size_gb for s in self.loaded_loras.values()) + info.size_gb
        while projected > self.max_vram and self.loaded_loras:
            # LRU: Ð²Ñ‹Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ ÑÐ°Ð¼Ñ‹Ð¹ Ð´Ð°Ð²Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€
            lru_key = min(self.loaded_loras.items(), key=lambda item: item[1].last_used)[0]
            if lru_key == adapter_key:
                break
            self.unload_lora(lru_key)
            projected = self._current_base_vram() + sum(s.info.size_gb for s in self.loaded_loras.values()) + info.size_gb

        if projected > self.max_vram:
            self.log(
                f"âŒ ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ VRAM Ð´Ð»Ñ LoRA {adapter_key}: Ð½ÑƒÐ¶Ð½Ð¾ {projected:.1f}Ð“Ð‘, Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ {self.max_vram:.1f}Ð“Ð‘"
            )
            return False
        return True

    def load_lora(self, adapter_key: str) -> bool:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LoRA Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ VRAM Ð¸ LRU."""
        if adapter_key not in LORA_ADAPTERS:
            self.log(f"âŒ ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ LoRA-Ð°Ð´Ð°Ð¿Ñ‚ÐµÑ€: {adapter_key}")
            return False

        info = LORA_ADAPTERS[adapter_key]
        if info.base_model_key and self.current_model != info.base_model_key:
            if not self.switch_to(info.base_model_key):
                return False

        if not self.can_load_lora(adapter_key):
            return False

        if adapter_key in self.loaded_loras:
            self.loaded_loras[adapter_key].last_used = time.time()
            return True

        if self._load_lora_via_api(info.adapter_name):
            self.loaded_loras[adapter_key] = LoadedLoraState(info=info)
            self._update_vram_usage()
            self.log(f"âœ¨ LoRA Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°: {info.adapter_name} (VRAM {info.size_gb} Ð“Ð‘)")
            return True
        return False

    def unload_lora(self, adapter_key: str):
        if adapter_key not in self.loaded_loras:
            return
        info = self.loaded_loras[adapter_key].info
        self._unload_lora_via_api(info.adapter_name)
        del self.loaded_loras[adapter_key]
        self._update_vram_usage()
        self.log(f"ðŸ—‘ï¸ Ð’Ñ‹Ð³Ñ€ÑƒÐ¶ÐµÐ½ LoRA: {info.adapter_name}")

    def switch_to(self, target_key: str) -> bool:
        """
        Switch to target model, managing VRAM

        Args:
            target_key: "code" / "reason" / "personality" / "cloud"

        Returns:
            True if successfully switched
        """
        if target_key not in MODELS:
            self.log(f"âŒ Unknown model key: {target_key}")
            return False

        model_info = MODELS[target_key]

        # Cloud model - also managed by Ollama, but no VRAM constraints
        if model_info.type == "cloud":
            if not self.cloud_models_available.get(target_key, False):
                self.log(f"âš ï¸ Cloud model '{target_key}' not available in Ollama")
                return False
            # Don't unload local models for cloud - it uses remote compute
            self.current_model = target_key
            self.log(f"ðŸŒ©ï¸ Using cloud model: {model_info.name}")
            return True

        # Local model - check if already loaded
        model_name = model_info.name
        if self.current_model == target_key:
            return True

        loaded = self.get_loaded_models()

        # Unload other models if needed
        for model in loaded:
            if model != model_name:
                self.unload_model(model)
                time.sleep(0.5)  # Give Ollama time to cleanup

        # Load target model if not already loaded
        if model_name not in loaded:
            success = self.preload_model(model_name)
            if not success:
                return False

        self.current_model = target_key
        self._evict_incompatible_loras()
        self._update_vram_usage()
        self.switch_count += 1
        return True

    def activate_lora_for_cell(self, cell_name: str, adapter_key: Optional[str]) -> None:
        """Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ LoRA Ð¿Ñ€Ð¸ Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¸ Ð½Ð° ÐºÐ»ÐµÑ‚ÐºÑƒ."""
        if not adapter_key:
            return
        if self.load_lora(adapter_key):
            self.log(f"ðŸ”Œ ÐšÐ»ÐµÑ‚ÐºÐ° {cell_name} Ð°ÐºÑ‚Ð¸Ð²Ð¸Ñ€Ð¾Ð²Ð°Ð»Ð° LoRA {adapter_key}")

    def get_stats(self) -> Dict[str, Any]:
        """Get manager statistics"""
        self._update_vram_usage()
        return {
            "current_model": self.current_model,
            "current_model_info": MODELS.get(self.current_model),
            "switches": self.switch_count,
            "loaded_models": self.get_loaded_models(),
            "cloud_models_available": self.cloud_models_available,
            "max_vram_gb": self.max_vram,
            "current_vram_gb": round(self.current_vram, 2),
            "loaded_loras": [state.info.adapter_name for state in self.loaded_loras.values()],
            "lora_registry": {k: v.adapter_name for k, v in LORA_ADAPTERS.items()}
        }

    def get_model_name(self, key: str) -> str:
        """Get actual model name from key"""
        if key in MODELS:
            return MODELS[key].name
        return ""

    def get_adapter_name(self, adapter_key: str) -> str:
        if adapter_key in LORA_ADAPTERS:
            return LORA_ADAPTERS[adapter_key].adapter_name
        return ""


# === TEST ===
if __name__ == "__main__":
    print("Model Manager Test")
    print("=" * 50)

    manager = ModelManager(verbose=True)

    print("\n1. Testing model switch to reason (mistral)...")
    manager.switch_to("reason")

    print("\n2. Stats:")
    stats = manager.get_stats()
    for key, val in stats.items():
        print(f"  {key}: {val}")

    print("\n3. Testing model switch to code (qwen-coder)...")
    manager.switch_to("code")

    print("\n4. Final stats:")
    stats = manager.get_stats()
    print(f"  Current: {stats['current_model']}")
    print(f"  Switches: {stats['switches']}")
    print(f"  Loaded: {stats['loaded_models']}")
