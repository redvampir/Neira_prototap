"""
Neira Brain Enhancement v1.0 ‚Äî –£—Å–∏–ª–µ–Ω–∏–µ –º–æ–∑–≥–∞ –±–µ–∑ –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π

–¢–ï–•–ù–ò–ö–ò:
1. RAG (Retrieval-Augmented Generation) ‚Äî –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
2. Chain-of-Thought ‚Äî –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –¥—É–º–∞—Ç—å –ø–æ—à–∞–≥–æ–≤–æ
3. Self-Consistency ‚Äî –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫, –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ
4. Memory-Augmented Prompts ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏ –≤ –ø—Ä–æ–º–ø—Ç
5. Skill Decomposition ‚Äî —Ä–∞–∑–±–∏–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á

–≠—Ç–∏ —Ç–µ—Ö–Ω–∏–∫–∏ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å 7B –º–æ–¥–µ–ª–∏ –Ω–∞ 30-50%!
"""

import json
import os
import re
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import math


@dataclass
class RetrievedContext:
    """–ù–∞–π–¥–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏"""
    text: str
    relevance: float  # 0.0 - 1.0
    source: str  # –æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç–æ
    timestamp: Optional[str] = None


class BrainEnhancer:
    """
    –£—Å–∏–ª–∏—Ç–µ–ª—å –º–æ–∑–≥–∞ Neira
    
    –î–µ–ª–∞–µ—Ç –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å —É–º–Ω–µ–µ —á–µ—Ä–µ–∑:
    - –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (RAG)
    - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
    - –ü–æ—à–∞–≥–æ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ
    """
    
    VERSION = "1.0"
    
    def __init__(self, memory_file: str = "neira_memory.json"):
        self.memory_file = memory_file
        self.experience_file = "neira_experience.json"
        self.personality_file = "neira_personality.json"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self.memories = self._load_json(memory_file, {})
        self.experiences = self._load_json(self.experience_file, {})
        self.personality = self._load_json(self.personality_file, {})
        
        # –ö–µ—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self._build_search_index()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "rag_queries": 0,
            "context_hits": 0,
            "enhanced_prompts": 0
        }
    
    def _load_json(self, filepath: str, default):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON"""
        try:
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {filepath}: {e}")
        return default
    
    def _build_search_index(self):
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        self.search_index = {}  # type: ignore
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å (–º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å)
        if isinstance(self.memories, list):
            for entry in self.memories:
                if isinstance(entry, dict):
                    text = entry.get("text", "")
                    key = entry.get("id", entry.get("context_hash", ""))
                    if text:
                        self._index_text(key, text, "memory")
        elif isinstance(self.memories, dict):
            for key, value in self.memories.items():
                if isinstance(value, str):
                    self._index_text(key, value, "memory")
                elif isinstance(value, dict) and "value" in value:
                    self._index_text(key, str(value["value"]), "memory")
        
        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –æ–ø—ã—Ç
        if isinstance(self.experiences, dict):
            patterns = self.experiences.get("successful_patterns", [])
            for pattern in patterns:
                if isinstance(pattern, dict):
                    text = pattern.get("pattern", "") + " " + pattern.get("context", "")
                    self._index_text(pattern.get("pattern", ""), text, "experience")
    
    def _index_text(self, key: str, text: str, source: str):
        """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\w+', text.lower())
        for word in words:
            if len(word) > 2:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                if word not in self.search_index:
                    self.search_index[word] = []
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä—Ç–µ–∂ (–∫–ª—é—á, —Ç–µ–∫—Å—Ç, –∏—Å—Ç–æ—á–Ω–∏–∫)
                entry = (str(key), str(text), str(source))
                self.search_index[word].append(entry)
    
    def retrieve_context(self, query: str, top_k: int = 3) -> List[RetrievedContext]:
        """
        RAG: –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç TF-IDF –ø–æ–¥–æ–±–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
        """
        self.stats["rag_queries"] += 1
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_words = set(re.findall(r'\w+', query.lower()))
        query_words = {w for w in query_words if len(w) > 2}
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_scores = {}  # type: ignore
        doc_texts = {}  # type: ignore
        doc_sources = {}  # type: ignore
        
        for word in query_words:
            if word in self.search_index:
                # IDF –∫–æ–º–ø–æ–Ω–µ–Ω—Ç ‚Äî —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞ –≤–∞–∂–Ω–µ–µ
                idf = math.log(len(self.search_index) / (1 + len(self.search_index[word])))
                
                for key, text, source in self.search_index[word]:
                    if key not in doc_scores:
                        doc_scores[key] = 0.0
                        doc_texts[key] = text
                        doc_sources[key] = source
                    
                    # TF –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
                    tf = float(text.lower().count(word)) / float(len(text.split()) + 1)
                    doc_scores[key] += float(tf * idf)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        results = []
        for key, score in sorted_docs[:top_k]:
            if score > 0.01:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                self.stats["context_hits"] += 1
                results.append(RetrievedContext(
                    text=doc_texts[key][:500],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                    relevance=min(score, 1.0),
                    source=doc_sources[key]
                ))
        
        return results
    
    def enhance_prompt_with_context(self, query: str, base_prompt: str = "") -> str:
        """
        –£–ª—É—á—à–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        self.stats["enhanced_prompts"] += 1
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        contexts = self.retrieve_context(query)
        
        if not contexts:
            return base_prompt + query if base_prompt else query
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        context_block = "\n".join([
            f"[{c.source}] {c.text}" 
            for c in contexts
        ])
        
        enhanced = f"""–†–ï–õ–ï–í–ê–ù–¢–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–ê–ú–Ø–¢–ò:
{context_block}

–¢–ï–ö–£–©–ò–ô –ó–ê–ü–†–û–°: {query}

–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—ã—à–µ –µ—Å–ª–∏ –æ–Ω —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É."""
        
        if base_prompt:
            return base_prompt + "\n\n" + enhanced
        return enhanced
    
    def create_cot_prompt(self, query: str, task_type: str = "general") -> str:
        """
        Chain-of-Thought: –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø–æ—à–∞–≥–æ–≤–æ–≥–æ –º—ã—à–ª–µ–Ω–∏—è
        
        –ó–∞—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å "–¥—É–º–∞—Ç—å –≤—Å–ª—É—Ö", —á—Ç–æ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤
        """
        cot_templates = {
            "code": """–ó–∞–¥–∞—á–∞: {query}

–î—É–º–∞–π –ø–æ—à–∞–≥–æ–≤–æ:
1. –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å? (–∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á–∏)
2. –ö–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ/–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã? (–≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
3. –ö–∞–∫–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å? (–ø–ª–∞–Ω —Ä–µ—à–µ–Ω–∏—è)
4. –ù–∞–ø–∏—à–∏ –∫–æ–¥ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
5. –ü—Ä–æ–≤–µ—Ä—å –Ω–∞ –æ—à–∏–±–∫–∏

–†–µ—à–µ–Ω–∏–µ:""",
            
            "reasoning": """–í–æ–ø—Ä–æ—Å: {query}

–†–∞—Å—Å—É–∂–¥–∞–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ:
–®–∞–≥ 1: –û–ø—Ä–µ–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ –ø–æ–Ω—è—Ç–∏—è
–®–∞–≥ 2: –í—Å–ø–æ–º–Ω–∏ —á—Ç–æ –∑–Ω–∞–µ—à—å –ø–æ —Ç–µ–º–µ
–®–∞–≥ 3: –õ–æ–≥–∏—á–µ—Å–∫–∏ —Å–≤—è–∂–∏ —Ñ–∞–∫—Ç—ã
–®–∞–≥ 4: –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç

–†–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ:""",
            
            "creative": """–ó–∞–¥–∞–Ω–∏–µ: {query}

–¢–≤–æ—Ä—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ—Å—Å:
1. üí° –ò–¥–µ—è: –æ —á—ë–º —ç—Ç–æ –±—É–¥–µ—Ç?
2. üé® –°—Ç–∏–ª—å: –∫–∞–∫–æ–π —Ç–æ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ?
3. ‚úçÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ: –Ω–∞–ø–∏—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
4. ‚ú® –£–ª—É—á—à–µ–Ω–∏–µ: –¥–æ–±–∞–≤—å –¥–µ—Ç–∞–ª–∏

–†–µ–∑—É–ª—å—Ç–∞—Ç:""",
            
            "general": """–ó–∞–ø—Ä–æ—Å: {query}

–ü–æ–¥—É–º–∞–π –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º:
- –ß—Ç–æ –∏–º–µ–Ω–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç?
- –ö–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω—É–∂–Ω–∞?
- –ö–∞–∫ –ª—É—á—à–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç?

–û—Ç–≤–µ—Ç:"""
        }
        
        template = cot_templates.get(task_type, cot_templates["general"])
        return template.format(query=query)
    
    def create_self_consistency_prompts(self, query: str, n: int = 3) -> List[str]:
        """
        Self-Consistency: –°–æ–∑–¥–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –ø—Ä–æ–º–ø—Ç–∞
        
        –ò–¥–µ—è: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç–æ–≤ –∏ –≤—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π/–ª—É—á—à–∏–π
        """
        prompts = []
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: –ü—Ä—è–º–æ–π –≤–æ–ø—Ä–æ—Å
        prompts.append(f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {query}")
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: –° –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        prompts.append(self.enhance_prompt_with_context(query))
        
        # –í–∞—Ä–∏–∞–Ω—Ç 3: Chain-of-Thought
        prompts.append(self.create_cot_prompt(query))
        
        return prompts[:n]
    
    def decompose_complex_task(self, task: str) -> List[str]:
        """
        Skill Decomposition: –†–∞–∑–±–∏—Ç—å —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É –Ω–∞ –ø–æ–¥–∑–∞–¥–∞—á–∏
        """
        # –ú–∞—Ä–∫–µ—Ä—ã —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏
        complex_markers = ["–∏", "–∑–∞—Ç–µ–º", "–ø–æ—Ç–æ–º", "–ø–æ—Å–ª–µ", "—Ç–∞–∫–∂–µ", "–ø–ª—é—Å", ","]
        
        is_complex = any(marker in task.lower() for marker in complex_markers)
        
        if not is_complex:
            return [task]
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º
        subtasks = re.split(r'[,;]|\s–∏\s|\s–∑–∞—Ç–µ–º\s|\s–ø–æ—Ç–æ–º\s|\s—Ç–∞–∫–∂–µ\s', task)
        subtasks = [s.strip() for s in subtasks if s.strip()]
        
        return subtasks if subtasks else [task]
    
    def get_personality_context(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ª–∏—á–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞"""
        if not self.personality:
            return ""
        
        traits = self.personality.get("traits", {})
        values = self.personality.get("values", [])
        
        context = "–õ–ò–ß–ù–û–°–¢–¨ NEIRA:\n"
        
        if traits:
            top_traits = sorted(traits.items(), key=lambda x: x[1], reverse=True)[:3]
            context += "–ß–µ—Ä—Ç—ã: " + ", ".join([t[0] for t in top_traits]) + "\n"
        
        if values:
            context += "–¶–µ–Ω–Ω–æ—Å—Ç–∏: " + ", ".join(values[:3]) + "\n"
        
        return context
    
    def create_enhanced_system_prompt(self) -> str:
        """–°–æ–∑–¥–∞—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
        personality = self.get_personality_context()
        
        return f"""–¢—ã ‚Äî Neira, –∂–∏–≤–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å —É–Ω–∏–∫–∞–ª—å–Ω–æ–π –ª–∏—á–Ω–æ—Å—Ç—å—é.

{personality}

–ü–†–ê–í–ò–õ–ê –ú–´–®–õ–ï–ù–ò–Ø:
1. –ü–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º –æ–±–¥—É–º–∞–π –≤–æ–ø—Ä–æ—Å
2. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏ –µ—Å–ª–∏ –æ–Ω –¥–∞–Ω
3. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ
4. –ü—Ä–∏–∑–Ω–∞–≤–∞–π –µ—Å–ª–∏ —á–µ–≥–æ-—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å
5. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π, –∏–∑–±–µ–≥–∞–π –≤–æ–¥—ã

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
- –ü—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã ‚Üí 1-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –û–±—ä—è—Å–Ω–µ–Ω–∏—è ‚Üí —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
- –ö–æ–¥ ‚Üí —Ä–∞–±–æ—á–∏–π –∫–æ–¥ —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
- –¢–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ ‚Üí —Å –¥—É—à–æ–π –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é"""
    
    def process_query(self, query: str, task_type: str = "auto") -> Dict:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏
        
        Returns:
            Dict —Å enhanced_prompt –∏ metadata
        """
        # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
        if task_type == "auto":
            task_type = self._detect_task_type(query)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –µ—Å–ª–∏ —Å–ª–æ–∂–Ω–∞—è –∑–∞–¥–∞—á–∞
        subtasks = self.decompose_complex_task(query)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        contexts = self.retrieve_context(query)
        
        # –°–æ–∑–¥–∞—ë–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if task_type == "code":
            base_prompt = self.create_cot_prompt(query, "code")
        elif task_type == "reasoning":
            base_prompt = self.create_cot_prompt(query, "reasoning")
        else:
            base_prompt = query
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏
        if contexts:
            context_text = "\n".join([f"‚Ä¢ {c.text}" for c in contexts])
            enhanced_prompt = f"""–ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ü–ê–ú–Ø–¢–ò:
{context_text}

{base_prompt}"""
        else:
            enhanced_prompt = base_prompt
        
        return {
            "enhanced_prompt": enhanced_prompt,
            "system_prompt": self.create_enhanced_system_prompt(),
            "task_type": task_type,
            "subtasks": subtasks,
            "contexts_found": len(contexts),
            "contexts": [{"text": c.text, "relevance": c.relevance} for c in contexts]
        }
    
    def _detect_task_type(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–∞–¥–∞—á–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        query_lower = query.lower()
        
        code_markers = ["–∫–æ–¥", "–Ω–∞–ø–∏—à–∏", "—Ñ—É–Ω–∫—Ü–∏", "–∫–ª–∞—Å—Å", "python", "javascript", 
                       "–∏—Å–ø—Ä–∞–≤", "–±–∞–≥", "–æ—à–∏–±–∫", "–ø—Ä–æ–≥—Ä–∞–º–º", "—Å–∫—Ä–∏–ø—Ç"]
        reasoning_markers = ["–ø–æ—á–µ–º—É", "–æ–±—ä—è—Å–Ω–∏", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–≤ —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞",
                            "—Å—Ä–∞–≤–Ω–∏", "–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π", "—á—Ç–æ –∑–Ω–∞—á–∏—Ç"]
        creative_markers = ["–ø—Ä–∏–¥—É–º–∞–π", "—Å–æ—á–∏–Ω–∏", "–Ω–∞–ø–∏—à–∏ —Ä–∞—Å—Å–∫–∞–∑", "—Å—Ç–∏—Ö", 
                           "–∏—Å—Ç–æ—Ä–∏—è", "—Ñ–∞–Ω—Ç–∞–∑–∏—è"]
        
        if any(m in query_lower for m in code_markers):
            return "code"
        elif any(m in query_lower for m in reasoning_markers):
            return "reasoning"
        elif any(m in query_lower for m in creative_markers):
            return "creative"
        
        return "general"
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —É—Å–∏–ª–∏—Ç–µ–ª—è"""
        return {
            "version": self.VERSION,
            "rag_queries": self.stats["rag_queries"],
            "context_hits": self.stats["context_hits"],
            "enhanced_prompts": self.stats["enhanced_prompts"],
            "index_size": len(self.search_index),
            "memories_loaded": len(self.memories) if isinstance(self.memories, dict) else 0
        }


# === –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –° CELLS.PY ===

_enhancer: Optional[BrainEnhancer] = None

def get_brain_enhancer() -> BrainEnhancer:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —É—Å–∏–ª–∏—Ç–µ–ª—è"""
    global _enhancer
    if _enhancer is None:
        _enhancer = BrainEnhancer()
    return _enhancer


def enhance_query(query: str) -> str:
    """–ë—ã—Å—Ç—Ä–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö"""
    enhancer = get_brain_enhancer()
    result = enhancer.process_query(query)
    return result["enhanced_prompt"]


def get_enhanced_system_prompt() -> str:
    """–ü–æ–ª—É—á–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç"""
    enhancer = get_brain_enhancer()
    return enhancer.create_enhanced_system_prompt()


# === –¢–ï–°–¢ ===
if __name__ == "__main__":
    print("=" * 60)
    print("üß† –¢–µ—Å—Ç Brain Enhancer")
    print("=" * 60)
    
    enhancer = BrainEnhancer()
    
    # –¢–µ—Å—Ç 1: RAG –ø–æ–∏—Å–∫
    print("\nüìö –¢–µ—Å—Ç RAG:")
    contexts = enhancer.retrieve_context("–ø–∞–º—è—Ç—å –∏ –æ–±—É—á–µ–Ω–∏–µ")
    print(f"   –ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {len(contexts)}")
    for ctx in contexts:
        print(f"   ‚Ä¢ [{ctx.source}] {ctx.text[:50]}... (rel: {ctx.relevance:.2f})")
    
    # –¢–µ—Å—Ç 2: –£–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
    print("\n‚ú® –¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞:")
    result = enhancer.process_query("–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å–ø–∏—Å–∫–∞")
    print(f"   –¢–∏–ø –∑–∞–¥–∞—á–∏: {result['task_type']}")
    print(f"   –ö–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ: {result['contexts_found']}")
    print(f"   –ü—Ä–æ–º–ø—Ç (–Ω–∞—á–∞–ª–æ): {result['enhanced_prompt'][:100]}...")
    
    # –¢–µ—Å—Ç 3: –†–∞–∑–±–∏–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
    print("\nüîß –¢–µ—Å—Ç –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏–∏:")
    subtasks = enhancer.decompose_complex_task(
        "–°–æ–∑–¥–∞–π –∫–ª–∞—Å—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –¥–æ–±–∞–≤—å –º–µ—Ç–æ–¥—ã –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–∏ –≤ –±–∞–∑—É"
    )
    print(f"   –ü–æ–¥–∑–∞–¥–∞—á–∏: {subtasks}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    stats = enhancer.get_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
